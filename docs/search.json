[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Complete bib file can be downloaded here."
  },
  {
    "objectID": "posts/taylor-swift-pca/index.html",
    "href": "posts/taylor-swift-pca/index.html",
    "title": "Principal Component Analysis (PCA) of Taylor Swift Discography",
    "section": "",
    "text": "This is a quick PCA of Taylor Swift‚Äôs discography using the dataset from the tidytuesday 2023, week 42. PCA is a technique used to reduce high dimensional data into principal components. This also allows us the ability to project and visualize high dimensional data in two dimensional space via a scatter plot.\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tidymodels)\nlibrary(plotly)\nlibrary(rlang)\nlibrary(tidytext)\n\ntheme_set(\n  theme_minimal(\n    base_size = 15\n  )\n)\n\nWe start off with loading the data using the tidytuesdayR package. We are also going to download the data using the tt_download function. The specific dataset we are using is the taylor_all_songs dataset.\n\ndatas &lt;- tt_load_gh(2023, 42)\n\ndatas &lt;- tt_download(datas)\n\n\n    Downloading file 1 of 3: `taylor_album_songs.csv`\n    Downloading file 2 of 3: `taylor_all_songs.csv`\n    Downloading file 3 of 3: `taylor_albums.csv`\n\ndata &lt;- datas$taylor_all_songs\n\nNext we are going to take a quick glimpse at the data to see what we are working with.\n\nglimpse(data)\n\nRows: 274\nColumns: 29\n$ album_name          &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T‚Ä¶\n$ ep                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F‚Ä¶\n$ album_release       &lt;date&gt; 2006-10-24, 2006-10-24, 2006-10-24, 2006-10-24, 2‚Ä¶\n$ track_number        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,‚Ä¶\n$ track_name          &lt;chr&gt; \"Tim McGraw\", \"Picture To Burn\", \"Teardrops On My ‚Ä¶\n$ artist              &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T‚Ä¶\n$ featuring           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ bonus_track         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F‚Ä¶\n$ promotional_release &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ single_release      &lt;date&gt; 2006-06-19, 2008-02-03, 2007-02-19, NA, NA, NA, N‚Ä¶\n$ track_release       &lt;date&gt; 2006-06-19, 2006-10-24, 2006-10-24, 2006-10-24, 2‚Ä¶\n$ danceability        &lt;dbl&gt; 0.580, 0.658, 0.621, 0.576, 0.418, 0.589, 0.479, 0‚Ä¶\n$ energy              &lt;dbl&gt; 0.491, 0.877, 0.417, 0.777, 0.482, 0.805, 0.578, 0‚Ä¶\n$ key                 &lt;dbl&gt; 0, 7, 10, 9, 5, 5, 2, 8, 4, 2, 2, 8, 7, 4, 10, 5, ‚Ä¶\n$ loudness            &lt;dbl&gt; -6.462, -2.098, -6.941, -2.881, -5.769, -4.055, -4‚Ä¶\n$ mode                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ speechiness         &lt;dbl&gt; 0.0251, 0.0323, 0.0231, 0.0324, 0.0266, 0.0293, 0.‚Ä¶\n$ acousticness        &lt;dbl&gt; 0.57500, 0.17300, 0.28800, 0.05100, 0.21700, 0.004‚Ä¶\n$ instrumentalness    &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, ‚Ä¶\n$ liveness            &lt;dbl&gt; 0.1210, 0.0962, 0.1190, 0.3200, 0.1230, 0.2400, 0.‚Ä¶\n$ valence             &lt;dbl&gt; 0.425, 0.821, 0.289, 0.428, 0.261, 0.591, 0.192, 0‚Ä¶\n$ tempo               &lt;dbl&gt; 76.009, 105.586, 99.953, 115.028, 175.558, 112.982‚Ä¶\n$ time_signature      &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,‚Ä¶\n$ duration_ms         &lt;dbl&gt; 232107, 173067, 203040, 199200, 239013, 207107, 24‚Ä¶\n$ explicit            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F‚Ä¶\n$ key_name            &lt;chr&gt; \"C\", \"G\", \"A#\", \"A\", \"F\", \"F\", \"D\", \"G#\", \"E\", \"D\"‚Ä¶\n$ mode_name           &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"major\", \"majo‚Ä¶\n$ key_mode            &lt;chr&gt; \"C major\", \"G major\", \"A# major\", \"A major\", \"F ma‚Ä¶\n$ lyrics              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nWe are now going to select the columns to use for PCA. We are going to extract the columns containing various attributes of the songs. We are going to use the track_name column as the response variable and the rest of the song attribute columns as the predictors. Any rows with missing values are going to be dropped from the analysis.\nPCA is done using the recipes package. The first step is to create a recipe using the recipe function. The recipe function takes in a formula and a dataset. The LHS of the formula is the response variable and the RHS is the predictors. The next step is to normalize the data using the step_normalize function. This function centers and scales the numeric predictors. The next step is actually performing PCA using the step_pca function. The step_pca function takes in the columns that we want to use for the PCA, and we also defines the number of components that we want to extract. The last step is to prep the recipe using the prep function.\n\ndata &lt;- data |&gt;\n  select(\n    track_name,\n    danceability,\n    energy,\n    key,\n    loudness,\n    speechiness,\n    acousticness,\n    instrumentalness,\n    liveness,\n    valence,\n    tempo,\n    duration_ms\n  ) |&gt; drop_na()\n\nrec &lt;- recipe(track_name ~ ., data = data) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 5) |&gt; \n  prep()\n\nAfter conducting PCA, we are going to take a look at the proportion of variances that are explained by each component. We can see that the first component explains 26% of the variance, the second component explains 16% of the variance, the third component explains 11% of the variance. The first two component explains 42% of the cumulative variances.\n\nlocal({\n  pca &lt;- rec$steps[[2]]$res |&gt; summary()\n  \n  plot_data &lt;- pca$importance |&gt; as_tibble(rownames = 'type')\n  \n  plot &lt;- plot_data |&gt;\n    pivot_longer(2:ncol(plot_data)) |&gt;\n    filter(type == 'Proportion of Variance') |&gt;\n    mutate(name = as_factor(name)) |&gt;\n    ggplot(aes(x = name, y = value)) +\n    geom_col() +\n    labs(y = 'Proportion of Variance', x = NULL) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    geom_text(\n      aes(label = scales::label_percent(1)(value)),\n      vjust = 0,\n      nudge_y = .001,\n      size = 5\n    )\n  \n  plot\n})\n\n\n\n\nWe are now going to take a look at the various attributes of the songs that are associated with each component.\nWe can see that the first component is associated with the energy, loudness, acousticness of the songs. The second component is associated with the danceability, duration, and speechiness of the songs.\n\ntidy(rec, 2) |&gt;\n  filter(component %in% paste0('PC', 1:5)) |&gt;\n  mutate(positive = ifelse(value &gt; 0, 'Positive', 'Negative')) |&gt;\n  mutate(value = abs(value)) |&gt;\n  \n  ggplot(aes(\n    x = value,\n    y = reorder_within(terms, value, component),\n    fill = positive\n  )) +\n  geom_col() +\n  scale_y_reordered() +\n  facet_wrap( ~ component, scales = 'free') +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = 'bottom') +\n  labs(x = 'Absolute Value of Coefficient', y = NULL, fill = NULL)\n\n\n\n\nFinally, we are going to visualize the first two components using a scatter plot. Based on the attributes that we have seen in the previous plot, we can see that a positive PC1 is associated with higher energy, loudness, and positivity (valence). A negative PC1 is associated with higher acousticness, duration, and instrumentalness. A positive PC2 is associated with lengthier songs, higher tempo, and loudness, while a negative PC2 is associated with higher danceability, speechiness, and positivity (valence)\n\nmake_pc_plot &lt;- \\(data, pc_x, pc_y) {\n  pc_x &lt;- parse_expr(pc_x)\n  pc_y &lt;- parse_expr(pc_y)\n  \n  p &lt;- expr({\n    ggplot(pca_plot, aes(\n      x = !!pc_x,\n      y = !!pc_y,\n      color = track_name\n    )) +\n      geom_point() +\n      theme_minimal(base_size = 15) +\n      theme(legend.position = \"none\") +\n      coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      geom_hline(yintercept = 0,\n                 linetype = 'dashed',\n                 alpha = .25) +\n      geom_vline(xintercept = 0,\n                 linetype = 'dashed',\n                 alpha = .25)\n    \n  }) |&gt; eval()\n  \n  ggplotly(p, tooltip = 'track_name')\n  \n}\n  \npca_plot &lt;- bake(rec, new_data = NULL)\n\nmake_pc_plot(pca_plot, 'PC2', 'PC1')\n\n\n\n\n\nAlthough traditionally most of the variance is usually explained by the first two PC. We can further visualize other combinations of PC in two dimensional space.\n\nmake_pc_plot(pca_plot, 'PC3', 'PC1')\n\n\n\n\nmake_pc_plot(pca_plot, 'PC4', 'PC1')\n\n\n\n\nmake_pc_plot(pca_plot, 'PC5', 'PC1')"
  },
  {
    "objectID": "posts/retrieve-pubmed-publications/index.html",
    "href": "posts/retrieve-pubmed-publications/index.html",
    "title": "Retrieve Pubmed publications using the RefManageR package",
    "section": "",
    "text": "For anyone that‚Äôs working in academia, it may be useful to keep a tabulation of all of the publications that you are a co-author of. We can use the RefManageR package, which includes a function (ReadPubMed()) that allows us to query the PubMed API for publications based on a PubMed query.\nBelow I generate a PubMed query q that would query PubMed for publications that involves Luu, Michael as an author with a Cedars-Sinai affiliation. I then pass this query into the ReadPubMed() function and save the results as pm. The output is a BibEntry object that can be further coerced into a tibble.\n\nq &lt;- '(Luu, Michael[Author]) AND (Cedars-Sinai[Affiliation])'\n\npm &lt;- RefManageR::ReadPubMed(q, retmax = 999)\n  \nout &lt;- pm |&gt; as_tibble()\n\nglimpse(out)\n\nRows: 68\nColumns: 15\n$ bibtype    &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article\", \"Article\", \"Art‚Ä¶\n$ title      &lt;chr&gt; \"Concurrent prognostic utility of lymph node count and lymp‚Ä¶\n$ author     &lt;chr&gt; \"John M Masterson and Michael Luu and Aurash Naser-Tavakoli‚Ä¶\n$ year       &lt;chr&gt; \"2023\", \"2022\", \"2023\", \"2022\", \"2022\", \"2022\", \"2022\", \"20‚Ä¶\n$ month      &lt;chr&gt; \"Jan\", \"Nov\", \"Apr\", \"Aug\", \"Oct\", \"Jul\", \"Aug\", \"Jul\", \"Ma‚Ä¶\n$ journal    &lt;chr&gt; \"Prostate cancer and prostatic diseases\", \"NPJ breast cance‚Ä¶\n$ eprint     &lt;chr&gt; \"36600045\", \"36402796\", \"36385470\", \"36054029\", \"35997126\",‚Ä¶\n$ doi        &lt;chr&gt; \"10.1038/s41391-022-00635-1\", \"10.1038/s41523-022-00489-9\",‚Ä¶\n$ language   &lt;chr&gt; \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"en‚Ä¶\n$ issn       &lt;chr&gt; \"1476-5608\", \"2374-4677\", \"1531-4995\", \"1531-4995\", \"1097-0‚Ä¶\n$ abstract   &lt;chr&gt; \"BACKGROUND: While both the number (+LN) and density (LND) ‚Ä¶\n$ eprinttype &lt;chr&gt; \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\",‚Ä¶\n$ volume     &lt;chr&gt; NA, \"8\", \"133\", NA, \"128\", \"113\", \"208\", \"114\", \"165\", \"40\"‚Ä¶\n$ number     &lt;chr&gt; NA, \"1\", \"4\", NA, \"20\", \"4\", \"2\", \"7\", \"2\", \"4\", \"1\", \"1\", ‚Ä¶\n$ pages      &lt;chr&gt; NA, \"123\", \"E25\", NA, \"3610-3619\", \"787-795\", \"301-308\", \"1‚Ä¶\n\nout\n\n# A tibble: 68 √ó 15\n   bibtype title author year  month journal eprint doi   language issn  abstract\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   \n 1 Article Conc‚Ä¶ John ‚Ä¶ 2023  Jan   Prosta‚Ä¶ 36600‚Ä¶ 10.1‚Ä¶ eng      1476‚Ä¶ \"BACKGR‚Ä¶\n 2 Article Toxi‚Ä¶ N Lyn‚Ä¶ 2022  Nov   NPJ br‚Ä¶ 36402‚Ä¶ 10.1‚Ä¶ eng      2374‚Ä¶ \"Adjuva‚Ä¶\n 3 Article In R‚Ä¶ Eric ‚Ä¶ 2023  Apr   The La‚Ä¶ 36385‚Ä¶ 10.1‚Ä¶ eng      1531‚Ä¶  &lt;NA&gt;   \n 4 Article Pred‚Ä¶ Eric ‚Ä¶ 2022  Aug   The La‚Ä¶ 36054‚Ä¶ 10.1‚Ä¶ eng      1531‚Ä¶ \"BACKGR‚Ä¶\n 5 Article Disp‚Ä¶ Yi-Te‚Ä¶ 2022  Oct   Cancer  35997‚Ä¶ 10.1‚Ä¶ eng      1097‚Ä¶ \"BACKGR‚Ä¶\n 6 Article Noda‚Ä¶ Diana‚Ä¶ 2022  Jul   Intern‚Ä¶ 35395‚Ä¶ 10.1‚Ä¶ eng      1879‚Ä¶ \"PURPOS‚Ä¶\n 7 Article Vari‚Ä¶ Timot‚Ä¶ 2022  Aug   The Jo‚Ä¶ 35377‚Ä¶ 10.1‚Ä¶ eng      1527‚Ä¶ \"PURPOS‚Ä¶\n 8 Article Quan‚Ä¶ Antho‚Ä¶ 2022  Jul   Journa‚Ä¶ 35311‚Ä¶ 10.1‚Ä¶ eng      1460‚Ä¶ \"BACKGR‚Ä¶\n 9 Article Path‚Ä¶ Eric ‚Ä¶ 2022  May   Gyneco‚Ä¶ 35216‚Ä¶ 10.1‚Ä¶ eng      1095‚Ä¶ \"PURPOS‚Ä¶\n10 Article Pred‚Ä¶ Paige‚Ä¶ 2022  Apr   Urolog‚Ä¶ 35067‚Ä¶ 10.1‚Ä¶ eng      1873‚Ä¶ \"BACKGR‚Ä¶\n# ‚Ñπ 58 more rows\n# ‚Ñπ 4 more variables: eprinttype &lt;chr&gt;, volume &lt;chr&gt;, number &lt;chr&gt;, pages &lt;chr&gt;\n\n\nNow that we have this information into a tibble, we can further visualize the frequency of occurrences among the list of co-authors using a word cloud\n\nplot_data &lt;- out |&gt;\n  select(author) |&gt;\n  separate_wider_delim(\n    author,\n    names = paste0('author', 1:20),\n    delim = ' and ',\n    too_few = 'align_start'\n  ) |&gt;\n  mutate(i = row_number()) |&gt;\n  pivot_longer(contains('author')) |&gt;\n  filter(!is.na(value)) |&gt; \n  count(value) |&gt; \n  arrange(desc(n)) |&gt; \n  filter(value != 'Michael Luu')\n\nset.seed(1)\nggplot(plot_data, aes(label = value, size = n, color = n)) +\n  geom_text_wordcloud_area() +\n  scale_size_area(max_size = 16) +\n  theme_minimal() +\n  scale_color_viridis_c()\n\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ggwordcloud_0.5.0 RefManageR_1.4.0  lubridate_1.9.2   forcats_1.0.0    \n [5] stringr_1.5.0     dplyr_1.1.1       purrr_1.0.1       readr_2.1.4      \n [9] tidyr_1.3.0       tibble_3.2.1      ggplot2_3.4.2     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0  xfun_0.38         colorspace_2.1-0  vctrs_0.6.1      \n [5] generics_0.1.3    viridisLite_0.4.1 htmltools_0.5.4   yaml_2.3.6       \n [9] utf8_1.2.3        rlang_1.1.0       pillar_1.9.0      glue_1.6.2       \n[13] withr_2.5.0       lifecycle_1.0.3   plyr_1.8.8        munsell_0.5.0    \n[17] gtable_0.3.3      htmlwidgets_1.6.2 evaluate_0.19     labeling_0.4.2   \n[21] knitr_1.41        tzdb_0.3.0        fastmap_1.1.0     curl_5.0.0       \n[25] fansi_1.0.4       Rcpp_1.0.10       renv_0.16.0       scales_1.2.1     \n[29] backports_1.4.1   jsonlite_1.8.4    farver_2.1.1      hms_1.1.3        \n[33] png_0.1-8         digest_0.6.31     stringi_1.7.8     grid_4.2.2       \n[37] bibtex_0.5.1      cli_3.6.1         tools_4.2.2       magrittr_2.0.3   \n[41] pkgconfig_2.0.3   xml2_1.3.3        timechange_0.2.0  rmarkdown_2.19   \n[45] httr_1.4.5        rstudioapi_0.14   R6_2.5.1          compiler_4.2.2"
  },
  {
    "objectID": "posts/quarto-shinylive/index.html",
    "href": "posts/quarto-shinylive/index.html",
    "title": "Implementation of ShinyLive in Quarto",
    "section": "",
    "text": "The following blogpost is an example implementation of ShinyLive.\nA traditional Shiny application requires a Shiny Server for computation, that handles both the computation and serving the application to the user.\n\nThe inverse is true with the implementation of ShinyLive and WebR, where the application can be hosted on a static web server similar to GitHub Pages, and the computation is handled by the user themselves.\n\nThe current blogpost is an example implementation of ShinyLive in Quarto using the shinylive-quarto extension. Source code for the current blogpost can be found here.\nMore information regarding the implementation of shinylive can be found here.\n#| standalone: true\n#| viewerHeight: 600\n\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/nlp-taylor-swift-beyonce/index.html",
    "href": "posts/nlp-taylor-swift-beyonce/index.html",
    "title": "Natural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday",
    "section": "",
    "text": "Let‚Äôs start off by loading the data from the tidytuesday github repository."
  },
  {
    "objectID": "posts/nlp-taylor-swift-beyonce/index.html#model-parameter-tuning",
    "href": "posts/nlp-taylor-swift-beyonce/index.html#model-parameter-tuning",
    "title": "Natural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday",
    "section": "Model parameter tuning",
    "text": "Model parameter tuning\nThe model parameters cost and rbf_sigma will be tuned via a grid search of 10 values\n\nsvm_wf &lt;- workflow() %&gt;%\n  add_model(svm_spec) %&gt;%\n  add_recipe(rec)\n\nsvm_tune_folds &lt;- vfold_cv(train, strata = artist)\n\nset.seed(1)\nsvm_tune_res &lt;- tune_grid(\n  svm_wf,\n  resamples = svm_tune_folds,\n  grid = 10\n)\n\ntune_metrics &lt;- svm_tune_res %&gt;% collect_metrics()\n\ntune_metrics %&gt;%\n  filter(., .metric == 'accuracy') %&gt;%\n  ggplot(.,\n         aes(y = rbf_sigma, x = cost, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c()\n\n\n\nsvm_tune_res %&gt;% show_best(metric = 'accuracy')\n\n# A tibble: 5 √ó 8\n    cost rbf_sigma .metric  .estimator  mean     n std_err .config              \n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 3.15    2.37e- 4 accuracy binary     0.804    10 0.0150  Preprocessor1_Model04\n2 0.439   8.95e- 2 accuracy binary     0.747    10 0.00199 Preprocessor1_Model01\n3 0.0342  4.45e- 3 accuracy binary     0.747    10 0.00199 Preprocessor1_Model02\n4 0.0177  4.34e- 8 accuracy binary     0.747    10 0.00199 Preprocessor1_Model03\n5 0.0856  8.84e-10 accuracy binary     0.747    10 0.00199 Preprocessor1_Model05\n\nbest_accuracy &lt;- svm_tune_res %&gt;% select_best(., metric = 'accuracy')\n\nbest_accuracy\n\n# A tibble: 1 √ó 3\n   cost rbf_sigma .config              \n  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1  3.15  0.000237 Preprocessor1_Model04"
  },
  {
    "objectID": "posts/meta-programming/index.html",
    "href": "posts/meta-programming/index.html",
    "title": "Meta-programming with R and rlang",
    "section": "",
    "text": "rlang is a powerful R package that allows the coder the ability to write code with code. The amazing book Advanced R by Hadley Wickham goes into this idea in much greater detail, but I wanted to present a small example of my favorite function parse_expr() from the rlang package in this post.\nIn short, rlang provides functions that facilitates the coder the ability to delay evaluation of expressions. Furthermore, we can manipulate the the expressions with various tools within rlang, by piecing together various expressions.\nI will be using the data from the palmerpenguins package\n\nlibrary(palmerpenguins)\nlibrary(rlang)\n\ndata &lt;- palmerpenguins::penguins\n\ndata\n\n# A tibble: 344 √ó 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ‚Ñπ 334 more rows\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nAs a simple example, let‚Äôs say we would like to build a linear regression model for bill_length_mm, using all other variables as covariates.\n\nfit &lt;-\n  lm(\n    bill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm +\n      body_mass_g + sex + year,\n    data = data\n  )\n\nfit\n\n\nCall:\nlm(formula = bill_length_mm ~ species + island + bill_depth_mm + \n    flipper_length_mm + body_mass_g + sex + year, data = data)\n\nCoefficients:\n      (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n       -3.893e+02          9.910e+00          6.487e+00         -4.624e-01  \n  islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n       -7.327e-02          3.272e-01          5.724e-02          1.136e-03  \n          sexmale               year  \n        2.054e+00          2.023e-01  \n\n\nThis is a simple example, but what would happen if there are many many covariates we would like to include in the model. We can construct the formula as a string and then use parse_expr() to parse the string into an expression.\n\nf &lt;- paste0('bill_length_mm ~ ', paste0(names(data)[names(data) != 'bill_length_mm'], collapse = ' + '))\n\nf &lt;- parse_expr(f)\n\nf\n\nbill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm + \n    body_mass_g + sex + year\n\n\nAs an example, we can construct an expression like so, and delay the evaluation for later time.\n\nexpr(lm(\"this is where the formula should be inserted\", data = data))\n\nlm(\"this is where the formula should be inserted\", data = data)\n\n\nUsing the example from above, we can construct a new expression, and inject the previous expression into it using the !! (bang bang) operator.\n\nlm_model &lt;- expr(lm(!!f, data = data))\n\nlm_model\n\nlm(bill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm + \n    body_mass_g + sex + year, data = data)\n\n\nAs we can see, using the !! operator allows us to piece together various expressions. However, do note that the expression is still not evaluated. If we would like to evaluate the expression, we can pass the object lm_model into eval()\n\nlm_model |&gt; eval()\n\n\nCall:\nlm(formula = bill_length_mm ~ species + island + bill_depth_mm + \n    flipper_length_mm + body_mass_g + sex + year, data = data)\n\nCoefficients:\n      (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n       -3.893e+02          9.910e+00          6.487e+00         -4.624e-01  \n  islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n       -7.327e-02          3.272e-01          5.724e-02          1.136e-03  \n          sexmale               year  \n        2.054e+00          2.023e-01  \n\n\nAlthough this is a fun toy example, the ability to piece together various pieces of expression and delay evaluation is a very power functional programming tool in R."
  },
  {
    "objectID": "posts/haunted-places/index.html",
    "href": "posts/haunted-places/index.html",
    "title": "Haunted Los Angeles - Interactive Mapping using Leaflet",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(leaflet)\n\nThe following blog post is a very simple visualization using the data from tidytuesday 2023, week 41. This visualization uses the leaflet library to produce a very simple spatial analysis of the ‚Äòhaunted‚Äô locations found in Los Angeles, CA.\nWe begin by loading the data from the tidytuesdayR package.\n\ndatas &lt;- tt_load(2023, 41)\n\n\n    Downloading file 1 of 1: `haunted_places.csv`\n\ndata &lt;- datas$haunted_places\n\nNext we take a glimpse of the data to see what we are working with.\n\nglimpse(data)\n\nRows: 10,992\nColumns: 10\n$ city           &lt;chr&gt; \"Ada\", \"Addison\", \"Adrian\", \"Adrian\", \"Albion\", \"Albion‚Ä¶\n$ country        &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"Uni‚Ä¶\n$ description    &lt;chr&gt; \"Ada witch - Sometimes you can see a misty blue figure ‚Ä¶\n$ location       &lt;chr&gt; \"Ada Cemetery\", \"North Adams Rd.\", \"Ghost Trestle\", \"Si‚Ä¶\n$ state          &lt;chr&gt; \"Michigan\", \"Michigan\", \"Michigan\", \"Michigan\", \"Michig‚Ä¶\n$ state_abbrev   &lt;chr&gt; \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"‚Ä¶\n$ longitude      &lt;dbl&gt; -85.50489, -84.38184, -84.03566, -84.01757, -84.74518, ‚Ä¶\n$ latitude       &lt;dbl&gt; 42.96211, 41.97142, 41.90454, 41.90571, 42.24401, 42.23‚Ä¶\n$ city_longitude &lt;dbl&gt; -85.49548, -84.34717, -84.03717, -84.03717, -84.75303, ‚Ä¶\n$ city_latitude  &lt;dbl&gt; 42.96073, 41.98643, 41.89755, 41.89755, 42.24310, 42.24‚Ä¶\n\n\nWe next subset the data for the observations found in Los Angeles, CA. The longitude and latitude are then used to plot the locations on a map using the leaflet library. The ‚ÄòJ.F.K. Library Third‚Äô location is removed from the data as it is an outlier and is not located in Los Angeles, CA.\n\ndata |&gt;\n  filter(city %in% c('Los Angeles')) |&gt;\n  filter(state_abbrev == 'CA') |&gt;\n  filter(location != 'J.F.K. Library Third') |&gt; \n  mutate(label = glue::glue('&lt;b&gt;{location}&lt;/b&gt; &lt;br&gt; {description}')) |&gt; \n  leaflet() |&gt;\n  addTiles() |&gt;\n  addMarkers(\n    lng = ~ longitude,\n    lat = ~ latitude,\n    popup = ~ label\n  )"
  },
  {
    "objectID": "posts/aoc-2023-d1/index.html",
    "href": "posts/aoc-2023-d1/index.html",
    "title": "Advent of Code 2023, Day 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\nIt‚Äôs the season for Advent of Code 2023. Advent of code is an ‚Äòadvent calendar‚Äô of small programming puzzles that begins on 12/1 of every year and last until Christmas 12/25. The following is my solution of Day 1, Part 1 and Part 2 of the 2023 puzzle solved using R.\nWe begin by loading the puzzle input.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d1', 'puzzle-input.txt')\n)\n\ndata &lt;- as_tibble(data)\n\n\nPart 1\nThe first part of the puzzle is fairly straight forward. We begin by using the str_remove_all function from the stringr package to remove all of the characters (and leave only the numeric) from the value column. Next we use str_extract to extract the first and last digit using regex, and paste0 to paste the numbers together to get the ‚Äòcalibration‚Äô value. Finally we sum the cal column to get the solution.\n\nres &lt;- data |&gt; \n  mutate(\n    digits = str_remove_all(value, '[:alpha:]'),\n    first = str_extract(digits, '^[:digit:]'),\n    last = str_extract(digits, '[:digit:]$'),\n    cal = as.numeric(paste0(first, last))\n  ) \n\nres\n\n# A tibble: 1,000 √ó 5\n   value                                 digits first last    cal\n   &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 9vxfg                                 9      9     9        99\n 2 19qdlpmdrxone7sevennine               197    1     7        17\n 3 1dzntwofour9nineffck                  19     1     9        19\n 4 7bx8hpldgzqjheight                    78     7     8        78\n 5 joneseven2sseven64chvczzn             264    2     4        24\n 6 seven82683                            82683  8     3        83\n 7 7onefour1eighttwo5three               715    7     5        75\n 8 8lmsk871eight7                        88717  8     7        87\n 9 ninefivefive2nine5ntvscdfdsmvqgcbxxxt 25     2     5        25\n10 onepx6hbgdssfivexs                    6      6     6        66\n# ‚Ñπ 990 more rows\n\nres |&gt; \n  summarise(results = sum(cal))\n\n# A tibble: 1 √ó 1\n  results\n    &lt;dbl&gt;\n1   54390\n\n\n\n\nPart 2\nThe second part of the puzzle was extremely tricky. There are a number of special cases where the words are overlapping. My messy solution was to ‚Äòfix‚Äô those overlapping words such as ‚Äòtwone‚Äô to ‚Äòtwoone‚Äô, using the str_replace_all function, then using additional str_replace_all to convert the words to numeric. Finally we would use a similar solution as above in extracting the first and last digit.\n\nconvert_char_to_numeric &lt;- \\(x) {\n  \n  # special cases with overlapping characters\n  x &lt;- str_replace_all(x, 'twone', 'twoone') |&gt; \n    str_replace_all('oneight', 'oneeight') |&gt; \n    str_replace_all('sevenine', 'sevennine') |&gt; \n    str_replace_all('threeight', 'threeeight') |&gt; \n    str_replace_all('fiveight', 'fiveeight') |&gt; \n    str_replace_all('eightwo', 'eighttwo') |&gt; \n    str_replace_all('eighthree', 'eightthree') |&gt; \n    str_replace_all('nineight', 'nineeight')\n  \n  x &lt;- str_replace_all(x, 'one', '1') |&gt;\n    str_replace_all('two', '2') |&gt;\n    str_replace_all('three', '3') |&gt;\n    str_replace_all('four', '4') |&gt;\n    str_replace_all('five', '5') |&gt;\n    str_replace_all('six', '6') |&gt;\n    str_replace_all('seven', '7') |&gt;\n    str_replace_all('eight', '8') |&gt;\n    str_replace_all('nine', '9')\n  \n}\n\nres &lt;- data |&gt; \n  mutate(\n    value2 = convert_char_to_numeric(value),\n    digits = str_remove_all(value2, '[:alpha:]'),\n    first = str_extract(digits, '^[:digit:]'),\n    last = str_extract(digits, '[:digit:]$'),\n    cal = as.numeric(paste0(first, last))\n  ) \n\nres |&gt; summarise(results = sum(cal))\n\n# A tibble: 1 √ó 1\n  results\n    &lt;dbl&gt;\n1   54277"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 1\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaunted Los Angeles - Interactive Mapping using Leaflet\n\n\n\ntidytuesday\n\n\nleaflet\n\n\ngis\n\n\nvisualization\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) of Taylor Swift Discography\n\n\n\ntidytuesday\n\n\nvisualization\n\n\npca\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecret Santa Randomizer using ShinyLive üéÖüéÑüéÅ\n\n\n\nshinylive\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of ShinyLive in Quarto\n\n\n\nshinylive\n\n\nquarto\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of WebR in Quarto\n\n\n\nquarto\n\n\nwebr\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-programming with R and rlang\n\n\n\nprogramming\n\n\n\n\n\n\n\nMichael Luu\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically generate Quarto tabs\n\n\n\nquarto\n\n\nprogramming\n\n\n\nUtilizing purrr to programatically generate tabs to organize and present a large list of outputs\n\n\n\nMichael Luu\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieve Pubmed publications using the RefManageR package\n\n\n\nprogramming\n\n\n\n\n\n\n\nMichael Luu\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublication Quality Kaplan-Meier Survival Curves using ggplot2\n\n\n\nvisualization\n\n\nsurvival analysis\n\n\n\nThe Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate ‚Äútime to event‚Äù analysis in clinical research. In this illustrative example‚Ä¶\n\n\n\nMichael Luu\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy life in Months - Making a ‚Äòlife plot‚Äô in R using ggplot2\n\n\n\nvisualization\n\n\n\nThis blog post is inspired by Sharla Gefland twitter post found‚Ä¶\n\n\n\nMichael Luu\n\n\nOct 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating the New York Times mask utilization survey data with the R opensource Leaflet package\n\n\n\nvisualization\n\n\n\nRecreating the New York Times mask utilization survey data with the R opensource Leaflet package\n\n\n\nMichael Luu\n\n\nOct 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday\n\n\n\nnlp\n\n\nmachine learning\n\n\ntidymodels\n\n\ntidytuesday\n\n\n\nNLP and building a machine learning clasifier on Beyonce and Taylor Swift Lyrics #TidyTuesday\n\n\n\nMichael Luu\n\n\nOct 2, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Luu, MPH",
    "section": "",
    "text": "Education\n\nMPH in Biostatistics & Epidemiology, 2015\nUniversity of Southern California, Keck School of Medicine\n\n\nBSc in Biological Sciences, 2009\nUniversity of California, Irvine\n\n\n\nExperience\n\nResearch Biostatistician III\nCedars-Sinai Samuel Oschin Comprehensive Cancer Center\nNov 2016 ‚Äì Present\n\n\nResearch Biostatistician I\nChildren‚Äôs Hospital Los Angles, Anesthesia Critical Care Medicine\nMar 2016 ‚Äì Nov 2016\n\n\nQuality Improvement Analyst\nChildren‚Äôs Hospital Los Angles, Neonatology\nJul 2015 ‚Äì Nov 2016"
  },
  {
    "objectID": "posts/ggplot2-km/index.html",
    "href": "posts/ggplot2-km/index.html",
    "title": "Publication Quality Kaplan-Meier Survival Curves using ggplot2",
    "section": "",
    "text": "The Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate ‚Äútime to event‚Äù analysis in clinical research. In this illustrative example, I will be using the veterans data from the survival package to construct KM survival curves using ggplot2 and building the figure from basic geoms within the package. I will also provide examples of other publicly available packages that can facilitate in constructing KM figures that utilizes ggplot2. I believe this is a good exercise and illustrative example in potentially more advanced and little known techniques in ggplot2, as well as provide insight in the flexibility and capabilities that are available in this package.\nThe veterans data comes from a randomised trial of two treatment regimens for lung cancer. Let‚Äôs start off by loading the veterans data from the survival and have a look at the data that we are currently working with.\n\ndf &lt;- survival::veteran %&gt;% as_tibble()\n\nglimpse(df)\n\nRows: 137\nColumns: 8\n$ trt      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ celltype &lt;fct&gt; squamous, squamous, squamous, squamous, squamous, squamous, s‚Ä¶\n$ time     &lt;dbl&gt; 72, 411, 228, 126, 118, 10, 82, 110, 314, 100, 42, 8, 144, 25‚Ä¶\n$ status   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0‚Ä¶\n$ karno    &lt;dbl&gt; 60, 70, 60, 60, 70, 20, 40, 80, 50, 70, 60, 40, 30, 80, 70, 6‚Ä¶\n$ diagtime &lt;dbl&gt; 7, 5, 3, 9, 11, 5, 10, 29, 18, 6, 4, 58, 4, 9, 11, 3, 9, 2, 4‚Ä¶\n$ age      &lt;dbl&gt; 69, 64, 38, 63, 65, 49, 69, 68, 43, 70, 81, 63, 63, 52, 48, 6‚Ä¶\n$ prior    &lt;dbl&gt; 0, 10, 0, 10, 10, 0, 10, 0, 0, 0, 0, 10, 0, 10, 10, 0, 0, 0, ‚Ä¶\n\ndf\n\n# A tibble: 137 √ó 8\n     trt celltype  time status karno diagtime   age prior\n   &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 squamous    72      1    60        7    69     0\n 2     1 squamous   411      1    70        5    64    10\n 3     1 squamous   228      1    60        3    38     0\n 4     1 squamous   126      1    60        9    63    10\n 5     1 squamous   118      1    70       11    65    10\n 6     1 squamous    10      1    20        5    49     0\n 7     1 squamous    82      1    40       10    69    10\n 8     1 squamous   110      1    80       29    68     0\n 9     1 squamous   314      1    50       18    43     0\n10     1 squamous   100      0    70        6    70     0\n# ‚Ñπ 127 more rows\n\n\nThe codebook for the dataset is provided below as follows:\n\ntrt: 1=standard 2=test\ncelltype: 1=squamous, 2=smallcell, 3=adeno, 4=large\ntime: survival time (days)\nstatus: censoring status\nkarno: Karnofsky performance score (100=good)\ndiagtime: months from diagnosis to randomisation\nage: in years\nprior: prior therapy 0=no, 10=yes\n\nTo handle ‚Äòtime to event‚Äô data in R, we will first need to construct a survival object that encapsulates both the time to event information time in our dataset as well as the event/censoring variable status. We can then fit the data using the survfit() function by constructing a formula with our response variable (survival object) on the left of the ~ and the explanatory variable trt on the right. The summary() of the object from survfit() provides us the probability of survival for a given treatment over time.\n\nfit &lt;- survfit(Surv(time, status) ~ trt, data = df)\n\nsummary(fit)\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = df)\n\n                trt=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    3     69       1   0.9855  0.0144      0.95771        1.000\n    4     68       1   0.9710  0.0202      0.93223        1.000\n    7     67       1   0.9565  0.0246      0.90959        1.000\n    8     66       2   0.9275  0.0312      0.86834        0.991\n   10     64       2   0.8986  0.0363      0.83006        0.973\n   11     62       1   0.8841  0.0385      0.81165        0.963\n   12     61       2   0.8551  0.0424      0.77592        0.942\n   13     59       1   0.8406  0.0441      0.75849        0.932\n   16     58       1   0.8261  0.0456      0.74132        0.921\n   18     57       2   0.7971  0.0484      0.70764        0.898\n   20     55       1   0.7826  0.0497      0.69109        0.886\n   21     54       1   0.7681  0.0508      0.67472        0.874\n   22     53       1   0.7536  0.0519      0.65851        0.862\n   27     51       1   0.7388  0.0529      0.64208        0.850\n   30     50       1   0.7241  0.0539      0.62580        0.838\n   31     49       1   0.7093  0.0548      0.60967        0.825\n   35     48       1   0.6945  0.0556      0.59368        0.812\n   42     47       1   0.6797  0.0563      0.57782        0.800\n   51     46       1   0.6650  0.0570      0.56209        0.787\n   52     45       1   0.6502  0.0576      0.54649        0.774\n   54     44       2   0.6206  0.0587      0.51565        0.747\n   56     42       1   0.6059  0.0591      0.50040        0.734\n   59     41       1   0.5911  0.0595      0.48526        0.720\n   63     40       1   0.5763  0.0598      0.47023        0.706\n   72     39       1   0.5615  0.0601      0.45530        0.693\n   82     38       1   0.5467  0.0603      0.44049        0.679\n   92     37       1   0.5320  0.0604      0.42577        0.665\n   95     36       1   0.5172  0.0605      0.41116        0.651\n  100     34       1   0.5020  0.0606      0.39615        0.636\n  103     32       1   0.4863  0.0607      0.38070        0.621\n  105     31       1   0.4706  0.0608      0.36537        0.606\n  110     30       1   0.4549  0.0607      0.35018        0.591\n  117     29       2   0.4235  0.0605      0.32017        0.560\n  118     27       1   0.4079  0.0602      0.30537        0.545\n  122     26       1   0.3922  0.0599      0.29069        0.529\n  126     24       1   0.3758  0.0596      0.27542        0.513\n  132     23       1   0.3595  0.0592      0.26031        0.496\n  139     22       1   0.3432  0.0587      0.24535        0.480\n  143     21       1   0.3268  0.0582      0.23057        0.463\n  144     20       1   0.3105  0.0575      0.21595        0.446\n  151     19       1   0.2941  0.0568      0.20151        0.429\n  153     18       1   0.2778  0.0559      0.18725        0.412\n  156     17       1   0.2614  0.0550      0.17317        0.395\n  162     16       2   0.2288  0.0527      0.14563        0.359\n  177     14       1   0.2124  0.0514      0.13218        0.341\n  200     12       1   0.1947  0.0501      0.11761        0.322\n  216     11       1   0.1770  0.0486      0.10340        0.303\n  228     10       1   0.1593  0.0468      0.08956        0.283\n  250      9       1   0.1416  0.0448      0.07614        0.263\n  260      8       1   0.1239  0.0426      0.06318        0.243\n  278      7       1   0.1062  0.0400      0.05076        0.222\n  287      6       1   0.0885  0.0371      0.03896        0.201\n  314      5       1   0.0708  0.0336      0.02793        0.180\n  384      4       1   0.0531  0.0295      0.01788        0.158\n  392      3       1   0.0354  0.0244      0.00917        0.137\n  411      2       1   0.0177  0.0175      0.00256        0.123\n  553      1       1   0.0000     NaN           NA           NA\n\n                trt=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     68       2   0.9706  0.0205      0.93125        1.000\n    2     66       1   0.9559  0.0249      0.90830        1.000\n    7     65       2   0.9265  0.0317      0.86647        0.991\n    8     63       2   0.8971  0.0369      0.82766        0.972\n   13     61       1   0.8824  0.0391      0.80900        0.962\n   15     60       2   0.8529  0.0429      0.77278        0.941\n   18     58       1   0.8382  0.0447      0.75513        0.930\n   19     57       2   0.8088  0.0477      0.72056        0.908\n   20     55       1   0.7941  0.0490      0.70360        0.896\n   21     54       1   0.7794  0.0503      0.68684        0.884\n   24     53       2   0.7500  0.0525      0.65383        0.860\n   25     51       3   0.7059  0.0553      0.60548        0.823\n   29     48       1   0.6912  0.0560      0.58964        0.810\n   30     47       1   0.6765  0.0567      0.57394        0.797\n   31     46       1   0.6618  0.0574      0.55835        0.784\n   33     45       1   0.6471  0.0580      0.54289        0.771\n   36     44       1   0.6324  0.0585      0.52754        0.758\n   43     43       1   0.6176  0.0589      0.51230        0.745\n   44     42       1   0.6029  0.0593      0.49717        0.731\n   45     41       1   0.5882  0.0597      0.48216        0.718\n   48     40       1   0.5735  0.0600      0.46724        0.704\n   49     39       1   0.5588  0.0602      0.45244        0.690\n   51     38       2   0.5294  0.0605      0.42313        0.662\n   52     36       2   0.5000  0.0606      0.39423        0.634\n   53     34       1   0.4853  0.0606      0.37993        0.620\n   61     33       1   0.4706  0.0605      0.36573        0.606\n   73     32       1   0.4559  0.0604      0.35163        0.591\n   80     31       2   0.4265  0.0600      0.32373        0.562\n   84     28       1   0.4112  0.0597      0.30935        0.547\n   87     27       1   0.3960  0.0594      0.29509        0.531\n   90     25       1   0.3802  0.0591      0.28028        0.516\n   95     24       1   0.3643  0.0587      0.26560        0.500\n   99     23       2   0.3326  0.0578      0.23670        0.467\n  111     20       2   0.2994  0.0566      0.20673        0.434\n  112     18       1   0.2827  0.0558      0.19203        0.416\n  133     17       1   0.2661  0.0550      0.17754        0.399\n  140     16       1   0.2495  0.0540      0.16326        0.381\n  164     15       1   0.2329  0.0529      0.14920        0.363\n  186     14       1   0.2162  0.0517      0.13538        0.345\n  201     13       1   0.1996  0.0503      0.12181        0.327\n  231     12       1   0.1830  0.0488      0.10851        0.308\n  242     10       1   0.1647  0.0472      0.09389        0.289\n  283      9       1   0.1464  0.0454      0.07973        0.269\n  340      8       1   0.1281  0.0432      0.06609        0.248\n  357      7       1   0.1098  0.0407      0.05304        0.227\n  378      6       1   0.0915  0.0378      0.04067        0.206\n  389      5       1   0.0732  0.0344      0.02912        0.184\n  467      4       1   0.0549  0.0303      0.01861        0.162\n  587      3       1   0.0366  0.0251      0.00953        0.140\n  991      2       1   0.0183  0.0180      0.00265        0.126\n  999      1       1   0.0000     NaN           NA           NA\n\n\nThe base R plotting method provides us with a basic KM figure. We can generate the figure by using the plot() function on the fit object.\n\nplot(fit)\n\n\n\n\nThe GGally package also includes ggsurv() which actually uses the ggplot2 in the backend to construct the figure.\n\nGGally::ggsurv(fit)\n\n\n\n\nAn even further improved KM figure comes from the survminer package that includes a ‚ÄòNumber at risk‚Äô table that is commonly show in combination with the KM figure.\n\nsurvminer::ggsurvplot(fit, data = df, risk.table = T)\n\n\n\n\nThe KM figure I‚Äôm constructing is going to be based on the survminer figure, that includes the secondary ‚ÄòNumber at risk‚Äô table.\nWe can start by estimating the survival estimates from day 0 to day 500, I chose 500 since it appears that survival trails off after 500 days and this is a method of truncating the figure. Then we can extract the survival estimates into a a structured tidy tibble.\n\ns &lt;- summary(fit, times = seq(0, 500, 1), extend = T)\n\nplot_data &lt;- tibble(\n  'time' = s$time,\n  'n.risk' = s$n.risk,\n  'n.event' = s$n.event,\n  'n.censor' = s$n.censor,\n  'estimate' = s$surv,\n  'std.error' = s$std.err,\n  'strata' = s$strata\n)\n\nNow that we have the ‚Äòtidied‚Äô data, we can start by constructing the base plot we will use to build from. We will map the x axis to time, the y axis to estimate, and the fill to strata.\n\np &lt;- ggplot(plot_data, aes(x = time, y = estimate, color = strata))\n\np\n\n\n\n\nThe primary geom in building the figure is geom_step()\n\np &lt;- p + geom_step(aes(linetype = strata), size = 1)\n\np\n\n\n\n\nThis is pretty close to the plot that is provided from the GGally package already, we just need a few more steps to further clean up the axes, adjust the aesthetics, and to add a theme. I also further expanded the x axes to 550 to provide some additional room for curve annotations.\n\np &lt;- p + \n  scale_x_continuous(breaks = seq(0, 500, 50)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_classic() +\n  theme(legend.position = 'top',\n        axis.title = element_text(face = 'bold')) +\n  labs(x = 'Days', y = 'Probability of Survival') + \n  coord_cartesian(xlim = c(0, 550)) + \n  ggsci::scale_color_d3()\n\n\np\n\n\n\n\nWe can further supplement this figure by adding markers on the curves using the ggrepel package. The simplest method I found to identify the coordinates of the ideal location of the annotations is by taking the last point of the curves by strata. Then we can use the geom_text_repel() function to add the text label to the curves accordingly. Now that we have the annotations on the figure, we can remove the legends to give the actual figure some additional room.\n\nannotate_data &lt;- plot_data %&gt;%\n  group_by(strata) %&gt;%\n  slice_tail(n = 1)\n\np &lt;- p + \n  ggrepel::geom_text_repel(data = annotate_data, aes(x = time, y = estimate, label = strata),\n                           xlim = c(500, NA)) + \n  theme(legend.position = 'none')\n\np\n\n\n\n\nNext, we can construct the ‚ÄòAt risk‚Äô table below the figure we just constructed. The table is actually a ggplot, where we are constructing a table of number of at risk plotted by time on the x axis and strata on the y axis. Since the time interval for the KM figure is per every 50 days, we will extract the ‚ÄòAt risk‚Äô data similarly on a per 100 days basis. The most important concept to remember is to make the scale of the x axis scale_x_continuous() is identical to the KM figure to have the alignment match between the two. The number at risk is then plotted using geom_text() with n.risk as the label.\n\ntable_data &lt;- plot_data %&gt;% \n  filter(\n    time %in% seq(0, 500, 50)\n  ) \n\nt &lt;- ggplot(table_data, aes(y = fct_rev(strata), x = time)) + \n  geom_text(aes(label = n.risk)) + \n  scale_x_continuous(breaks = seq(0, 500, 50), limits = c(0, 550))\n\nt\n\n\n\n\nNow that we have a basis of the plot for the table, we can further customize it by adding a theme, and then further clean up the axes and the labels of the figure.\n\nt &lt;- t + \n  theme(panel.background = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        axis.text = element_text(face = 'bold'))\n\nt\n\n\n\n\nWe now have 2 ggplot objects, p and t. The patchwork package is the ‚Äòglue‚Äô we need to put the two plots together.\n\nlibrary(patchwork)\n\nkm &lt;- (p / t) + plot_layout(height = c(1, .25))\n\nkm\n\n\n\n\nNow we have the final figure! I hope this post was informative in the possibilities with ggplot2. The benefits of making this figure from scratch as opposed to the packages available are the ability to further customize the figure to meet your needs.\n\nSession info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22621)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] patchwork_1.1.3 lubridate_1.9.3 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.3     purrr_1.0.2     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.4   tidyverse_2.0.0 survival_3.5-5 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4       xfun_0.41          ggrepel_0.9.4      rstatix_0.7.2     \n [5] GGally_2.1.2       lattice_0.21-8     tzdb_0.4.0         vctrs_0.6.4       \n [9] tools_4.3.1        generics_0.1.3     fansi_1.0.5        pkgconfig_2.0.3   \n[13] Matrix_1.5-4.1     data.table_1.14.8  RColorBrewer_1.1-3 lifecycle_1.0.4   \n[17] compiler_4.3.1     farver_2.1.1       textshaping_0.3.7  munsell_0.5.0     \n[21] ggsci_3.0.0        carData_3.0-5      htmltools_0.5.7    yaml_2.3.7        \n[25] pillar_1.9.0       car_3.1-2          ggpubr_0.6.0       survminer_0.4.9   \n[29] abind_1.4-5        km.ci_0.5-6        commonmark_1.9.0   tidyselect_1.2.0  \n[33] digest_0.6.33      stringi_1.7.12     labeling_0.4.3     splines_4.3.1     \n[37] fastmap_1.1.1      grid_4.3.1         colorspace_2.1-0   cli_3.6.1         \n[41] magrittr_2.0.3     utf8_1.2.4         broom_1.0.5        withr_2.5.2       \n[45] scales_1.2.1       backports_1.4.1    timechange_0.2.0   rmarkdown_2.25    \n[49] ggtext_0.1.2       gridExtra_2.3      ggsignif_0.6.4     ragg_1.2.6        \n[53] zoo_1.8-12         hms_1.1.3          evaluate_0.23      knitr_1.45        \n[57] KMsurv_0.1-5       markdown_1.11      survMisc_0.5.6     rlang_1.1.2       \n[61] gridtext_0.1.5     Rcpp_1.0.11        xtable_1.8-4       glue_1.6.2        \n[65] xml2_1.3.5         renv_0.16.0        rstudioapi_0.15.0  reshape_0.8.9     \n[69] jsonlite_1.8.7     R6_2.5.1           plyr_1.8.9         systemfonts_1.0.5"
  },
  {
    "objectID": "posts/leaflet-covid19-mask-usage/index.html",
    "href": "posts/leaflet-covid19-mask-usage/index.html",
    "title": "Recreating the New York Times mask utilization survey data with the R opensource Leaflet package",
    "section": "",
    "text": "We‚Äôre going to recreate the NY Times mask-use survey data using R and the leaflet open source interactive mapping package. We can start off by loading the data from the New York Times github repository found here\n\nurl &lt;- 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/mask-use/mask-use-by-county.csv'\n\ndf &lt;- read_csv(url)\n\nNow that we have the data loaded, lets have a look at the data to see what we‚Äôre working with\n\nglimpse(df)\n\nRows: 3,142\nColumns: 6\n$ COUNTYFP   &lt;chr&gt; \"01001\", \"01003\", \"01005\", \"01007\", \"01009\", \"01011\", \"0101‚Ä¶\n$ NEVER      &lt;dbl&gt; 0.053, 0.083, 0.067, 0.020, 0.053, 0.031, 0.102, 0.152, 0.1‚Ä¶\n$ RARELY     &lt;dbl&gt; 0.074, 0.059, 0.121, 0.034, 0.114, 0.040, 0.053, 0.108, 0.0‚Ä¶\n$ SOMETIMES  &lt;dbl&gt; 0.134, 0.098, 0.120, 0.096, 0.180, 0.144, 0.257, 0.130, 0.1‚Ä¶\n$ FREQUENTLY &lt;dbl&gt; 0.295, 0.323, 0.201, 0.278, 0.194, 0.286, 0.137, 0.167, 0.1‚Ä¶\n$ ALWAYS     &lt;dbl&gt; 0.444, 0.436, 0.491, 0.572, 0.459, 0.500, 0.451, 0.442, 0.5‚Ä¶\n\ndf\n\n# A tibble: 3,142 √ó 6\n   COUNTYFP NEVER RARELY SOMETIMES FREQUENTLY ALWAYS\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 01001    0.053  0.074     0.134      0.295  0.444\n 2 01003    0.083  0.059     0.098      0.323  0.436\n 3 01005    0.067  0.121     0.12       0.201  0.491\n 4 01007    0.02   0.034     0.096      0.278  0.572\n 5 01009    0.053  0.114     0.18       0.194  0.459\n 6 01011    0.031  0.04      0.144      0.286  0.5  \n 7 01013    0.102  0.053     0.257      0.137  0.451\n 8 01015    0.152  0.108     0.13       0.167  0.442\n 9 01017    0.117  0.037     0.15       0.136  0.56 \n10 01019    0.135  0.027     0.161      0.158  0.52 \n# ‚Ñπ 3,132 more rows\n\n\nAccording to the repository, the definitions of the variables are as follows:\n\nCOUNTYFP: The county FIPS code.\nNEVER: The estimated share of people in this county who would say never in response to the question ‚ÄúHow often do you wear a mask in public when you expect to be within six feet of another person?‚Äù\nRARELY: The estimated share of people in this county who would say rarely\nSOMETIMES: The estimated share of people in this county who would say sometimes\nFREQUENTLY: The estimated share of people in this county who would say frequently\nALWAYS: The estimated share of people in this county who would say always\n\nThey are also plotting the probability of encountering a mask usage among 5 random encounters in the county.\n\nThe chance all five people are wearing masks in five random encounters is calculated by assuming that survey respondents who answered ‚ÄòAlways‚Äô were wearing masks all of the time, those who answered ‚ÄòFrequently‚Äô were wearing masks 80 percent of the time, those who answered ‚ÄòSometimes‚Äô were wearing masks 50 percent of the time, those who answered ‚ÄòRarely‚Äô were wearing masks 20 percent of the time and those who answered ‚ÄòNever‚Äô were wearing masks none of the time.\n\nWe can calculate this simply by using the supplied weights (1, .8, .5, .2, and 0) among ALWAYS, FREQUENTLY, SOMETIMES, RARELY, and NEVER mask usage, and taking the sum of the proportion of mask usage among all 5 different types of individuals that have equal probability of encountering.\n\ndf &lt;- df %&gt;%\n  mutate(\n    prob = ((ALWAYS * 1) + (FREQUENTLY * .8) + (SOMETIMES * .5) + (RARELY * .2) + (NEVER * 0)) \n  )\n\nSince we have the county FIPS code data available, we‚Äôll need to merge this data with county geojson data for the United States which I was able to obtain from here\n\ncounties &lt;- rgdal::readOGR('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_5m.json')\n\nOGR data source with driver: GeoJSON \nSource: \"https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_5m.json\", layer: \"gz_2010_us_050_00_5m\"\nwith 3221 features\nIt has 6 fields\n\n\nAfter reading in the US counties data, we can merge the mask usage survey data with the geojson file, by the state and FIPS code. We can create a COUNTYFP variable by pasting together the STATE and COUNTY code\n\ncounties@data &lt;- counties@data %&gt;%\n  mutate(\n    COUNTYFP = paste0(STATE, COUNTY)\n  ) %&gt;%\n  left_join(\n    df\n  )\n\nFurthermore after merging the data, we can create a label by merging together the % mask usage data into a HTML string\n\ncounties@data &lt;- counties@data %&gt;%\n  mutate(\n    label = glue::glue(\n      '&lt;b&gt;{NAME}&lt;/b&gt;&lt;br&gt;\n      {paste0(format(round(NEVER*100, 1), 1), \"%\")} estimated NEVER wear a mask &lt;br&gt;\n      {paste0(format(round(RARELY*100, 1), 1), \"%\")} estimated RARELY wear a mask &lt;br&gt;\n      {paste0(format(round(SOMETIMES*100, 1), 1), \"%\")} estimated SOMETIMES wear a mask &lt;br&gt;\n      {paste0(format(round(FREQUENTLY*100, 1), 1), \"%\")} estimated FREQUENTLY wear a mask &lt;br&gt;\n      {paste0(format(round(ALWAYS*100, 1), 1), \"%\")} estimated ALWAYS wear a mask &lt;br&gt;&lt;br&gt;\n      This translates to a &lt;b&gt;{paste0(format(round(prob*100, 1), 1), \"%\")}&lt;/b&gt; chance that everyone is masked in five random encounters'\n    ),\n    label = map(label, ~ htmltools::HTML(.x))\n  )\n\nFinally, let‚Äôs put this all together and create a Chloropleth map using the leaflet package\n\ncolor_pal &lt;- colorNumeric('plasma', counties$prob)\n\nmap &lt;- leaflet(counties) %&gt;%\n  addTiles() %&gt;%\n  fitBounds(\n    lng1 = -131.519605,\n    lng2 = -64.312607,\n    lat1 = 50.623510,\n    lat2 = 23.415249\n  ) %&gt;%\n  addPolygons(\n    fillColor = ~ color_pal(prob),\n    fillOpacity = .75,\n    weight = 1,\n    color = 'white',\n    label = ~ label,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    position = 'bottomright',\n    pal = color_pal,\n    values = ~ counties$prob,\n    title = '% Mask Usage',\n    labFormat = labelFormat(\n      suffix = '%',\n      transform = function(x)\n        x * 100\n    )\n  )\n\nmap\n\n\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] leaflet_2.1.2   lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.1     purrr_1.0.1     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   xfun_0.38          lattice_0.20-45    colorspace_2.1-0  \n [5] vctrs_0.6.1        generics_0.1.3     viridisLite_0.4.1  htmltools_0.5.4   \n [9] yaml_2.3.6         utf8_1.2.3         rlang_1.1.0        pillar_1.9.0      \n[13] glue_1.6.2         withr_2.5.0        RColorBrewer_1.1-3 sp_1.6-0          \n[17] bit64_4.0.5        lifecycle_1.0.3    munsell_0.5.0      gtable_0.3.3      \n[21] htmlwidgets_1.6.2  evaluate_0.19      knitr_1.41         tzdb_0.3.0        \n[25] fastmap_1.1.0      crosstalk_1.2.0    parallel_4.2.2     curl_5.0.0        \n[29] fansi_1.0.4        renv_0.16.0        scales_1.2.1       vroom_1.6.1       \n[33] jsonlite_1.8.4     farver_2.1.1       bit_4.0.5          gridExtra_2.3     \n[37] hms_1.1.3          digest_0.6.31      stringi_1.7.8      grid_4.2.2        \n[41] rgdal_1.6-5        cli_3.6.1          tools_4.2.2        magrittr_2.0.3    \n[45] crayon_1.5.2       pkgconfig_2.0.3    ellipsis_0.3.2     timechange_0.2.0  \n[49] viridis_0.6.2      rmarkdown_2.19     rstudioapi_0.14    R6_2.5.1          \n[53] compiler_4.2.2"
  },
  {
    "objectID": "posts/my-life-in-months/index.html",
    "href": "posts/my-life-in-months/index.html",
    "title": "My life in Months - Making a ‚Äòlife plot‚Äô in R using ggplot2",
    "section": "",
    "text": "This blog post is inspired by Sharla Gefland twitter post found here, where she made a ‚ÄòMy Life in Months‚Äô plot.\nAnnotations have always been the bane of my existence in ggplot2, and I figured this would be a fun project to get some practice. Looking at her github repo found here, she made this figure using the waffle plot package found here. Although using the waffle package may simplify some aspects of making this figure, recreating this figure in pure ggplot2 will open up the arguments for further customization that may not be available via the waffle package.\nWe can start off by creating a tibble for the basis of the plot. The goal here is to create a tibble starting from the starting month (month/year) I was born, until the current month/year. I can create this with the help of the lubridate package, which simplifies the handling of dates in R, and using this package to further extract the month and year information from the date sequence.\n\ndf &lt;- tibble(\n  date = seq(mdy('9/1/1987'), floor_date(Sys.Date(), 'month'), 'month')\n) %&gt;%\n  mutate(\n    month = month(date),\n    year = year(date)\n  )\n\ndf &lt;- tibble(date = seq(mdy('9/1/1987'), Sys.Date(), '1 month')) %&gt;%\n  mutate(month = month(date),\n         year = year(date))\n\ndf\n\n# A tibble: 428 √ó 3\n   date       month  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1987-09-01     9  1987\n 2 1987-10-01    10  1987\n 3 1987-11-01    11  1987\n 4 1987-12-01    12  1987\n 5 1988-01-01     1  1988\n 6 1988-02-01     2  1988\n 7 1988-03-01     3  1988\n 8 1988-04-01     4  1988\n 9 1988-05-01     5  1988\n10 1988-06-01     6  1988\n# ‚Ñπ 418 more rows\n\n\nUsing the tibble I just created, I can further define the ‚Äòeras‚Äô that I would like to highlight in the life plot.\n\nplot_data &lt;- df %&gt;%\n  mutate(\n    era = case_when(\n      date %in% mdy('9/1/1987'):mdy(\"9/1/1991\") ~ 'Childhood',\n      date %in% mdy('10/1/1991'):mdy('6/1/2005') ~ 'K-12 Grade School',\n      date %in% mdy('7/1/2005'):mdy('12/1/2009') ~ 'BSc in Biological Sciences',\n      date %in% mdy('1/1/2010'):mdy('7/1/2013') ~ 'Pre Graduate Work',\n      date %in% mdy('8/1/2013'):mdy('6/1/2015') ~ 'MPH in Biostatistics & Epidemiology',\n      date %in% mdy('7/1/2015'):mdy('8/1/2016') ~ 'Data Analyst',\n      date %in% mdy('9/1/2016'):Sys.Date() ~ 'Biostatistician'\n    )\n  ) %&gt;%\n  mutate(era = factor(\n    era,\n    levels = c(\n      'Childhood',\n      'K-12 Grade School',\n      'BSc in Biological Sciences',\n      'Pre Graduate Work',\n      'MPH in Biostatistics & Epidemiology',\n      'Data Analyst',\n      'Biostatistician'\n    )\n  ))\n\nplot_data\n\n# A tibble: 428 √ó 4\n   date       month  year era      \n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n 1 1987-09-01     9  1987 Childhood\n 2 1987-10-01    10  1987 Childhood\n 3 1987-11-01    11  1987 Childhood\n 4 1987-12-01    12  1987 Childhood\n 5 1988-01-01     1  1988 Childhood\n 6 1988-02-01     2  1988 Childhood\n 7 1988-03-01     3  1988 Childhood\n 8 1988-04-01     4  1988 Childhood\n 9 1988-05-01     5  1988 Childhood\n10 1988-06-01     6  1988 Childhood\n# ‚Ñπ 418 more rows\n\n\nNext, I‚Äôll create a base plot using ggplot2, where I‚Äôll map the x axis to year, and the y axis to month. I‚Äôll also use the geom, geom_tile() to create the ‚Äòblocks‚Äô that we see in the life plot, where we‚Äôll map the fill to era.\n\n ggplot(plot_data, aes(y = month, x = year)) + \n  geom_tile(color = 'white', aes(fill = era), size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\nNow that we have a simple base plot to work with, we can further customize and clean up the figure. A trick to give us a bigger ‚Äòspace‚Äô to work with is to expand the limits of the y and x axis. Furthermore, I will use scale_fill_d3() to add a fill theme to the plot.\n\nbase_plot &lt;- ggplot(plot_data, aes(y = month, x = year)) + \n  geom_tile(color = 'white', aes(fill = era), size = 1) + \n  scale_y_continuous(breaks = -6:18, limits = c(-6, 18)) +\n  scale_x_continuous(breaks = 1980:2020) +\n  labs(y = 'Month', x = 'Year') + \n  theme(legend.position = 'bottom') + \n  scale_fill_d3()\n  \n\nbase_plot\n\n\n\n\nAnnotations have always been tricky, because we have to specifically define the coordinates of the annotations we are trying to add. I‚Äôm going to start off small with a small annotation on the top left corner with an arrow point to the top left square. The segments are created using the geom_curve() and the text annotations are created using annotate() via geom_text()\n\n## annotate the definition of 1 square = 1 month\nplot &lt;- base_plot +\n  geom_curve(\n    x = 1987,\n    y = 12,\n    xend = 1986,\n    yend = 14,\n    curvature = -.4,\n    arrow = arrow(length = unit(0.01, \"npc\"), ends = 'first'),\n    color = 'black'\n  ) + \n  annotate(\n    'text',\n    x = 1985,\n    y = 15,\n    hjust = 0,\n    label = '1 square = 1 month',\n    family = \"Segoe Script\"\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNext I‚Äôll start to map out exactly where I want each of the labels for the eras to be placed. This definitely took a while, and it helps if you have some forethought on where you want to place the labels.\n\n### set colors \npallete_colors &lt;- pal_d3(\"category10\")(10)\n\n## set size\nannotation_size &lt;- 5\n\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1989,\n    y = -1,\n    label = 'Childhood',\n    color = pallete_colors[[1]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  )  +\n  annotate(\n    'text',\n    x = 1998,\n    y = -1,\n    label = 'K-12 Grade School',\n    color = pallete_colors[[2]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2007.5,\n    y = -1,\n    label = 'BSc in Biological Sciences',\n    color = pallete_colors[[3]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2011,\n    y = 14,\n    label = 'Pre Graduate Employment',\n    color = pallete_colors[[4]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2013,\n    y = -3,\n    label = 'MPH in Biostatistics & Epidemiology',\n    color = pallete_colors[[5]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2012.5,\n    y = 16,\n    label = 'Data Analyst',\n    color = pallete_colors[[6]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2018.5,\n    y = -1,\n    label = 'Biostatistician',\n    color = pallete_colors[[7]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNow that we have the text placed in all the designated coordinates, we can start working on the arrows.\n\n## add additional curve segments for labels\n\nplot &lt;- plot + \n  geom_curve(\n    x = 1989,\n    y = 1,\n    xend = 1989,\n    yend = -.5,\n    curvature = .2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 1998,\n    y = 1,\n    xend = 1998,\n    yend = -.5,\n    curvature = .2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2007,\n    y = 1,\n    xend = 2007,\n    yend = -.5,\n    curvature = -.2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2011,\n    y = 12,\n    xend = 2011,\n    yend = 13.5,\n    curvature = -.2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2015,\n    y = 12,\n    xend =  2015,\n    yend = 16,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black',\n    curvature = .8\n  ) +\n  geom_curve(\n    x = 2014,\n    y = 1,\n    xend =  2014,\n    yend = -2.5,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    curvature = -0.2,\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2018,\n    y = 1,\n    xend =  2018,\n    yend = -0.5,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    curvature = -0.2,\n    color = 'black'\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNow that we have most of the annotations on there, we can add some supplemental annotations, e.g.¬†adding an annotations regarding each column is 1 year, and the segments to finish off the look.\n\n## let's add a label for 1 column equals 1 year of age \n\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1985,\n    y = 6,\n    label = '1 year',\n    angle = 90,\n    size = 7,\n    color = 'black',\n    family = \"Segoe Script\"\n  ) + \n  annotate(\n    'text',\n    x = 1988,\n    y = 13,\n    label = 'age',\n    size = 5,\n    color = 'black',\n    family = \"Segoe Script\"\n  ) +\n  geom_segment(\n    x = 1988.75,\n    y = 13,\n    xend = 1993,\n    yend = 13,\n    arrow = arrow(ends = 'last', length = unit(.01, units = 'npc')),\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1985,\n    xend = 1985,\n    y = 8,\n    yend = 12,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1985,\n    xend = 1985,\n    y = 1,\n    yend = 4,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1984.5,\n    xend = 1985.5,\n    y = 12,\n    yend = 12,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1984.5,\n    xend = 1985.5,\n    y = 1,\n    yend = 1,\n    color = 'black'\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nWe‚Äôre almost there - now that we have all the annotations we want on there, we can remove the legend and use a theme to further remove the grid as well as the x and y axis.\n\nplot &lt;- plot +\n  theme_void() +\n  theme(\n    legend.position = 'none'\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nLet‚Äôs finish off this off by adding a title\n\n## lets add a title\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1987,\n    y = -5,\n    label = 'Michael Luu',\n    size = 25,\n    hjust = 0,\n    fontface = 'bold.italic'\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nFull resolution figure can be found here along with the github repo for the full code here\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] extrafont_0.19  ggsci_3.0.0     lubridate_1.9.2 forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.1     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] compiler_4.2.2    pillar_1.9.0      tools_4.2.2       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   yaml_2.3.6        xfun_0.38         fastmap_1.1.0    \n[17] Rttf2pt1_1.3.12   withr_2.5.0       knitr_1.41        systemfonts_1.0.4\n[21] hms_1.1.3         generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[25] grid_4.2.2        tidyselect_1.2.0  glue_1.6.2        R6_2.5.1         \n[29] textshaping_0.3.6 fansi_1.0.4       rmarkdown_2.19    farver_2.1.1     \n[33] extrafontdb_1.0   tzdb_0.3.0        magrittr_2.0.3    scales_1.2.1     \n[37] htmltools_0.5.4   colorspace_2.1-0  renv_0.16.0       ragg_1.2.5       \n[41] labeling_0.4.2    utf8_1.2.3        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/quarto-generate-tabs/index.html",
    "href": "posts/quarto-generate-tabs/index.html",
    "title": "Programatically generate Quarto tabs",
    "section": "",
    "text": "When working with a list of objects, it may be useful to organize the objects into tabs instead of a huge list of individual objects\nUsing the iris dataset as a working example, I generate a list of ggplot objects.\n\ndata &lt;- iris |&gt; as_tibble() |&gt; janitor::clean_names()\n\nout &lt;- data |&gt; \n  group_nest(species) |&gt; \n  deframe()\n\nout &lt;- out |&gt; \n  map(\\(data) {\n    \n    ggplot(data, aes(x = sepal_length, y = sepal_width)) + \n      geom_point()\n    \n  })\n\nThe list of ggplot2 objects can be called and presented as below\n\nout\n\n$setosa\n\n\n\n\n\n\n$versicolor\n\n\n\n\n\n\n$virginica\n\n\n\n\n\nInstead of presenting a long list of plots, we can organize the plots into individual tabs. In order to do this, we utilize a combination of imap_chr() and knit_child(). We use imap_chr() to pass on individual plots into knit_child(). We wrap this chunk within a fenced div panel-tabset, and utilize results: asis.\n\n```{r}\n#| eval: false\n\nout &lt;- imap_chr(out, \\(out, title) {\n  \n  text &lt;- glue::glue(\"## `r title`\",\n                     \"```{r}\",\n                     \"out\",\n                     \"```\",\n                     \"\", .sep = '\\n\\n')\n  \n  knitr::knit_child(text = text, envir = environment(),\n                    quiet = T)\n  \n})\n\ncat(out, sep = '\\n')\n```\n\n\nsetosaversicolorvirginica\n\n\n\nout\n\n\n\n\n\n\n\nout\n\n\n\n\n\n\n\nout"
  },
  {
    "objectID": "posts/quarto-webr/index.html",
    "href": "posts/quarto-webr/index.html",
    "title": "Implementation of WebR in Quarto",
    "section": "",
    "text": "WebR is a web implementation of the R statistical software in a web browser built using the Node.js and WebAssembly framework. The official documentation will have a better explanation of how WebR works. However, the gist of WebR is the ability to run R code directly in the web browser, without the need of installing R or running code on a server.\nThe purpose of this blog post is to provide a baseline example implementation of WebR in Quarto. The example instructions provided here is based on the the following github example by James J Balamuta, the developer of the webr quarto extension.\nThe below code chunk will construct a section of the rendered quarto document where R code can be ran directly in the browser. Although the development of WebR is still early, the ability to run R code directly in the browser, and without the need of a server or additional infrastructure is a very power tool.\nSome of the current limitations of WebR is speed, and the need to ‚Äòinstall‚Äô packages to prime the WebR environment. I can only imagine that issue with speed will only improve in due time. Packages will also need to be specifically compiled for WebAssembly. The current default repository for R packages compiled for WebR can be found here.\nBefore a specific R package can be utilized, we will need to ‚Äòinstall‚Äô the WebR compiled version of the package in a WebR environment like so.\n\n  üü° Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\nOnce the WebR package has been installed, we can load the library as we normally would.\n\n  üü° Loading\n    webR..."
  },
  {
    "objectID": "posts/secret-santa-shinylive/index.html",
    "href": "posts/secret-santa-shinylive/index.html",
    "title": "Secret Santa Randomizer using ShinyLive üéÖüéÑüéÅ",
    "section": "",
    "text": "It‚Äôs the time of year for giving üéÖüéÑüéÅ\nThe purpose of this post is to showcase a simple Secret Santa Randomizer built using Shiny, the bslib dashboard framework, and ShinyLive (a server-less Shiny implementation).\nThe source code for this application can be found here."
  }
]