[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Complete bib file can be downloaded here."
  },
  {
    "objectID": "posts/taylor-swift-pca/index.html",
    "href": "posts/taylor-swift-pca/index.html",
    "title": "Principal Component Analysis (PCA) of Taylor Swift Discography",
    "section": "",
    "text": "This is a quick PCA of Taylor Swift’s discography using the dataset from the tidytuesday 2023, week 42. PCA is a technique used to reduce high dimensional data into principal components. This also allows us the ability to project and visualize high dimensional data in two dimensional space via a scatter plot.\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(tidymodels)\nlibrary(plotly)\nlibrary(rlang)\nlibrary(tidytext)\n\ntheme_set(\n  theme_minimal(\n    base_size = 15\n  )\n)\n\nWe start off with loading the data using the tidytuesdayR package. We are also going to download the data using the tt_download function. The specific dataset we are using is the taylor_all_songs dataset.\n\ndatas &lt;- tt_load_gh(2023, 42)\n\ndatas &lt;- tt_download(datas)\n\n\n    Downloading file 1 of 3: `taylor_album_songs.csv`\n    Downloading file 2 of 3: `taylor_all_songs.csv`\n    Downloading file 3 of 3: `taylor_albums.csv`\n\ndata &lt;- datas$taylor_all_songs\n\nNext we are going to take a quick glimpse at the data to see what we are working with.\n\nglimpse(data)\n\nRows: 274\nColumns: 29\n$ album_name          &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ ep                  &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ album_release       &lt;date&gt; 2006-10-24, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ track_number        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ track_name          &lt;chr&gt; \"Tim McGraw\", \"Picture To Burn\", \"Teardrops On My …\n$ artist              &lt;chr&gt; \"Taylor Swift\", \"Taylor Swift\", \"Taylor Swift\", \"T…\n$ featuring           &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ bonus_track         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ promotional_release &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ single_release      &lt;date&gt; 2006-06-19, 2008-02-03, 2007-02-19, NA, NA, NA, N…\n$ track_release       &lt;date&gt; 2006-06-19, 2006-10-24, 2006-10-24, 2006-10-24, 2…\n$ danceability        &lt;dbl&gt; 0.580, 0.658, 0.621, 0.576, 0.418, 0.589, 0.479, 0…\n$ energy              &lt;dbl&gt; 0.491, 0.877, 0.417, 0.777, 0.482, 0.805, 0.578, 0…\n$ key                 &lt;dbl&gt; 0, 7, 10, 9, 5, 5, 2, 8, 4, 2, 2, 8, 7, 4, 10, 5, …\n$ loudness            &lt;dbl&gt; -6.462, -2.098, -6.941, -2.881, -5.769, -4.055, -4…\n$ mode                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ speechiness         &lt;dbl&gt; 0.0251, 0.0323, 0.0231, 0.0324, 0.0266, 0.0293, 0.…\n$ acousticness        &lt;dbl&gt; 0.57500, 0.17300, 0.28800, 0.05100, 0.21700, 0.004…\n$ instrumentalness    &lt;dbl&gt; 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, …\n$ liveness            &lt;dbl&gt; 0.1210, 0.0962, 0.1190, 0.3200, 0.1230, 0.2400, 0.…\n$ valence             &lt;dbl&gt; 0.425, 0.821, 0.289, 0.428, 0.261, 0.591, 0.192, 0…\n$ tempo               &lt;dbl&gt; 76.009, 105.586, 99.953, 115.028, 175.558, 112.982…\n$ time_signature      &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ duration_ms         &lt;dbl&gt; 232107, 173067, 203040, 199200, 239013, 207107, 24…\n$ explicit            &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ key_name            &lt;chr&gt; \"C\", \"G\", \"A#\", \"A\", \"F\", \"F\", \"D\", \"G#\", \"E\", \"D\"…\n$ mode_name           &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"major\", \"majo…\n$ key_mode            &lt;chr&gt; \"C major\", \"G major\", \"A# major\", \"A major\", \"F ma…\n$ lyrics              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nWe are now going to select the columns to use for PCA. We are going to extract the columns containing various attributes of the songs. We are going to use the track_name column as the response variable and the rest of the song attribute columns as the predictors. Any rows with missing values are going to be dropped from the analysis.\nPCA is done using the recipes package. The first step is to create a recipe using the recipe function. The recipe function takes in a formula and a dataset. The LHS of the formula is the response variable and the RHS is the predictors. The next step is to normalize the data using the step_normalize function. This function centers and scales the numeric predictors. The next step is actually performing PCA using the step_pca function. The step_pca function takes in the columns that we want to use for the PCA, and we also defines the number of components that we want to extract. The last step is to prep the recipe using the prep function.\n\ndata &lt;- data |&gt;\n  select(\n    track_name,\n    danceability,\n    energy,\n    key,\n    loudness,\n    speechiness,\n    acousticness,\n    instrumentalness,\n    liveness,\n    valence,\n    tempo,\n    duration_ms\n  ) |&gt; drop_na()\n\nrec &lt;- recipe(track_name ~ ., data = data) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  step_pca(all_numeric_predictors(), num_comp = 5) |&gt; \n  prep()\n\nAfter conducting PCA, we are going to take a look at the proportion of variances that are explained by each component. We can see that the first component explains 26% of the variance, the second component explains 16% of the variance, the third component explains 11% of the variance. The first two component explains 42% of the cumulative variances.\n\nlocal({\n  pca &lt;- rec$steps[[2]]$res |&gt; summary()\n  \n  plot_data &lt;- pca$importance |&gt; as_tibble(rownames = 'type')\n  \n  plot &lt;- plot_data |&gt;\n    pivot_longer(2:ncol(plot_data)) |&gt;\n    filter(type == 'Proportion of Variance') |&gt;\n    mutate(name = as_factor(name)) |&gt;\n    ggplot(aes(x = name, y = value)) +\n    geom_col() +\n    labs(y = 'Proportion of Variance', x = NULL) +\n    scale_y_continuous(labels = scales::label_percent()) +\n    geom_text(\n      aes(label = scales::label_percent(1)(value)),\n      vjust = 0,\n      nudge_y = .001,\n      size = 5\n    )\n  \n  plot\n})\n\n\n\n\nWe are now going to take a look at the various attributes of the songs that are associated with each component.\nWe can see that the first component is associated with the energy, loudness, acousticness of the songs. The second component is associated with the danceability, duration, and speechiness of the songs.\n\ntidy(rec, 2) |&gt;\n  filter(component %in% paste0('PC', 1:5)) |&gt;\n  mutate(positive = ifelse(value &gt; 0, 'Positive', 'Negative')) |&gt;\n  mutate(value = abs(value)) |&gt;\n  \n  ggplot(aes(\n    x = value,\n    y = reorder_within(terms, value, component),\n    fill = positive\n  )) +\n  geom_col() +\n  scale_y_reordered() +\n  facet_wrap( ~ component, scales = 'free') +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = 'bottom') +\n  labs(x = 'Absolute Value of Coefficient', y = NULL, fill = NULL)\n\n\n\n\nFinally, we are going to visualize the first two components using a scatter plot. Based on the attributes that we have seen in the previous plot, we can see that a positive PC1 is associated with higher energy, loudness, and positivity (valence). A negative PC1 is associated with higher acousticness, duration, and instrumentalness. A positive PC2 is associated with lengthier songs, higher tempo, and loudness, while a negative PC2 is associated with higher danceability, speechiness, and positivity (valence)\n\nmake_pc_plot &lt;- \\(data, pc_x, pc_y) {\n  pc_x &lt;- parse_expr(pc_x)\n  pc_y &lt;- parse_expr(pc_y)\n  \n  p &lt;- expr({\n    ggplot(pca_plot, aes(\n      x = !!pc_x,\n      y = !!pc_y,\n      color = track_name\n    )) +\n      geom_point() +\n      theme_minimal(base_size = 15) +\n      theme(legend.position = \"none\") +\n      coord_cartesian(xlim = c(-6, 6), ylim = c(-6, 6)) +\n      geom_hline(yintercept = 0,\n                 linetype = 'dashed',\n                 alpha = .25) +\n      geom_vline(xintercept = 0,\n                 linetype = 'dashed',\n                 alpha = .25)\n    \n  }) |&gt; eval()\n  \n  ggplotly(p, tooltip = 'track_name')\n  \n}\n  \npca_plot &lt;- bake(rec, new_data = NULL)\n\nmake_pc_plot(pca_plot, 'PC2', 'PC1')\n\n\n\n\n\nAlthough traditionally most of the variance is usually explained by the first two PC. We can further visualize other combinations of PC in two dimensional space.\n\nmake_pc_plot(pca_plot, 'PC3', 'PC1')\n\n\n\n\nmake_pc_plot(pca_plot, 'PC4', 'PC1')\n\n\n\n\nmake_pc_plot(pca_plot, 'PC5', 'PC1')"
  },
  {
    "objectID": "posts/retrieve-pubmed-publications/index.html",
    "href": "posts/retrieve-pubmed-publications/index.html",
    "title": "Retrieve Pubmed publications using the RefManageR package",
    "section": "",
    "text": "For anyone that’s working in academia, it may be useful to keep a tabulation of all of the publications that you are a co-author of. We can use the RefManageR package, which includes a function (ReadPubMed()) that allows us to query the PubMed API for publications based on a PubMed query.\nBelow I generate a PubMed query q that would query PubMed for publications that involves Luu, Michael as an author with a Cedars-Sinai affiliation. I then pass this query into the ReadPubMed() function and save the results as pm. The output is a BibEntry object that can be further coerced into a tibble.\n\nq &lt;- '(Luu, Michael[Author]) AND (Cedars-Sinai[Affiliation])'\n\npm &lt;- RefManageR::ReadPubMed(q, retmax = 999)\n  \nout &lt;- pm |&gt; as_tibble()\n\nglimpse(out)\n\nRows: 68\nColumns: 15\n$ bibtype    &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article\", \"Article\", \"Art…\n$ title      &lt;chr&gt; \"Concurrent prognostic utility of lymph node count and lymp…\n$ author     &lt;chr&gt; \"John M Masterson and Michael Luu and Aurash Naser-Tavakoli…\n$ year       &lt;chr&gt; \"2023\", \"2022\", \"2023\", \"2022\", \"2022\", \"2022\", \"2022\", \"20…\n$ month      &lt;chr&gt; \"Jan\", \"Nov\", \"Apr\", \"Aug\", \"Oct\", \"Jul\", \"Aug\", \"Jul\", \"Ma…\n$ journal    &lt;chr&gt; \"Prostate cancer and prostatic diseases\", \"NPJ breast cance…\n$ eprint     &lt;chr&gt; \"36600045\", \"36402796\", \"36385470\", \"36054029\", \"35997126\",…\n$ doi        &lt;chr&gt; \"10.1038/s41391-022-00635-1\", \"10.1038/s41523-022-00489-9\",…\n$ language   &lt;chr&gt; \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"en…\n$ issn       &lt;chr&gt; \"1476-5608\", \"2374-4677\", \"1531-4995\", \"1531-4995\", \"1097-0…\n$ abstract   &lt;chr&gt; \"BACKGROUND: While both the number (+LN) and density (LND) …\n$ eprinttype &lt;chr&gt; \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\", \"pubmed\",…\n$ volume     &lt;chr&gt; NA, \"8\", \"133\", NA, \"128\", \"113\", \"208\", \"114\", \"165\", \"40\"…\n$ number     &lt;chr&gt; NA, \"1\", \"4\", NA, \"20\", \"4\", \"2\", \"7\", \"2\", \"4\", \"1\", \"1\", …\n$ pages      &lt;chr&gt; NA, \"123\", \"E25\", NA, \"3610-3619\", \"787-795\", \"301-308\", \"1…\n\nout\n\n# A tibble: 68 × 15\n   bibtype title author year  month journal eprint doi   language issn  abstract\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   \n 1 Article Conc… John … 2023  Jan   Prosta… 36600… 10.1… eng      1476… \"BACKGR…\n 2 Article Toxi… N Lyn… 2022  Nov   NPJ br… 36402… 10.1… eng      2374… \"Adjuva…\n 3 Article In R… Eric … 2023  Apr   The La… 36385… 10.1… eng      1531…  &lt;NA&gt;   \n 4 Article Pred… Eric … 2022  Aug   The La… 36054… 10.1… eng      1531… \"BACKGR…\n 5 Article Disp… Yi-Te… 2022  Oct   Cancer  35997… 10.1… eng      1097… \"BACKGR…\n 6 Article Noda… Diana… 2022  Jul   Intern… 35395… 10.1… eng      1879… \"PURPOS…\n 7 Article Vari… Timot… 2022  Aug   The Jo… 35377… 10.1… eng      1527… \"PURPOS…\n 8 Article Quan… Antho… 2022  Jul   Journa… 35311… 10.1… eng      1460… \"BACKGR…\n 9 Article Path… Eric … 2022  May   Gyneco… 35216… 10.1… eng      1095… \"PURPOS…\n10 Article Pred… Paige… 2022  Apr   Urolog… 35067… 10.1… eng      1873… \"BACKGR…\n# ℹ 58 more rows\n# ℹ 4 more variables: eprinttype &lt;chr&gt;, volume &lt;chr&gt;, number &lt;chr&gt;, pages &lt;chr&gt;\n\n\nNow that we have this information into a tibble, we can further visualize the frequency of occurrences among the list of co-authors using a word cloud\n\nplot_data &lt;- out |&gt;\n  select(author) |&gt;\n  separate_wider_delim(\n    author,\n    names = paste0('author', 1:20),\n    delim = ' and ',\n    too_few = 'align_start'\n  ) |&gt;\n  mutate(i = row_number()) |&gt;\n  pivot_longer(contains('author')) |&gt;\n  filter(!is.na(value)) |&gt; \n  count(value) |&gt; \n  arrange(desc(n)) |&gt; \n  filter(value != 'Michael Luu')\n\nset.seed(1)\nggplot(plot_data, aes(label = value, size = n, color = n)) +\n  geom_text_wordcloud_area() +\n  scale_size_area(max_size = 16) +\n  theme_minimal() +\n  scale_color_viridis_c()\n\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] ggwordcloud_0.5.0 RefManageR_1.4.0  lubridate_1.9.2   forcats_1.0.0    \n [5] stringr_1.5.0     dplyr_1.1.1       purrr_1.0.1       readr_2.1.4      \n [9] tidyr_1.3.0       tibble_3.2.1      ggplot2_3.4.2     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0  xfun_0.38         colorspace_2.1-0  vctrs_0.6.1      \n [5] generics_0.1.3    viridisLite_0.4.1 htmltools_0.5.4   yaml_2.3.6       \n [9] utf8_1.2.3        rlang_1.1.0       pillar_1.9.0      glue_1.6.2       \n[13] withr_2.5.0       lifecycle_1.0.3   plyr_1.8.8        munsell_0.5.0    \n[17] gtable_0.3.3      htmlwidgets_1.6.2 evaluate_0.19     labeling_0.4.2   \n[21] knitr_1.41        tzdb_0.3.0        fastmap_1.1.0     curl_5.0.0       \n[25] fansi_1.0.4       Rcpp_1.0.10       renv_0.16.0       scales_1.2.1     \n[29] backports_1.4.1   jsonlite_1.8.4    farver_2.1.1      hms_1.1.3        \n[33] png_0.1-8         digest_0.6.31     stringi_1.7.8     grid_4.2.2       \n[37] bibtex_0.5.1      cli_3.6.1         tools_4.2.2       magrittr_2.0.3   \n[41] pkgconfig_2.0.3   xml2_1.3.3        timechange_0.2.0  rmarkdown_2.19   \n[45] httr_1.4.5        rstudioapi_0.14   R6_2.5.1          compiler_4.2.2"
  },
  {
    "objectID": "posts/quarto-shinylive/index.html",
    "href": "posts/quarto-shinylive/index.html",
    "title": "Implementation of ShinyLive in Quarto",
    "section": "",
    "text": "The following blogpost is an example implementation of ShinyLive.\nA traditional Shiny application requires a Shiny Server for computation, that handles both the computation and serving the application to the user.\n\nThe inverse is true with the implementation of ShinyLive and WebR, where the application can be hosted on a static web server similar to GitHub Pages, and the computation is handled by the user themselves.\n\nThe current blogpost is an example implementation of ShinyLive in Quarto using the shinylive-quarto extension. Source code for the current blogpost can be found here.\nMore information regarding the implementation of shinylive can be found here.\n#| standalone: true\n#| viewerHeight: 600\n\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/nlp-taylor-swift-beyonce/index.html",
    "href": "posts/nlp-taylor-swift-beyonce/index.html",
    "title": "Natural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday",
    "section": "",
    "text": "Let’s start off by loading the data from the tidytuesday github repository."
  },
  {
    "objectID": "posts/nlp-taylor-swift-beyonce/index.html#model-parameter-tuning",
    "href": "posts/nlp-taylor-swift-beyonce/index.html#model-parameter-tuning",
    "title": "Natural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday",
    "section": "Model parameter tuning",
    "text": "Model parameter tuning\nThe model parameters cost and rbf_sigma will be tuned via a grid search of 10 values\n\nsvm_wf &lt;- workflow() %&gt;%\n  add_model(svm_spec) %&gt;%\n  add_recipe(rec)\n\nsvm_tune_folds &lt;- vfold_cv(train, strata = artist)\n\nset.seed(1)\nsvm_tune_res &lt;- tune_grid(\n  svm_wf,\n  resamples = svm_tune_folds,\n  grid = 10\n)\n\ntune_metrics &lt;- svm_tune_res %&gt;% collect_metrics()\n\ntune_metrics %&gt;%\n  filter(., .metric == 'accuracy') %&gt;%\n  ggplot(.,\n         aes(y = rbf_sigma, x = cost, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c()\n\n\n\nsvm_tune_res %&gt;% show_best(metric = 'accuracy')\n\n# A tibble: 5 × 8\n    cost rbf_sigma .metric  .estimator  mean     n std_err .config              \n   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 3.15    2.37e- 4 accuracy binary     0.804    10 0.0150  Preprocessor1_Model04\n2 0.439   8.95e- 2 accuracy binary     0.747    10 0.00199 Preprocessor1_Model01\n3 0.0342  4.45e- 3 accuracy binary     0.747    10 0.00199 Preprocessor1_Model02\n4 0.0177  4.34e- 8 accuracy binary     0.747    10 0.00199 Preprocessor1_Model03\n5 0.0856  8.84e-10 accuracy binary     0.747    10 0.00199 Preprocessor1_Model05\n\nbest_accuracy &lt;- svm_tune_res %&gt;% select_best(., metric = 'accuracy')\n\nbest_accuracy\n\n# A tibble: 1 × 3\n   cost rbf_sigma .config              \n  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1  3.15  0.000237 Preprocessor1_Model04"
  },
  {
    "objectID": "posts/meta-programming/index.html",
    "href": "posts/meta-programming/index.html",
    "title": "Meta-programming with R and rlang",
    "section": "",
    "text": "rlang is a powerful R package that allows the coder the ability to write code with code. The amazing book Advanced R by Hadley Wickham goes into this idea in much greater detail, but I wanted to present a small example of my favorite function parse_expr() from the rlang package in this post.\nIn short, rlang provides functions that facilitates the coder the ability to delay evaluation of expressions. Furthermore, we can manipulate the the expressions with various tools within rlang, by piecing together various expressions.\nI will be using the data from the palmerpenguins package\n\nlibrary(palmerpenguins)\nlibrary(rlang)\n\ndata &lt;- palmerpenguins::penguins\n\ndata\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nAs a simple example, let’s say we would like to build a linear regression model for bill_length_mm, using all other variables as covariates.\n\nfit &lt;-\n  lm(\n    bill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm +\n      body_mass_g + sex + year,\n    data = data\n  )\n\nfit\n\n\nCall:\nlm(formula = bill_length_mm ~ species + island + bill_depth_mm + \n    flipper_length_mm + body_mass_g + sex + year, data = data)\n\nCoefficients:\n      (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n       -3.893e+02          9.910e+00          6.487e+00         -4.624e-01  \n  islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n       -7.327e-02          3.272e-01          5.724e-02          1.136e-03  \n          sexmale               year  \n        2.054e+00          2.023e-01  \n\n\nThis is a simple example, but what would happen if there are many many covariates we would like to include in the model. We can construct the formula as a string and then use parse_expr() to parse the string into an expression.\n\nf &lt;- paste0('bill_length_mm ~ ', paste0(names(data)[names(data) != 'bill_length_mm'], collapse = ' + '))\n\nf &lt;- parse_expr(f)\n\nf\n\nbill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm + \n    body_mass_g + sex + year\n\n\nAs an example, we can construct an expression like so, and delay the evaluation for later time.\n\nexpr(lm(\"this is where the formula should be inserted\", data = data))\n\nlm(\"this is where the formula should be inserted\", data = data)\n\n\nUsing the example from above, we can construct a new expression, and inject the previous expression into it using the !! (bang bang) operator.\n\nlm_model &lt;- expr(lm(!!f, data = data))\n\nlm_model\n\nlm(bill_length_mm ~ species + island + bill_depth_mm + flipper_length_mm + \n    body_mass_g + sex + year, data = data)\n\n\nAs we can see, using the !! operator allows us to piece together various expressions. However, do note that the expression is still not evaluated. If we would like to evaluate the expression, we can pass the object lm_model into eval()\n\nlm_model |&gt; eval()\n\n\nCall:\nlm(formula = bill_length_mm ~ species + island + bill_depth_mm + \n    flipper_length_mm + body_mass_g + sex + year, data = data)\n\nCoefficients:\n      (Intercept)   speciesChinstrap      speciesGentoo        islandDream  \n       -3.893e+02          9.910e+00          6.487e+00         -4.624e-01  \n  islandTorgersen      bill_depth_mm  flipper_length_mm        body_mass_g  \n       -7.327e-02          3.272e-01          5.724e-02          1.136e-03  \n          sexmale               year  \n        2.054e+00          2.023e-01  \n\n\nAlthough this is a fun toy example, the ability to piece together various pieces of expression and delay evaluation is a very power functional programming tool in R."
  },
  {
    "objectID": "posts/haunted-places/index.html",
    "href": "posts/haunted-places/index.html",
    "title": "Haunted Los Angeles - Interactive Mapping using Leaflet",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(leaflet)\n\nThe following blog post is a very simple visualization using the data from tidytuesday 2023, week 41. This visualization uses the leaflet library to produce a very simple spatial analysis of the ‘haunted’ locations found in Los Angeles, CA.\nWe begin by loading the data from the tidytuesdayR package.\n\ndatas &lt;- tt_load(2023, 41)\n\n\n    Downloading file 1 of 1: `haunted_places.csv`\n\ndata &lt;- datas$haunted_places\n\nNext we take a glimpse of the data to see what we are working with.\n\nglimpse(data)\n\nRows: 10,992\nColumns: 10\n$ city           &lt;chr&gt; \"Ada\", \"Addison\", \"Adrian\", \"Adrian\", \"Albion\", \"Albion…\n$ country        &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"Uni…\n$ description    &lt;chr&gt; \"Ada witch - Sometimes you can see a misty blue figure …\n$ location       &lt;chr&gt; \"Ada Cemetery\", \"North Adams Rd.\", \"Ghost Trestle\", \"Si…\n$ state          &lt;chr&gt; \"Michigan\", \"Michigan\", \"Michigan\", \"Michigan\", \"Michig…\n$ state_abbrev   &lt;chr&gt; \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"MI\", \"…\n$ longitude      &lt;dbl&gt; -85.50489, -84.38184, -84.03566, -84.01757, -84.74518, …\n$ latitude       &lt;dbl&gt; 42.96211, 41.97142, 41.90454, 41.90571, 42.24401, 42.23…\n$ city_longitude &lt;dbl&gt; -85.49548, -84.34717, -84.03717, -84.03717, -84.75303, …\n$ city_latitude  &lt;dbl&gt; 42.96073, 41.98643, 41.89755, 41.89755, 42.24310, 42.24…\n\n\nWe next subset the data for the observations found in Los Angeles, CA. The longitude and latitude are then used to plot the locations on a map using the leaflet library. The ‘J.F.K. Library Third’ location is removed from the data as it is an outlier and is not located in Los Angeles, CA.\n\ndata |&gt;\n  filter(city %in% c('Los Angeles')) |&gt;\n  filter(state_abbrev == 'CA') |&gt;\n  filter(location != 'J.F.K. Library Third') |&gt; \n  mutate(label = glue::glue('&lt;b&gt;{location}&lt;/b&gt; &lt;br&gt; {description}')) |&gt; \n  leaflet() |&gt;\n  addTiles() |&gt;\n  addMarkers(\n    lng = ~ longitude,\n    lat = ~ latitude,\n    popup = ~ label\n  )"
  },
  {
    "objectID": "posts/aoc-2023-d9/index.html",
    "href": "posts/aoc-2023-d9/index.html",
    "title": "Advent of Code 2023, Day 9",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\n\nPart 1\nPart 1 of this puzzle is fairly straight forward. The goal of Part 1 is to identify the next number in the ‘history’. In order to identify the next value in the history, we need to identify the difference between the values, until the differences in the values is 0. We would then sum the last value of all the differences in order to obtain the next value in the history.\nThe puzzle prompt is as follows:\n\nAnalyze your OASIS report and extrapolate the next value for each history. What is the sum of these extrapolated values?\n\nWe would then take the sum of the predicted next history among all of the histories.\nLet’s start off with reading in the puzzle input.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d9', 'puzzle-input.txt')\n)\n\ndata |&gt; head()\n\n[1] \"12 12 11 5 -5 -5 40 194 558 1278 2553 4643 7877 12661 19486 28936 41696 58560 80439 108369 143519\"                  \n[2] \"6 12 33 79 163 320 638 1301 2644 5220 9879 17859 30889 51304 82172 127433 192050 282172 405309 570519 788607\"       \n[3] \"3 5 5 3 9 60 262 872 2445 6091 13915 29744 60271 116756 217404 390473 678031 1140057 1858241 2938353 4509389\"       \n[4] \"15 21 23 31 67 161 350 681 1224 2114 3674 6758 13659 30354 71646 172084 407585 935627 2065861 4378033 8910071\"      \n[5] \"13 32 75 155 286 477 724 1011 1347 1894 3301 7501 19549 51737 132449 324337 759840 1707391 3689542 7685418 15464987\"\n[6] \"9 23 59 129 245 419 663 989 1409 1935 2579 3353 4269 5339 6575 7989 9593 11399 13419 15665 18149\"                   \n\n\nNow let’s clean up the data a bit, we’ll start by splitting the history by spaces and converting the history into a column of a tibble.\n\ndata &lt;- map(data, \\(x) {\n  x |&gt;\n    str_split(' ')  |&gt;\n    unlist() |&gt;\n    as_tibble() |&gt;\n    mutate(value = as.numeric(value)) |&gt;\n    rename('c0' = value)\n  \n})\n\ndata |&gt; head()\n\n[[1]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1    12\n 2    12\n 3    11\n 4     5\n 5    -5\n 6    -5\n 7    40\n 8   194\n 9   558\n10  1278\n# ℹ 11 more rows\n\n[[2]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1     6\n 2    12\n 3    33\n 4    79\n 5   163\n 6   320\n 7   638\n 8  1301\n 9  2644\n10  5220\n# ℹ 11 more rows\n\n[[3]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1     3\n 2     5\n 3     5\n 4     3\n 5     9\n 6    60\n 7   262\n 8   872\n 9  2445\n10  6091\n# ℹ 11 more rows\n\n[[4]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1    15\n 2    21\n 3    23\n 4    31\n 5    67\n 6   161\n 7   350\n 8   681\n 9  1224\n10  2114\n# ℹ 11 more rows\n\n[[5]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1    13\n 2    32\n 3    75\n 4   155\n 5   286\n 6   477\n 7   724\n 8  1011\n 9  1347\n10  1894\n# ℹ 11 more rows\n\n[[6]]\n# A tibble: 21 × 1\n      c0\n   &lt;dbl&gt;\n 1     9\n 2    23\n 3    59\n 4   129\n 5   245\n 6   419\n 7   663\n 8   989\n 9  1409\n10  1935\n# ℹ 11 more rows\n\n\nI’m going to create two helper functions to facilitate this puzzle, first it’s generate_new_columns. This function will create new columns with the differences based on the previous column. The second function predicted_next_history will continue to calculate the differences in new columns until all the values are zero.\n\ngenerate_new_columns &lt;- \\(data, new_column, prev_column) {\n  prev_column &lt;- rlang::parse_expr(prev_column)\n  new_column &lt;- rlang::parse_expr(new_column)\n  \n  temp &lt;- data |&gt;\n    mutate(!!new_column := !!prev_column - lag(!!prev_column)) |&gt;\n    select(!!new_column)\n  \n  bind_cols(data, temp)\n  \n}\n\npredicted_next_history &lt;- \\(history) {\n\n  all_zeroes &lt;- FALSE\n  i &lt;- 0\n  temp &lt;- history\n  prev &lt;- NULL\n  \n  while (all_zeroes == FALSE) {\n    new &lt;- paste0('c', i + 1)\n    prev &lt;- paste0('c', i)\n    \n    temp &lt;- generate_new_columns(temp, new, prev)\n    new_column &lt;- temp |&gt; pull(!!new) |&gt; na.omit()\n    all_zeroes &lt;- all(new_column == 0)\n    \n    i &lt;- i + 1\n    \n  }\n  \n  values &lt;- map(temp, \\(x) {\n    vals &lt;- x |&gt; na.omit()\n    \n    total_vals &lt;- length(vals)\n    \n    return(vals[total_vals])\n    \n  })\n  \n\n  values |&gt; unlist() |&gt; sum()\n\n}\n\nNow we can apply these helper functions to all of the histories.\n\nresults &lt;-\n  map(data, \\(x) predicted_next_history(x), .progress = 'Predicting Next History:')\n\nresults |&gt; head()\n\n[[1]]\n[1] 187199\n\n[[2]]\n[1] 1072336\n\n[[3]]\n[1] 6716828\n\n[[4]]\n[1] 17445671\n\n[[5]]\n[1] 30121668\n\n[[6]]\n[1] 20883\n\n\nOne we have all of the predicted next hsitories, we can take the sum to get the answer.\n\nresults |&gt; unlist() |&gt; sum()\n\n\n\n\nPart 2\nPart 2 of the puzzle is fairly simple, instead of the next value in the history we need to identify the previous value of the history. We simply modify the code to the predicted_next_history function to identify the reverse.\n\npredicted_prev_history &lt;- \\(history) {\n\n  all_zeroes &lt;- FALSE\n  i &lt;- 0\n  temp &lt;- history\n  prev &lt;- NULL\n  \n  while (all_zeroes == FALSE) {\n    new &lt;- paste0('c', i + 1)\n    prev &lt;- paste0('c', i)\n    \n    temp &lt;- generate_new_columns(temp, new, prev)\n    new_column &lt;- temp |&gt; pull(!!new) |&gt; na.omit()\n    all_zeroes &lt;- all(new_column == 0)\n    \n    i &lt;- i + 1\n    \n  }\n  \n  values &lt;- map(temp, \\(x) {\n    vals &lt;- x |&gt; na.omit()\n    \n    return(vals[1])\n    \n  })\n  \n  \n  values &lt;- values |&gt;\n    unlist()\n  \n  values &lt;- values[1:length(values) - 1]\n  \n  values &lt;- rev(values)\n  \n  current_value &lt;- 0\n  values &lt;- map(values, \\(x) {\n    current_value &lt;&lt;- x - current_value\n    \n    \n  }) |&gt; unlist()\n  \n  values[length(values)] |&gt; unname()\n  \n}\n\nresults &lt;-\n  map(data, \\(x) predicted_prev_history(x), .progress = 'Predicting Prev History:')\n\nresults |&gt; head()\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 10\n\n[[5]]\n[1] 8\n\n[[6]]\n[1] 5\n\n\nWith the results, we can take the sum to get the answer.\n\nresults |&gt; unlist() |&gt; sum()"
  },
  {
    "objectID": "posts/aoc-2023-d7/index.html",
    "href": "posts/aoc-2023-d7/index.html",
    "title": "Advent of Code 2023, Day 7",
    "section": "",
    "text": "Time for some Camel Cards (Poker)! This was a fun puzzle, that really made me think outside of the box.\n\nPart 1\nWe are given a hand and a set of rules on how to rank the hand. The rules are as follows:\n\nIn Camel Cards, you get a list of hands, and your goal is to order them based on the strength of each hand. A hand consists of five cards labeled one of A, K, Q, J, T, 9, 8, 7, 6, 5, 4, 3, or 2. The relative strength of each card follows this order, where A is the highest and 2 is the lowest. The strength of a hand is determined by the highest ranked card in the hand, with ties broken by the second highest ranked card, and so on.\n\nLet’s start off by reading in the data and separating the hand and bid into separate columns.\n\nlibrary(tidyverse)\nlibrary(here)\n\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d7', 'puzzle-input.txt')\n)\n\ndata &lt;- data |&gt; \n  as_tibble() |&gt; \n  separate(value, into = c('hand', 'bid'), sep = ' ')\n\nNow let’s recap on the prompt of the puzzle.\n\nFind the rank of every hand in your set. What are the total winnings?\n\nI wrote a simple function, check_hand_type to identify the type of hand that we are given. The function works by taking a count of the number of unique values in the hand, then assigning the type of either, ‘ONE PAIR’, ‘TWO PAIR’, ‘THREE OF A KIND’, ‘FULL HOUSE’, ‘FOUR OF A KIND’, or ‘STRAIGHT’. If the hand is not one of these types, then it is a ‘HIGH CARD’.\n\ncheck_hand_type &lt;- \\(x) {\n  values &lt;- str_split(x, '') |&gt; unlist()\n  \n  counts &lt;- values |&gt; as_tibble() |&gt; count(value)\n  \n  counts &lt;- counts |&gt; pull(n)\n  \n  if (length(counts) == 4) {\n    if (all(sort(counts) == c(1, 1, 1, 2))) {\n      type &lt;- 'ONE PAIR'\n    }\n    \n  }\n  \n  if (length(counts) == 3) {\n    if (all(sort(counts) == c(1, 2, 2))) {\n      type &lt;- 'TWO PAIR'\n    }\n    \n    if (all(sort(counts) == c(1, 1, 3))) {\n      type &lt;- 'THREE OF A KIND'\n    }\n    \n  }\n  \n  if (length(counts) == 2) {\n    \n    if (all(sort(counts) == c(2, 3))) {\n      type &lt;- 'FULL HOUSE'\n    }\n    \n    if (all(sort(counts) == c(1, 4))) {\n      type &lt;- 'FOUR OF A KIND'\n    }\n    \n  }\n  \n  if (length(counts) == 1) {\n    type &lt;- 'FIVE OF A KIND'\n  }\n  \n  if (length(counts) == 5) {\n    type &lt;- 'HIGH CARD'\n  }\n  \n  type\n  \n}\n\nNext, let’s apply this function to every hand in our data. Then we will convert the type to a factor, and order the levels in the correct order of increasing strength.\n\ndata &lt;- data |&gt; \n  rowwise() |&gt; \n  mutate(\n    type = check_hand_type(hand)\n  ) |&gt; \n  ungroup()\n\ndata &lt;- data |&gt; \n  mutate(\n    type = factor(\n      type,\n      levels = c(\n        'HIGH CARD',\n        'ONE PAIR',\n        'TWO PAIR',\n        'THREE OF A KIND',\n        'FULL HOUSE',\n        'FOUR OF A KIND',\n        'FIVE OF A KIND'\n      ),\n      ordered = TRUE\n    )\n  )\n\nNow that we have the type of each hand, we can apply the first level of ranking by the the type. Next within each type, we will further rank the hands by the highest card, then the second highest card, and so on. We will do this by splitting the hand into separate columns, then using arrange on each of the columns.\n\nweights &lt;- rev(c('A', 'K', 'Q', 'J', 'T', '9', '8', '7', '6', '5', '4', '3', '2'))\n\ndata &lt;- data |&gt;\n  group_nest(type) |&gt;\n  mutate(data = map(data, \\(data) {\n    data &lt;- data |&gt;\n      mutate(splits = str_split(hand, '')) |&gt;\n      rowwise() |&gt;\n      mutate(\n        card1 = splits[[1]],\n        card2 = splits[[2]],\n        card3 = splits[[3]],\n        card4 = splits[[4]],\n        card5 = splits[[5]]\n      ) |&gt;\n      ungroup() |&gt;\n      arrange(card1)\n    \n    data &lt;- data |&gt;\n      mutate(across(contains('card'), \\(x) {\n        factor(x, levels = weights, ordered = TRUE)\n      }))\n    \n    data |&gt;\n      arrange(card1,\n              card2,\n              card3,\n              card4,\n              card5) |&gt; \n      select(-splits)\n    \n  })) \n\nOnce we have the ranking of the hands, we can calculate the total winnings by multiplying the rank by the bid. Then we can sum the total winnings to get the answer to the puzzle.\n\ndata &lt;- data |&gt; \n  unnest(data) |&gt; \n  mutate(\n    rank = row_number(),\n    .before = type\n  ) |&gt; \n  mutate(\n    total_winnings = rank * as.numeric(bid)\n  )\n\n\nsum(data$total_winnings)\n\n\n\nPart 2\nThe second part of the puzzle asks us to consider the following:\n\nTo make things a little more interesting, the Elf introduces one additional rule. Now, J cards are jokers - wildcards that can act like whatever card would make the hand the strongest type possible.\n\nWith the prompt as follows:\n\nUsing the new joker rule, find the rank of every hand in your set. What are the new total winnings?\n\nOverall the second part of the puzzle is very similar to the first part. The only difference is that we need to consider the jokers in our ranking. To do this, we will add a new column to our data, jokers, which will tally the number of jokers in each hand. Then we will create a new column, new_type, which will be the type of the hand with the jokers considered. We will do this by using case_when to assign the new type based on the original type and the number of jokers.\n\ndata &lt;- data |&gt;\n  mutate(jokers = str_count(hand, 'J')) |&gt;\n  mutate(\n    new_type = case_when(\n      type == 'FOUR OF A KIND' & jokers == 1 ~ 'FIVE OF A KIND',\n      type == 'HIGH CARD' & jokers == 1 ~ 'ONE PAIR',\n      type == 'ONE PAIR' & jokers == 1 ~ 'THREE OF A KIND',\n      type == 'ONE PAIR' & jokers == 2 ~ 'THREE OF A KIND',\n      type == 'TWO PAIR' & jokers == 1 ~ 'FULL HOUSE',\n      type == 'TWO PAIR' & jokers == 2 ~ 'FOUR OF A KIND',\n      type == 'THREE OF A KIND' & jokers == 1 ~ 'FOUR OF A KIND',\n      type == 'FULL HOUSE' & jokers == 2 ~ 'FIVE OF A KIND',\n      type == 'FULL HOUSE' & jokers == 3 ~ 'FIVE OF A KIND',\n      type == 'FOUR OF A KIND' & jokers == 4 ~ 'FIVE OF A KIND',\n      type == 'FIVE OF A KIND' & jokers == 5 ~ 'FIVE OF A KIND',\n      jokers == 0 ~ type\n    )\n  )\n\ndata &lt;- data |&gt; \n  mutate(\n    new_type = factor(\n      new_type,\n      levels = c(\n        'HIGH CARD',\n        'ONE PAIR',\n        'TWO PAIR',\n        'THREE OF A KIND',\n        'FULL HOUSE',\n        'FOUR OF A KIND',\n        'FIVE OF A KIND'\n      ),\n      ordered = TRUE\n    )\n  )\n\nNow that we have the new type of each hand, we can apply the first level of ranking by the the type. We will modify the previous code in which we will now rank the J as the weakest card. The rest is identical to Part 1 to tally the total winnings.\n\nweights &lt;- rev(c('A', 'K', 'Q', 'T', '9', '8', '7', '6', '5', '4', '3', '2', 'J'))\n\ndata &lt;- data |&gt;\n  group_nest(new_type) |&gt;\n  mutate(data = map(data, \\(data) {\n    data &lt;- data |&gt;\n      mutate(splits = str_split(hand, '')) |&gt;\n      rowwise() |&gt;\n      mutate(\n        card1 = splits[[1]],\n        card2 = splits[[2]],\n        card3 = splits[[3]],\n        card4 = splits[[4]],\n        card5 = splits[[5]]\n      ) |&gt;\n      ungroup() |&gt;\n      arrange(card1)\n    \n    data &lt;- data |&gt;\n      mutate(across(contains('card'), \\(x) {\n        factor(x, levels = weights, ordered = TRUE)\n      }))\n    \n    data |&gt;\n      arrange(card1,\n              card2,\n              card3,\n              card4,\n              card5) |&gt; \n      select(-splits)\n    \n  })) \n\ndata &lt;- data |&gt; \n  unnest(data) |&gt; \n  mutate(\n    rank = row_number(),\n    .before = type\n  ) |&gt; \n  mutate(\n    total_winnings = rank * as.numeric(bid)\n  )\n\n\nsum(data$total_winnings)"
  },
  {
    "objectID": "posts/aoc-2023-d5/index.html",
    "href": "posts/aoc-2023-d5/index.html",
    "title": "Advent of Code 2023, Day 5",
    "section": "",
    "text": "Let’s just say this puzzle made my processor go BRRRRRRRRRRRRR.\n\nWhat is the lowest location number that corresponds to any of the initial seed numbers?\n\nThe idea of this puzzle is that they provided specific instructions in the puzzle input that allows us to construct the conversion tables need to convert from seed to location. The conversion tables are used to convert the seed numbers to soil, then soil to fertilizer, then fertilizer to water, then water to light, then light to temp, then temp to humidity, then finally humidity to location.\nI don’t have the most efficient method of doing this, but it works. The strategy of this puzzle isn’t very difficulty, however the sheer amount of data that needs to be processed is what makes this puzzle difficult. Each conversion table that being generated are in the hundreds of millions of rows, and there are 7 conversion tables that need to be generated for us to convert from seed to location. My strategy was to use the arrow package, which allows for out of memory processing of big data.\n\nPart 1\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(arrow)\n\nWe first read in the puzzle prompt, and extract out the specific instructions for each conversion table into its own data frame. We create two helper functions that helps us process the data. The first function generate_map_conversion_data, takes in the conversion table data, and the title of the conversion table, and returns a list of arrow tables that contains the conversion table data. The second function takes in a list of arrow tables, and the source numbers, and returns the destination numbers.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d5', 'puzzle-input.txt')\n)\n\nseeds &lt;- data[1] |&gt; \n  str_remove('seeds: ') |&gt; \n  str_split(pattern = ' ') |&gt; \n  unlist() |&gt; \n  as.numeric()\n\nseed_to_soil_map &lt;- data[4:50]\nsoil_to_fertilizer_map &lt;- data[53:70]\nfertilizer_to_water_map &lt;- data[73:84]\nwater_to_light_map &lt;- data[87:135]\nlight_to_temp_map &lt;- data[138:167]\ntemp_to_humidity_map &lt;- data[170:192]\nhumidity_to_location_map &lt;- data[195:237]\n\ngenerate_map_conversion_data &lt;- \\(map_data, title) {\n  map_data &lt;- as_tibble(map_data) |&gt;\n    separate(value, c('destination', 'origin', 'range'), sep = ' ') |&gt;\n    mutate(across(everything(), \\(x) as.numeric(x)))\n  \n  \n  table &lt;- pmap(map_data, \\(destination, origin, range) {\n    arrow_table(\n      origin = seq(origin, origin + range - 1, by = 1),\n      destination = seq(destination, destination + range - 1, by = 1)\n    )\n    \n  }, .progress = glue::glue('Constructing Conversion Table for {title}:'))\n  \n  table &lt;- concat_tables(!!!table)\n  \n}\n\nextract_map_conversion_data &lt;-\n  \\(map_data, origin_values, title) {\n\n    map_data &lt;- generate_map_conversion_data(map_data, title)\n    \n    map_data &lt;- map_data |&gt;\n      filter(origin %in% origin_values) |&gt;\n      collect()\n    \n    origin &lt;- map_data |&gt; pull(origin)\n    \n    missing &lt;- setdiff(origin_values, origin)\n    \n    if (length(missing) &gt; 0) {\n      map_data &lt;- tibble(origin = missing,\n                         destination = missing) |&gt;\n        bind_rows(map_data)\n    }\n    \n    map_data\n  }\n\nHere we generate a tibble that contains the conversion table data, and the title of the conversion table.\n\nparameters &lt;- tibble(\n  title = c(\n    'Seed to Soil',\n    'Soil to Fertilizer',\n    'Fertilizer to Water',\n    'Water to Light',\n    'Light to Temp',\n    'Temp to Humidity',\n    'Humidity to Location'\n  ),\n  data = list(\n    seed_to_soil_map,\n    soil_to_fertilizer_map,\n    fertilizer_to_water_map,\n    water_to_light_map,\n    light_to_temp_map,\n    temp_to_humidity_map,\n    humidity_to_location_map\n  )\n)\n\nparameters\n\n# A tibble: 7 × 2\n  title                data      \n  &lt;chr&gt;                &lt;list&gt;    \n1 Seed to Soil         &lt;chr [47]&gt;\n2 Soil to Fertilizer   &lt;chr [18]&gt;\n3 Fertilizer to Water  &lt;chr [12]&gt;\n4 Water to Light       &lt;chr [49]&gt;\n5 Light to Temp        &lt;chr [30]&gt;\n6 Temp to Humidity     &lt;chr [23]&gt;\n7 Humidity to Location &lt;chr [43]&gt;\n\n\nWe then use the pwalk function to iterate through the list of conversion tables, and extract the destination numbers from the conversion tables. We then use the pull function to extract the destination numbers from the arrow table, and assign it to the origin variable. This iterates through the list of conversion tables, and extracts the destination numbers from each conversion table, from seed to eventually location.\n\norigin &lt;- seeds\npwalk(parameters, \\(title, data) {\n  \n  out &lt;- extract_map_conversion_data(\n    data, origin, title\n  )\n  \n  origin &lt;&lt;- out |&gt; pull(destination)\n  \n  return(out)\n  \n}, .progress = 'Extracting Destination Data:')\n\nWe then use the min function to find the lowest location number that corresponds to any of the initial seed numbers.\n\norigin |&gt; min()"
  },
  {
    "objectID": "posts/aoc-2023-d3/index.html",
    "href": "posts/aoc-2023-d3/index.html",
    "title": "Advent of Code 2023, Day 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\nWow what a doozie, the increase in difficulty was significant compared to Day 2. This was a fun one though, I think I could have done it in a more efficient way, but I was happy with the solution I came up with.\n\nPart 1\nLet’s start off with the prompt for this puzzle\n\nWhat is the sum of all of the part numbers in the engine schematic?\n\nWe start off with reading in and preparing the data. I replaced all of the . with spaces so that I could use a more simple regex to identify the symbols later.\n\ndata &lt;- read_lines(here('posts', 'aoc-2023-d3', 'puzzle-input.txt'))\n\ndata &lt;- as_tibble(data)\n\ndata &lt;- data |&gt;\n  mutate(row = row_number(),\n         .before = value) |&gt;\n  mutate(value = str_replace_all(value, '\\\\.', ' '))\n\nHere are some observations I made regarding the symbols and the values.\nrow 2, column 28 is a symbol\n\nrow 1, column 27 is a valid spot (above and left of the symbol)\nrow 1, column 28 is a valid spot (above the symbol)\nrow 1, column 29 is a valid spot (above and right of the symbol)\nrow 2, column 27 is a valid spot (left of the symbol)\nrow 2, column 28 is location of the symbol\nrow 2, column 29 is a valid spot (right of the symbol)\nrow 3, column 27 is a valid spot (below and left of the symbol)\nrow 3, column 28 is a valid spot (below the symbol)\nrow 3, column 29 is a valid spot (below and right of the symbol)\n\nThe main observations is that the symbol is always in the center of a 3 by 3 grid. We can use this to identify the locations of the symbols. We need to identify all of the adjacent cells that are valid, and then we can use this to identify the locations of the valid parts numbers.\n\ndata\n\n# A tibble: 140 × 2\n     row value                                                                  \n   &lt;int&gt; &lt;chr&gt;                                                                  \n 1     1 \"                       153  988    502  842         588     441 468  …\n 2     2 \"            805            *      #             %               *    …\n 3     3 \"        914         #   617  201         271     671      52  898    …\n 4     4 \"          #      361          *           *            -4            …\n 5     5 \"490*                          350   *   664        806               …\n 6     6 \"    245               805           467       449   +              31…\n 7     7 \"                        *     150             *              8       …\n 8     8 \"435                 688 8              $      330                 474…\n 9     9 \"   * *    920        %                666  90     786$  221       *  …\n10    10 \" 718 120  @             $ 931                *           *    15   10…\n# ℹ 130 more rows\n\n\nWe write two helper functions to identify the locations of the symbols and the part numbers. We use the [:symbol:] and [:punct:] character classes to identify the symbols, and the [:digit:] character class to identify the part numbers. For the symbols location, I also tabulated the ‘start’, and ‘end’ of the locations adjacent to the symbol in the helper function.\n\nidentify_symbols_location &lt;- \\(x) {\n  str_locate_all(x, '[:symbol:]|[:punct:]')[[1]] |&gt;\n    as_tibble() |&gt; \n    select(symbol = start) |&gt; \n    mutate(\n      symbol_start = symbol - 1,\n      symbol_end = symbol + 1\n    )\n}\n\nidentify_parts_location &lt;- \\(x) {\n  \n  str_locate_all(x, '[:digit:]+')[[1]] |&gt; \n    as_tibble() |&gt; \n    rename(\n      'parts_start' = start,\n      'parts_end' = end\n    )\n  \n}\n\ndata &lt;- data |&gt;\n  mutate(\n    symbol_locations = map(value, \\(value) identify_symbols_location(value)),\n    part_number_locations = map(value, identify_parts_location),\n    part_number_locations = map2(value, part_number_locations, \\(value, part_number_locations) {\n      part_number_locations |&gt;\n        mutate(part_number = str_sub(value, parts_start, parts_end),\n               .before = parts_start)\n    })\n  )\n\nI next construct a tibble of the grid that we are working with. This is a 140 by 140 grid, and we will use this as a template to merge the locations of the symbols, parts number, and the values into.\n\ngrid &lt;- crossing(\n  row = 1:140,\n  column = 1:140\n)\n\nOnce we get the symbol locations we merge this back into our grid. The trick to this part is to identify the adjacent cells to the symbol, and then merge this back into the grid. I did this by first unnesting the symbol locations, and then I created a list of the adjacent cells to the symbol. I then unnested this list, and then merged this back into the grid. I then filled in the symbol column with a 1 if the cell was a symbol, and a 0 if it was not.\n\nsymbol_locations &lt;- data |&gt;\n  select(row, symbol_locations) |&gt;\n  unnest(symbol_locations) |&gt;\n  rowwise() |&gt;\n  mutate(row = list(row + c(-1, 0, 1))) |&gt;\n  unnest(row) |&gt;\n  rowwise() |&gt;\n  mutate(column = list(symbol_start:symbol_end)) |&gt;\n  unnest(column) |&gt;\n  select(row, column) |&gt; \n  mutate(symbol = 1)\n\ngrid &lt;- grid |&gt; \n  left_join(symbol_locations, by = join_by(row, column)) |&gt; \n  mutate(symbol = ifelse(is.na(symbol), 0, symbol))\n\nNext I wanted to extract out the values from the puzzle input. I split the values into individual characters, and then I merged this back into the grid.\n\nvalues &lt;- data |&gt; \n  select(row, value) |&gt; \n  mutate(value = str_split(value, '')) |&gt; \n  unnest(value) |&gt; \n  group_by(row) |&gt;\n  mutate(column = row_number()) |&gt; \n  ungroup()\n\ngrid &lt;- grid |&gt; \n  left_join(values, by = join_by(row, column))\n\nNext I extracted out the location of the parts numbers, then again merged this back into the grid. I further checked to see if the parts numbers are ‘valid’ or adjacent to a symbol. I did this by checking whether the parts number falls on any of the symbol adjacent cells symbol = 1.\n\nparts_number &lt;- data |&gt;\n  select(row, part_number_locations) |&gt;\n  unnest(part_number_locations) |&gt;\n  mutate(i = row_number()) |&gt; \n  rowwise() |&gt;\n  mutate(column = list(parts_start:parts_end)) |&gt; \n  unnest(column) |&gt; \n  select(i, row, column, part_number)\n\ngrid &lt;- grid |&gt; \n  left_join(parts_number, by = join_by(row, column)) |&gt; \n  group_by(row, part_number) |&gt;\n  mutate(\n    valid = ifelse(any(symbol == 1), TRUE, FALSE),\n    valid = ifelse(is.na(part_number), FALSE, valid)\n  ) |&gt; \n  ungroup()\n\nNow that we have all the locations of the symbols, parts numbers, and the values, we can plot the grid. We can see that the symbols are in the center of a 3 by 3 grid (grey). We can also see that any part numbers that fall within the 3 by 3 grid are valid (red).\n\nggplot(grid, aes(x = column, y = row, fill = factor(symbol))) + \n  geom_tile() + \n  coord_equal() +\n  scale_y_reverse() +\n  geom_text(aes(label = value, color = valid), size = 2) +\n  theme_void() + \n  scale_color_manual(values = c('black', 'red')) +\n  scale_fill_manual(values = c('white', 'grey90')) +\n  theme(\n    legend.position = 'none'\n  )\n\n\n\n\nNow that we have all of the data in a tidy dataframe, we can identify the valid parts numbers, and take the sum of the parts numbers to get the answer.\n\ngrid |&gt; \n  filter(!is.na(part_number)) |&gt;\n  filter(valid == TRUE) |&gt; \n  select(i, row, part_number) |&gt;\n  distinct() |&gt;\n  summarize(part_number = sum(as.numeric(part_number)))"
  },
  {
    "objectID": "posts/aoc-2023-d12/index.html",
    "href": "posts/aoc-2023-d12/index.html",
    "title": "Advent of Code 2023, Day 12",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\nlibrary(rlang)\n\n\nPart 1\nThis is a fairly straightforward puzzle. The trickiest part of this puzzle is writing out the regex to match the proper conditions, based on the arragnements.\nThe idea of this puzzle is that we are given a set of conditions of ‘hot springs’, where . are considered ‘working’ and # are considered ‘broken’. We area also given ? which means it can either be working or broken. We are also given a set of arragnements, or ‘sets’ of broken hot springs. For row #1, the arragements of 4,4 indicates there are 2 sets of 4 consecutive broken hot springs, separated by at elast 1 working hot spring. The goal is to determine the number of possible arrangements of hot springs that satisfy the given conditions.\nAs always, lets start off with reading in the puzzle input.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d12', 'puzzle-input.txt')\n)\n\ndata &lt;- data |&gt; \n  as_tibble() |&gt; \n  separate(value, into = c('conditions', 'arrangements'), sep = ' ')\n\ndata\n\n# A tibble: 1,000 × 2\n   conditions          arrangements\n   &lt;chr&gt;               &lt;chr&gt;       \n 1 ???#??????          4,4         \n 2 ??..#??#??.????     5,1         \n 3 ?.?.???????##?????  1,2,8       \n 4 .#????.????#??????  2,1,1,2,1,1 \n 5 ??????#????...#?... 9,1         \n 6 .???.?????.         3,1,1       \n 7 ??..?..???.         1,1         \n 8 ??.??.#..?..??###?? 2,1,1,1,6   \n 9 ????.???##          3,1,3       \n10 ???#?###?#?#.???    1,1,7,1,1   \n# ℹ 990 more rows\n\n\nNow that we have the puzzle input properly formatted, I wrote the following function to help facilitate identifying the number of possible arrangements based on the given criteria. The function reads in the conditions and arrangements, and then creates a grid of all possible combinations of working and broken hot springs. It then filters the grid based on the regex pattern of the given arrangements, and returns the number of rows in the filtered grid.\n\ndetermine_possible_arrangements &lt;- \\(conditions, arrangements) {\n  conditions &lt;- conditions |&gt; str_split('') |&gt; unlist()\n  \n  conditions &lt;- conditions |&gt;\n    set_names(paste0('c', 1:length(conditions)))\n  \n  params &lt;- tibble(!!!conditions) |&gt;\n    pivot_longer(everything(), names_to = 'col_name')\n  \n  params &lt;- params |&gt;\n    deframe() |&gt;\n    as.list()\n  \n  params &lt;- map(params, \\(x) {\n    if (x == '?') {\n      expr(c('.', '#'))\n    } else {\n      expr(as.character(!!x))\n    }\n    \n  })\n  \n  grid &lt;- expr({\n    crossing(!!!params)\n  }) |&gt; eval()\n  \n  arrangements &lt;- arrangements |&gt;\n    str_split(',') |&gt;\n    unlist()\n  \n  arrangements &lt;-\n    map(arrangements, \\(x) paste0('#{', x, '}')) |&gt; unlist()\n  \n  arrangements &lt;- glue::glue_collapse(arrangements, sep = '\\\\.+')\n  \n  arrangements &lt;- glue::glue_collapse(c('^\\\\.*', arrangements, '\\\\.*$')) |&gt; as.character()\n  \n  cols &lt;- paste0('c', 1:length(grid)) |&gt; syms()\n  \n  grid &lt;- grid |&gt;\n    mutate(c = paste(!!!cols, sep = ''))\n  \n  grid &lt;- grid |&gt;\n    filter(str_detect(c, arrangements))\n  \n  nrow(grid)\n  \n}\n\nNow that we have our function written, let’s apply this for all rows in our puzzle input. We can use purrr::map2 to iterate over both the conditions and arrangements columns, and apply our function to each row. We can then sum the results to get our final answer.\n\nresults &lt;- map2(data$conditions, data$arrangements, \\(conditions, arrangements) {\n  \n  determine_possible_arrangements(conditions, arrangements)\n  \n}, .progress = TRUE)\n\nresults |&gt; unlist() |&gt; sum()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Luu, MPH",
    "section": "",
    "text": "Education\n\nMPH in Biostatistics & Epidemiology, 2015\nUniversity of Southern California, Keck School of Medicine\n\n\nBSc in Biological Sciences, 2009\nUniversity of California, Irvine\n\n\n\nExperience\n\nResearch Biostatistician III\nCedars-Sinai Samuel Oschin Comprehensive Cancer Center\nNov 2016 – Present\n\n\nResearch Biostatistician I\nChildren’s Hospital Los Angles, Anesthesia Critical Care Medicine\nMar 2016 – Nov 2016\n\n\nQuality Improvement Analyst\nChildren’s Hospital Los Angles, Neonatology\nJul 2015 – Nov 2016"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 12\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 9\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 8\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 7\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 6\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 5\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 4\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 3\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 2\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code 2023, Day 1\n\n\n\nadvent of code\n\n\n\n\n\n\n\nMichael Luu\n\n\nDec 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaunted Los Angeles - Interactive Mapping using Leaflet\n\n\n\ntidytuesday\n\n\nleaflet\n\n\ngis\n\n\nvisualization\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis (PCA) of Taylor Swift Discography\n\n\n\ntidytuesday\n\n\nvisualization\n\n\npca\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecret Santa Randomizer using ShinyLive 🎅🎄🎁\n\n\n\nshinylive\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of ShinyLive in Quarto\n\n\n\nshinylive\n\n\nquarto\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of WebR in Quarto\n\n\n\nquarto\n\n\nwebr\n\n\n\n\n\n\n\nMichael Luu\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeta-programming with R and rlang\n\n\n\nprogramming\n\n\n\n\n\n\n\nMichael Luu\n\n\nMay 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgramatically generate Quarto tabs\n\n\n\nquarto\n\n\nprogramming\n\n\n\nUtilizing purrr to programatically generate tabs to organize and present a large list of outputs\n\n\n\nMichael Luu\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieve Pubmed publications using the RefManageR package\n\n\n\nprogramming\n\n\n\n\n\n\n\nMichael Luu\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublication Quality Kaplan-Meier Survival Curves using ggplot2\n\n\n\nvisualization\n\n\nsurvival analysis\n\n\n\nThe Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate “time to event” analysis in clinical research. In this illustrative example…\n\n\n\nMichael Luu\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy life in Months - Making a ‘life plot’ in R using ggplot2\n\n\n\nvisualization\n\n\n\nThis blog post is inspired by Sharla Gefland twitter post found…\n\n\n\nMichael Luu\n\n\nOct 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating the New York Times mask utilization survey data with the R opensource Leaflet package\n\n\n\nvisualization\n\n\n\nRecreating the New York Times mask utilization survey data with the R opensource Leaflet package\n\n\n\nMichael Luu\n\n\nOct 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday\n\n\n\nnlp\n\n\nmachine learning\n\n\ntidymodels\n\n\ntidytuesday\n\n\n\nNLP and building a machine learning clasifier on Beyonce and Taylor Swift Lyrics #TidyTuesday\n\n\n\nMichael Luu\n\n\nOct 2, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/aoc-2023-d1/index.html",
    "href": "posts/aoc-2023-d1/index.html",
    "title": "Advent of Code 2023, Day 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\nIt’s the season for Advent of Code 2023. Advent of code is an ‘advent calendar’ of small programming puzzles that begins on 12/1 of every year and last until Christmas 12/25. The following is my solution of Day 1, Part 1 and Part 2 of the 2023 puzzle solved using R.\nWe begin by loading the puzzle input.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d1', 'puzzle-input.txt')\n)\n\ndata &lt;- as_tibble(data)\n\n\nPart 1\nThe first part of the puzzle is fairly straight forward. We begin by using the str_remove_all function from the stringr package to remove all of the characters (and leave only the numeric) from the value column. Next we use str_extract to extract the first and last digit using regex, and paste0 to paste the numbers together to get the ‘calibration’ value. Finally we sum the cal column to get the solution.\n\nres &lt;- data |&gt; \n  mutate(\n    digits = str_remove_all(value, '[:alpha:]'),\n    first = str_extract(digits, '^[:digit:]'),\n    last = str_extract(digits, '[:digit:]$'),\n    cal = as.numeric(paste0(first, last))\n  ) \n\nres\n\n# A tibble: 1,000 × 5\n   value                                 digits first last    cal\n   &lt;chr&gt;                                 &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 9vxfg                                 9      9     9        99\n 2 19qdlpmdrxone7sevennine               197    1     7        17\n 3 1dzntwofour9nineffck                  19     1     9        19\n 4 7bx8hpldgzqjheight                    78     7     8        78\n 5 joneseven2sseven64chvczzn             264    2     4        24\n 6 seven82683                            82683  8     3        83\n 7 7onefour1eighttwo5three               715    7     5        75\n 8 8lmsk871eight7                        88717  8     7        87\n 9 ninefivefive2nine5ntvscdfdsmvqgcbxxxt 25     2     5        25\n10 onepx6hbgdssfivexs                    6      6     6        66\n# ℹ 990 more rows\n\n\n\nres |&gt; \n  summarise(results = sum(cal))\n\n\n\nPart 2\nThe second part of the puzzle was extremely tricky. There are a number of special cases where the words are overlapping. My messy solution was to ‘fix’ those overlapping words such as ‘twone’ to ‘twoone’, using the str_replace_all function, then using additional str_replace_all to convert the words to numeric. Finally we would use a similar solution as above in extracting the first and last digit.\n\nconvert_char_to_numeric &lt;- \\(x) {\n  \n  # special cases with overlapping characters\n  x &lt;- str_replace_all(x, 'twone', 'twoone') |&gt; \n    str_replace_all('oneight', 'oneeight') |&gt; \n    str_replace_all('sevenine', 'sevennine') |&gt; \n    str_replace_all('threeight', 'threeeight') |&gt; \n    str_replace_all('fiveight', 'fiveeight') |&gt; \n    str_replace_all('eightwo', 'eighttwo') |&gt; \n    str_replace_all('eighthree', 'eightthree') |&gt; \n    str_replace_all('nineight', 'nineeight')\n  \n  x &lt;- str_replace_all(x, 'one', '1') |&gt;\n    str_replace_all('two', '2') |&gt;\n    str_replace_all('three', '3') |&gt;\n    str_replace_all('four', '4') |&gt;\n    str_replace_all('five', '5') |&gt;\n    str_replace_all('six', '6') |&gt;\n    str_replace_all('seven', '7') |&gt;\n    str_replace_all('eight', '8') |&gt;\n    str_replace_all('nine', '9')\n  \n}\n\nres &lt;- data |&gt; \n  mutate(\n    value2 = convert_char_to_numeric(value),\n    digits = str_remove_all(value2, '[:alpha:]'),\n    first = str_extract(digits, '^[:digit:]'),\n    last = str_extract(digits, '[:digit:]$'),\n    cal = as.numeric(paste0(first, last))\n  ) \n\nres\n\n# A tibble: 1,000 × 6\n   value                                 value2         digits first last    cal\n   &lt;chr&gt;                                 &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 9vxfg                                 9vxfg          9      9     9        99\n 2 19qdlpmdrxone7sevennine               19qdlpmdrx1779 191779 1     9        19\n 3 1dzntwofour9nineffck                  1dzn2499ffck   12499  1     9        19\n 4 7bx8hpldgzqjheight                    7bx8hpldgzqjh8 788    7     8        78\n 5 joneseven2sseven64chvczzn             j172s764chvcz… 172764 1     4        14\n 6 seven82683                            782683         782683 7     3        73\n 7 7onefour1eighttwo5three               71418253       71418… 7     3        73\n 8 8lmsk871eight7                        8lmsk87187     887187 8     7        87\n 9 ninefivefive2nine5ntvscdfdsmvqgcbxxxt 955295ntvscdf… 955295 9     5        95\n10 onepx6hbgdssfivexs                    1px6hbgdss5xs  165    1     5        15\n# ℹ 990 more rows\n\n\n\nres |&gt; summarise(results = sum(cal))"
  },
  {
    "objectID": "posts/aoc-2023-d2/index.html",
    "href": "posts/aoc-2023-d2/index.html",
    "title": "Advent of Code 2023, Day 2",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\n\nPart 1\nThe solution for Part 1 was actually quite simple. The major complexity of this puzzle is due to the data not being in a ‘tidy’ format. The majority of the code below is just cleaning the data\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d2', 'puzzle-input.txt')\n)\n\ndata &lt;- as_tibble(data)\n\ndata &lt;- data |&gt; \n  separate(value, sep = ': ', into = c('game', 'value')) |&gt; \n  separate(value, sep = '; ', into = paste0('set', 1:10)) |&gt; \n  pivot_longer(contains('set'), names_to = 'set') |&gt; \n  filter(!is.na(value)) |&gt; \n  separate(value, sep = ', ', into = paste0('cube', 1:3)) |&gt; \n  pivot_longer(contains('cube'), names_to = 'cube') |&gt; \n  filter(!is.na(value)) |&gt; \n  separate(value, sep = ' ', into = c('count', 'color')) |&gt; \n  pivot_wider(names_from = 'color', values_from = 'count', values_fill = '0') |&gt; \n  mutate(across(c('blue', 'red', 'green'), as.numeric)) |&gt; \n  mutate(game_id = str_extract(game, '\\\\d+'), .after = game)\n\ndata\n\n# A tibble: 1,161 × 7\n   game   game_id set   cube   blue   red green\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Game 1 1       set1  cube1     1     0     0\n 2 Game 1 1       set1  cube2     0     1     0\n 3 Game 1 1       set2  cube1     0    10     0\n 4 Game 1 1       set3  cube1     0     8     0\n 5 Game 1 1       set3  cube2     1     0     0\n 6 Game 1 1       set3  cube3     0     0     1\n 7 Game 1 1       set4  cube1     0     0     1\n 8 Game 1 1       set4  cube2     5     0     0\n 9 Game 2 2       set1  cube1     0     0     9\n10 Game 2 2       set1  cube2     0    11     0\n# ℹ 1,151 more rows\n\n\nNow that we have the data in a ‘tidy’ format, let’s refer back to the puzzle prompt.\n\nThe Elf would first like to know which games would have been possible if the bag contained only 12 red cubes, 13 green cubes, and 14 blue cubes?\n\nWe can figure this out by using filter on each color columns and checking whether any of the games contained any invalid number of cubes.\n\nres &lt;- data |&gt; \n  group_by(game) |&gt;\n  mutate(\n    valid = case_when(\n      any(red &gt; 12 | green &gt; 13 | blue &gt; 14) ~ FALSE,\n      .default = TRUE\n    )\n  ) |&gt; \n  ungroup() |&gt; \n  filter(valid == TRUE)\n\nres\n\n# A tibble: 539 × 8\n   game   game_id set   cube   blue   red green valid\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;\n 1 Game 1 1       set1  cube1     1     0     0 TRUE \n 2 Game 1 1       set1  cube2     0     1     0 TRUE \n 3 Game 1 1       set2  cube1     0    10     0 TRUE \n 4 Game 1 1       set3  cube1     0     8     0 TRUE \n 5 Game 1 1       set3  cube2     1     0     0 TRUE \n 6 Game 1 1       set3  cube3     0     0     1 TRUE \n 7 Game 1 1       set4  cube1     0     0     1 TRUE \n 8 Game 1 1       set4  cube2     5     0     0 TRUE \n 9 Game 2 2       set1  cube1     0     0     9 TRUE \n10 Game 2 2       set1  cube2     0    11     0 TRUE \n# ℹ 529 more rows\n\n\nOnce we have the valid games, we can simply sum the game_id columns to get the solution\n\nres |&gt; pull(game_id) |&gt; unique() |&gt; \n  as.numeric() |&gt; sum()\n\n\n\nPart 2\nAgain, the solution for Part 2 is quite simple once we have the data in a ‘tidy’ format.\n\nFor each game, find the minimum set of cubes that must have been present. What is the sum of the power of these sets?\n\nThe elfs would like to know what is the ‘minimum’ number of cubes that has to be in each bag for each of the games played. The solution is the sum of the ‘power’ (defined as the product of the number of cubes for each color). We can solve this by taking the maximum number of cubes observed for a given game, then taking the product of the maximum number of cubes for each color.\n\nres &lt;- data |&gt; \n  group_by(game) |&gt;\n  summarise(across(c(blue, red, green), \\(x) max(x))) |&gt; \n  mutate(power = blue * red * green)\n\nres\n\n# A tibble: 100 × 5\n   game      blue   red green power\n   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Game 1       5    10     1    50\n 2 Game 10      8     9    12   864\n 3 Game 100     5     7     1    35\n 4 Game 11      1    15     3    45\n 5 Game 12      8     9     7   504\n 6 Game 13      8     2    18   288\n 7 Game 14      4     3     5    60\n 8 Game 15     15     3     7   315\n 9 Game 16     20     6    13  1560\n10 Game 17     11     4     1    44\n# ℹ 90 more rows\n\n\n\nres |&gt; summarise(total_power = sum(power))"
  },
  {
    "objectID": "posts/aoc-2023-d4/index.html",
    "href": "posts/aoc-2023-d4/index.html",
    "title": "Advent of Code 2023, Day 4",
    "section": "",
    "text": "Part 1\n\nlibrary(tidyverse)\nlibrary(here)\n\nThe complexity of this puzzle dropped a bit for Day 4. Definitely wasn’t as difficulty as Day 3.\nLet’s again take a look at the prompt for the puzzle\n\nTake a seat in the large pile of colorful cards. How many points are they worth in total?\n\nThe idea is that we have a whole bunch of scratch cards, with a set of winning numbers and your set of numbers. We need to identify the total number of matches for each card, and then calculate the points for each card.\nWe calculate the points by using the following formula.\n\nIf there are no matches then the card is worth 0 points\nIf there are 1 match then the card is worth 1 point\nIf there are more than 1 match, then the card is worth 2 times the number of matches\n\nLet’s first read in the data\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d4', 'puzzle-input.txt')\n)\n\ndata &lt;- as_tibble(data)\n\ndata\n\n# A tibble: 196 × 1\n   value                                                                        \n   &lt;chr&gt;                                                                        \n 1 Card   1: 61 73 92 28 96 76 32 62 44 53 | 61 17 26 13 92  5 73 29 53 42 62 4…\n 2 Card   2:  3 88 36 12  2  9 15 55 21 89 | 23 39 98 36  2 24  9  3 78 95 55 3…\n 3 Card   3: 96 44 52 56 82 89 73 50  9 68 | 39 71 64 32 13 57 56 67 34 84 51 5…\n 4 Card   4: 54 84 76 44 38 33 12 17 93 94 | 18 21 53 11  7 98 78 92  9 32 29 5…\n 5 Card   5:  8 11 33 98 37 80 39 76 53 91 | 82 35 27 29 50 73 24  4  5 53 93 6…\n 6 Card   6: 10 58 39 28 35 79 14 73 64 96 | 90 86 38 93 74 29 21 14 33 16 85  …\n 7 Card   7:  1 96 73 38 64 87 45 25 99 10 | 11 30 96 43 17 72 24 55 79 64 98 4…\n 8 Card   8: 24  3 23 50 58 35 57 51 22  2 | 95 50 22 75 27 57 72 25 12 61 82 1…\n 9 Card   9: 98 59 30 22 10 69 68 17 48  8 | 22 75 34 63  7 72 30 73 19 13 35 8…\n10 Card  10: 82 32 48 60 17 85 97 22 26 87 | 33 49 81 29 70  8 74 45 97 68 36 7…\n# ℹ 186 more rows\n\n\nNow that the data is read in, let’s tidy the data up a bit. We’ll use the separate function to split the data into the card and the value. We’ll then use str_extract to extract the card number from the card string. We’ll then use separate again to split the value into the winning numbers and the elf’s numbers. We’ll then use str_split to split the winning numbers and elf’s numbers into a list of numbers. We’ll then use as.numeric to convert the numbers from strings to numbers. We’ll then use na.omit to remove any NA values from the list of numbers.\n\ndata &lt;- data |&gt;\n  separate(value, c('card', 'value'), sep = ': ') |&gt;\n  mutate(card = str_extract(card, '\\\\d+') |&gt; as.numeric()) |&gt;\n  separate(value, c('winning_numbers', 'elfs_numbers'), sep = ' \\\\| ') |&gt;\n  mutate(winning_numbers = str_split(winning_numbers, ' ') |&gt;\n           map(\\(x) as.numeric(x)),) |&gt;\n  mutate(winning_numbers = map(winning_numbers, \\(x) na.omit(x)))\n\ndata &lt;- data |&gt; \n  mutate(\n    elfs_numbers = str_split(elfs_numbers, ' '),\n    elfs_numbers = map(elfs_numbers, \\(x) as.numeric(x) |&gt; na.omit())\n  )\n\ndata\n\n# A tibble: 196 × 3\n    card winning_numbers elfs_numbers\n   &lt;dbl&gt; &lt;list&gt;          &lt;list&gt;      \n 1     1 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 2     2 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 3     3 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 4     4 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 5     5 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 6     6 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 7     7 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 8     8 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n 9     9 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n10    10 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;  \n# ℹ 186 more rows\n\n\nNow that the data is tidied up, let’s calculate the number of matches for each card. We’ll use the intersect function to calculate the number of matches. We’ll then use length to calculate the number of matches. We’ll then use map_int to calculate the points for each card, based on the above rule.\n\ncalculate_matches &lt;- \\(winning_numbers, elfs_numbers) {\n  intersect(elfs_numbers, winning_numbers) |&gt;\n    length()\n  \n}\n\ndata &lt;- data |&gt; \n  rowwise() |&gt; \n  mutate(\n    matches = calculate_matches(winning_numbers, elfs_numbers)\n  ) |&gt; \n  ungroup()\n\ndata &lt;- data |&gt;\n  mutate(points = map_int(matches, \\(matches) {\n    if (matches == 0) {\n      points &lt;- 0\n    } else if (matches == 1) {\n      points &lt;- 1\n    } else if (matches &gt; 1) {\n      points &lt;- 2 ^ (matches - 1)\n    }\n    \n    return(points)\n  }))\n\ndata\n\n# A tibble: 196 × 5\n    card winning_numbers elfs_numbers matches points\n   &lt;dbl&gt; &lt;list&gt;          &lt;list&gt;         &lt;int&gt;  &lt;int&gt;\n 1     1 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;        10    512\n 2     2 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;        10    512\n 3     3 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         1      1\n 4     4 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         1      1\n 5     5 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         2      2\n 6     6 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         3      4\n 7     7 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;        10    512\n 8     8 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;        10    512\n 9     9 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         5     16\n10    10 &lt;dbl [10]&gt;      &lt;dbl [25]&gt;         2      2\n# ℹ 186 more rows\n\n\nFinally we just add up the points, and we have the answer\n\nsum(data$points)"
  },
  {
    "objectID": "posts/aoc-2023-d6/index.html",
    "href": "posts/aoc-2023-d6/index.html",
    "title": "Advent of Code 2023, Day 6",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\nWhat is going on with these puzzles?! Difficulty dropped quite a bit again for the even day puzzles.\n\nPart 1\nLet’s recap on the puzzle prompt.\n\nDetermine the number of ways you could beat the record in each race. What do you get if you multiply these numbers together?\n\nLet’s first start off with reading in the puzzle input and tidy the data.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d6', 'puzzle-input.txt')\n)\n\ndata &lt;- data |&gt;\n  as_tibble() |&gt;\n  separate(value, c('type', 'value'), sep = ': ') |&gt;\n  mutate(\n    value = str_trim(value),\n    value = str_split(value, ' '),\n    value = map(value, \\(x) as.numeric(x) |&gt; na.omit())\n  ) |&gt;\n  unnest(value) |&gt;\n  mutate(race = rep(1:4, 2)) |&gt;\n  pivot_wider(names_from = type, values_from = value)\n\ndata &lt;- data |&gt; janitor::clean_names()\n\ndata\n\n# A tibble: 4 × 3\n   race  time distance\n  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     1    53      333\n2     2    83     1635\n3     3    72     1289\n4     4    88     1532\n\n\nThe idea of this puzzle is that we have 4 toy boat races that we need to figure out the number of ways that we can beat the record for the boat races. For each toy boat, we can hold the button down to make the boat go faster. The speed is directly correlated with the amount of time the button is held e.g. 1 milisecond = 1 milimeter/milisecond, 2 milisecond = 2 milimeter/milisecond, etc. However for each milisecond we hold the button down, the time will go against the amount of time alotted to travel the distance. If we hold the button down for too long, then we won’t have time to travel. If we hold the button down for too short, then the boat won’t have any speed to travel the necessary distance to beat the record.\nLet’s create a function that allows us to tabulate the button hold, dependent on the total time and the amount of time the button was held.\n\ncalculate_total_distance &lt;- \\(total_time, button_hold) {\n  speed &lt;- button_hold\n  \n  time_remaining &lt;- total_time - button_hold\n  total_distance &lt;- speed * time_remaining\n  \n  total_distance\n  \n}\n\nLet’s create a tibble with all possible amount of time to hold the button from 0 milisecond to the total time for each race. Now let’s tabulate the total distance traveled for all possible time the button was held. Finally let’s identify the rows, where the total distance traveled from our toy boat exceeeded the distance traveled for the record. We can then count the total rows for each record, and determine the product for the answer.\n\nres &lt;- pmap(data, \\(race, time, distance) {\n  temp &lt;- tibble(total_time = time,\n                 button_hold = 0:time,\n                 distance)\n  \n  temp |&gt;\n    mutate(total_distance = calculate_total_distance(total_time, button_hold)) |&gt; \n    filter(total_distance &gt; distance) |&gt; \n    nrow()\n  \n})\n\nres\n\n[[1]]\n[1] 38\n\n[[2]]\n[1] 18\n\n[[3]]\n[1] 5\n\n[[4]]\n[1] 41\n\n\n\nres |&gt; unlist() |&gt; prod()\n\nfirst attempt: 140220\n\n\nPart 2\n\nHow many ways can you beat the record in this one much longer race?\n\nThe strategy for part 2 is identical for part 1, however now we have 1 single race instead of 4 races. We then calculate the total number of ways we can beat the record in this much longe race\n\ndata &lt;- read_lines(here('posts', 'aoc-2023-d6', 'puzzle-input.txt'))\n\ndata &lt;- data |&gt;\n  as_tibble() |&gt;\n  separate(value, c('type', 'value'), sep = ': ') |&gt;\n  mutate(\n    value = str_trim(value, side = 'both'),\n    value = str_remove_all(value, ' '),\n    value = as.numeric(value)\n  ) |&gt;\n  pivot_wider(names_from = type, values_from = value) |&gt;\n  janitor::clean_names()\n\ndata\n\n# A tibble: 1 × 2\n      time distance\n     &lt;dbl&gt;    &lt;dbl&gt;\n1 53837288  3.33e14\n\n\n\nres &lt;- tibble(\n  total_time = data$time,\n  button_hold = 0:data$time,\n  distance = data$distance\n)\n\nres\n\n# A tibble: 53,837,289 × 3\n   total_time button_hold distance\n        &lt;dbl&gt;       &lt;int&gt;    &lt;dbl&gt;\n 1   53837288           0  3.33e14\n 2   53837288           1  3.33e14\n 3   53837288           2  3.33e14\n 4   53837288           3  3.33e14\n 5   53837288           4  3.33e14\n 6   53837288           5  3.33e14\n 7   53837288           6  3.33e14\n 8   53837288           7  3.33e14\n 9   53837288           8  3.33e14\n10   53837288           9  3.33e14\n# ℹ 53,837,279 more rows\n\n\n\nres |&gt;\n  mutate(total_distance = calculate_total_distance(total_time, button_hold)) |&gt;\n  filter(total_distance &gt; distance) |&gt; \n  nrow()"
  },
  {
    "objectID": "posts/aoc-2023-d8/index.html",
    "href": "posts/aoc-2023-d8/index.html",
    "title": "Advent of Code 2023, Day 8",
    "section": "",
    "text": "Part 1\nThis was definitely a fun puzzle. As always, lets start off with reading in the puzzle input, and tidy it up into something we can work with.\n\nlibrary(tidyverse)\nlibrary(here)\n\nThere’s two parts to the input, the first part is the instructions, and the second part are the nodes that can be traversed.\n\ndata &lt;- read_lines(\n  here('posts', 'aoc-2023-d8', 'puzzle-input.txt')\n)\n\ninstructions &lt;- data[[1]]\ninstructions &lt;- instructions |&gt; str_split('') \ninstructions &lt;- instructions[[1]]\n\ndata &lt;- data[2:776] |&gt; \n  as_tibble() |&gt; \n  filter(value != '') |&gt; \n  separate(value, into = c('id', 'direction'), sep = ' = ') |&gt; \n  separate(direction, into = c('L', 'R'), sep = ', ') |&gt; \n  mutate(L = str_remove(L, '\\\\(')) |&gt; \n  mutate(R = str_remove(R, '\\\\)')) |&gt; \n  arrange(id)\n\ndata &lt;- data |&gt; \n  pivot_longer(cols = c('L', 'R'), names_to = 'direction', values_to = 'value')\n\nNow that we have the data into something we can work with let’s recap on the instructions. These are the set of instructions that needs to be traversed, e.g. the L or the R node in the data.\nThe following are a vector of instructions on whether to choose the L or the R for each of the nodes.\n\ninstructions\n\n  [1] \"L\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\" \"R\" \"L\" \"R\" \"L\"\n [19] \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"L\" \"L\"\n [37] \"R\" \"R\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\"\n [55] \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\"\n [73] \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"L\" \"L\" \"L\" \"R\"\n [91] \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\"\n[109] \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\"\n[127] \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\"\n[145] \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\"\n[163] \"R\" \"L\" \"L\" \"L\" \"L\" \"R\" \"R\" \"L\" \"L\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\"\n[181] \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\"\n[199] \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\"\n[217] \"R\" \"L\" \"L\" \"R\" \"L\" \"L\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\"\n[235] \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"L\" \"L\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\"\n[253] \"R\" \"L\" \"R\" \"L\" \"R\" \"L\" \"L\" \"R\" \"L\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"L\" \"R\"\n[271] \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\" \"R\" \"R\" \"R\" \"L\" \"R\" \"R\" \"L\"\n[289] \"L\" \"R\" \"R\" \"R\" \"R\"\n\n\nLet’s also have a look at the nodes that can be traversed.\n\ndata\n\n# A tibble: 1,548 × 3\n   id    direction value\n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;\n 1 AAA   L         PBJ  \n 2 AAA   R         RXK  \n 3 BBA   L         TVV  \n 4 BBA   R         VLF  \n 5 BBJ   L         MCN  \n 6 BBJ   R         RJD  \n 7 BBN   L         FPQ  \n 8 BBN   R         NFC  \n 9 BBQ   L         MHL  \n10 BBQ   R         VKQ  \n# ℹ 1,538 more rows\n\n\nThe prompt of the puzzle is as follows.\n\nStarting at AAA, follow the left/right instructions. How many steps are required to reach ZZZ?\n\nWe start off with writing a simple function that traces the path of the instructions. e.g. identify the node, and extract the value of the node.\n\ntrace_id_direction &lt;- \\(data, start_id, dir) {\n  data |&gt;\n    filter(id == start_id) |&gt;\n    filter(direction == dir) |&gt;\n    pull(value)\n  \n}\n\nNow we write a simple while loop to traverse the instructions. We start off at the node AAA, and then we trace the path of the instructions. We then update the node to the new node, and repeat the process until we reach the node ZZZ. The total number of iterations is the solution to the puzzle\n\ni &lt;- 1\nposition_id &lt;- 'AAA'\nresults &lt;- c()\nwhile (position_id != 'ZZZ') {\n  res &lt;- map(instructions, \\(x) {\n    position_id &lt;&lt;- trace_id_direction(data, position_id, x)\n    \n    cat('position_id: ', position_id, ', iteration: ', i, '\\n')\n    i &lt;&lt;- i + 1\n    \n    return(position_id)\n  }) |&gt; unlist()\n  \n  results &lt;- c(results, res)\n  \n}"
  },
  {
    "objectID": "posts/ggplot2-km/index.html",
    "href": "posts/ggplot2-km/index.html",
    "title": "Publication Quality Kaplan-Meier Survival Curves using ggplot2",
    "section": "",
    "text": "The Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate “time to event” analysis in clinical research. In this illustrative example, I will be using the veterans data from the survival package to construct KM survival curves using ggplot2 and building the figure from basic geoms within the package. I will also provide examples of other publicly available packages that can facilitate in constructing KM figures that utilizes ggplot2. I believe this is a good exercise and illustrative example in potentially more advanced and little known techniques in ggplot2, as well as provide insight in the flexibility and capabilities that are available in this package.\nThe veterans data comes from a randomised trial of two treatment regimens for lung cancer. Let’s start off by loading the veterans data from the survival and have a look at the data that we are currently working with.\n\ndf &lt;- survival::veteran %&gt;% as_tibble()\n\nglimpse(df)\n\nRows: 137\nColumns: 8\n$ trt      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ celltype &lt;fct&gt; squamous, squamous, squamous, squamous, squamous, squamous, s…\n$ time     &lt;dbl&gt; 72, 411, 228, 126, 118, 10, 82, 110, 314, 100, 42, 8, 144, 25…\n$ status   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n$ karno    &lt;dbl&gt; 60, 70, 60, 60, 70, 20, 40, 80, 50, 70, 60, 40, 30, 80, 70, 6…\n$ diagtime &lt;dbl&gt; 7, 5, 3, 9, 11, 5, 10, 29, 18, 6, 4, 58, 4, 9, 11, 3, 9, 2, 4…\n$ age      &lt;dbl&gt; 69, 64, 38, 63, 65, 49, 69, 68, 43, 70, 81, 63, 63, 52, 48, 6…\n$ prior    &lt;dbl&gt; 0, 10, 0, 10, 10, 0, 10, 0, 0, 0, 0, 10, 0, 10, 10, 0, 0, 0, …\n\ndf\n\n# A tibble: 137 × 8\n     trt celltype  time status karno diagtime   age prior\n   &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 squamous    72      1    60        7    69     0\n 2     1 squamous   411      1    70        5    64    10\n 3     1 squamous   228      1    60        3    38     0\n 4     1 squamous   126      1    60        9    63    10\n 5     1 squamous   118      1    70       11    65    10\n 6     1 squamous    10      1    20        5    49     0\n 7     1 squamous    82      1    40       10    69    10\n 8     1 squamous   110      1    80       29    68     0\n 9     1 squamous   314      1    50       18    43     0\n10     1 squamous   100      0    70        6    70     0\n# ℹ 127 more rows\n\n\nThe codebook for the dataset is provided below as follows:\n\ntrt: 1=standard 2=test\ncelltype: 1=squamous, 2=smallcell, 3=adeno, 4=large\ntime: survival time (days)\nstatus: censoring status\nkarno: Karnofsky performance score (100=good)\ndiagtime: months from diagnosis to randomisation\nage: in years\nprior: prior therapy 0=no, 10=yes\n\nTo handle ‘time to event’ data in R, we will first need to construct a survival object that encapsulates both the time to event information time in our dataset as well as the event/censoring variable status. We can then fit the data using the survfit() function by constructing a formula with our response variable (survival object) on the left of the ~ and the explanatory variable trt on the right. The summary() of the object from survfit() provides us the probability of survival for a given treatment over time.\n\nfit &lt;- survfit(Surv(time, status) ~ trt, data = df)\n\nsummary(fit)\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = df)\n\n                trt=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    3     69       1   0.9855  0.0144      0.95771        1.000\n    4     68       1   0.9710  0.0202      0.93223        1.000\n    7     67       1   0.9565  0.0246      0.90959        1.000\n    8     66       2   0.9275  0.0312      0.86834        0.991\n   10     64       2   0.8986  0.0363      0.83006        0.973\n   11     62       1   0.8841  0.0385      0.81165        0.963\n   12     61       2   0.8551  0.0424      0.77592        0.942\n   13     59       1   0.8406  0.0441      0.75849        0.932\n   16     58       1   0.8261  0.0456      0.74132        0.921\n   18     57       2   0.7971  0.0484      0.70764        0.898\n   20     55       1   0.7826  0.0497      0.69109        0.886\n   21     54       1   0.7681  0.0508      0.67472        0.874\n   22     53       1   0.7536  0.0519      0.65851        0.862\n   27     51       1   0.7388  0.0529      0.64208        0.850\n   30     50       1   0.7241  0.0539      0.62580        0.838\n   31     49       1   0.7093  0.0548      0.60967        0.825\n   35     48       1   0.6945  0.0556      0.59368        0.812\n   42     47       1   0.6797  0.0563      0.57782        0.800\n   51     46       1   0.6650  0.0570      0.56209        0.787\n   52     45       1   0.6502  0.0576      0.54649        0.774\n   54     44       2   0.6206  0.0587      0.51565        0.747\n   56     42       1   0.6059  0.0591      0.50040        0.734\n   59     41       1   0.5911  0.0595      0.48526        0.720\n   63     40       1   0.5763  0.0598      0.47023        0.706\n   72     39       1   0.5615  0.0601      0.45530        0.693\n   82     38       1   0.5467  0.0603      0.44049        0.679\n   92     37       1   0.5320  0.0604      0.42577        0.665\n   95     36       1   0.5172  0.0605      0.41116        0.651\n  100     34       1   0.5020  0.0606      0.39615        0.636\n  103     32       1   0.4863  0.0607      0.38070        0.621\n  105     31       1   0.4706  0.0608      0.36537        0.606\n  110     30       1   0.4549  0.0607      0.35018        0.591\n  117     29       2   0.4235  0.0605      0.32017        0.560\n  118     27       1   0.4079  0.0602      0.30537        0.545\n  122     26       1   0.3922  0.0599      0.29069        0.529\n  126     24       1   0.3758  0.0596      0.27542        0.513\n  132     23       1   0.3595  0.0592      0.26031        0.496\n  139     22       1   0.3432  0.0587      0.24535        0.480\n  143     21       1   0.3268  0.0582      0.23057        0.463\n  144     20       1   0.3105  0.0575      0.21595        0.446\n  151     19       1   0.2941  0.0568      0.20151        0.429\n  153     18       1   0.2778  0.0559      0.18725        0.412\n  156     17       1   0.2614  0.0550      0.17317        0.395\n  162     16       2   0.2288  0.0527      0.14563        0.359\n  177     14       1   0.2124  0.0514      0.13218        0.341\n  200     12       1   0.1947  0.0501      0.11761        0.322\n  216     11       1   0.1770  0.0486      0.10340        0.303\n  228     10       1   0.1593  0.0468      0.08956        0.283\n  250      9       1   0.1416  0.0448      0.07614        0.263\n  260      8       1   0.1239  0.0426      0.06318        0.243\n  278      7       1   0.1062  0.0400      0.05076        0.222\n  287      6       1   0.0885  0.0371      0.03896        0.201\n  314      5       1   0.0708  0.0336      0.02793        0.180\n  384      4       1   0.0531  0.0295      0.01788        0.158\n  392      3       1   0.0354  0.0244      0.00917        0.137\n  411      2       1   0.0177  0.0175      0.00256        0.123\n  553      1       1   0.0000     NaN           NA           NA\n\n                trt=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     68       2   0.9706  0.0205      0.93125        1.000\n    2     66       1   0.9559  0.0249      0.90830        1.000\n    7     65       2   0.9265  0.0317      0.86647        0.991\n    8     63       2   0.8971  0.0369      0.82766        0.972\n   13     61       1   0.8824  0.0391      0.80900        0.962\n   15     60       2   0.8529  0.0429      0.77278        0.941\n   18     58       1   0.8382  0.0447      0.75513        0.930\n   19     57       2   0.8088  0.0477      0.72056        0.908\n   20     55       1   0.7941  0.0490      0.70360        0.896\n   21     54       1   0.7794  0.0503      0.68684        0.884\n   24     53       2   0.7500  0.0525      0.65383        0.860\n   25     51       3   0.7059  0.0553      0.60548        0.823\n   29     48       1   0.6912  0.0560      0.58964        0.810\n   30     47       1   0.6765  0.0567      0.57394        0.797\n   31     46       1   0.6618  0.0574      0.55835        0.784\n   33     45       1   0.6471  0.0580      0.54289        0.771\n   36     44       1   0.6324  0.0585      0.52754        0.758\n   43     43       1   0.6176  0.0589      0.51230        0.745\n   44     42       1   0.6029  0.0593      0.49717        0.731\n   45     41       1   0.5882  0.0597      0.48216        0.718\n   48     40       1   0.5735  0.0600      0.46724        0.704\n   49     39       1   0.5588  0.0602      0.45244        0.690\n   51     38       2   0.5294  0.0605      0.42313        0.662\n   52     36       2   0.5000  0.0606      0.39423        0.634\n   53     34       1   0.4853  0.0606      0.37993        0.620\n   61     33       1   0.4706  0.0605      0.36573        0.606\n   73     32       1   0.4559  0.0604      0.35163        0.591\n   80     31       2   0.4265  0.0600      0.32373        0.562\n   84     28       1   0.4112  0.0597      0.30935        0.547\n   87     27       1   0.3960  0.0594      0.29509        0.531\n   90     25       1   0.3802  0.0591      0.28028        0.516\n   95     24       1   0.3643  0.0587      0.26560        0.500\n   99     23       2   0.3326  0.0578      0.23670        0.467\n  111     20       2   0.2994  0.0566      0.20673        0.434\n  112     18       1   0.2827  0.0558      0.19203        0.416\n  133     17       1   0.2661  0.0550      0.17754        0.399\n  140     16       1   0.2495  0.0540      0.16326        0.381\n  164     15       1   0.2329  0.0529      0.14920        0.363\n  186     14       1   0.2162  0.0517      0.13538        0.345\n  201     13       1   0.1996  0.0503      0.12181        0.327\n  231     12       1   0.1830  0.0488      0.10851        0.308\n  242     10       1   0.1647  0.0472      0.09389        0.289\n  283      9       1   0.1464  0.0454      0.07973        0.269\n  340      8       1   0.1281  0.0432      0.06609        0.248\n  357      7       1   0.1098  0.0407      0.05304        0.227\n  378      6       1   0.0915  0.0378      0.04067        0.206\n  389      5       1   0.0732  0.0344      0.02912        0.184\n  467      4       1   0.0549  0.0303      0.01861        0.162\n  587      3       1   0.0366  0.0251      0.00953        0.140\n  991      2       1   0.0183  0.0180      0.00265        0.126\n  999      1       1   0.0000     NaN           NA           NA\n\n\nThe base R plotting method provides us with a basic KM figure. We can generate the figure by using the plot() function on the fit object.\n\nplot(fit)\n\n\n\n\nThe GGally package also includes ggsurv() which actually uses the ggplot2 in the backend to construct the figure.\n\nGGally::ggsurv(fit)\n\n\n\n\nAn even further improved KM figure comes from the survminer package that includes a ‘Number at risk’ table that is commonly show in combination with the KM figure.\n\nsurvminer::ggsurvplot(fit, data = df, risk.table = T)\n\n\n\n\nThe KM figure I’m constructing is going to be based on the survminer figure, that includes the secondary ‘Number at risk’ table.\nWe can start by estimating the survival estimates from day 0 to day 500, I chose 500 since it appears that survival trails off after 500 days and this is a method of truncating the figure. Then we can extract the survival estimates into a a structured tidy tibble.\n\ns &lt;- summary(fit, times = seq(0, 500, 1), extend = T)\n\nplot_data &lt;- tibble(\n  'time' = s$time,\n  'n.risk' = s$n.risk,\n  'n.event' = s$n.event,\n  'n.censor' = s$n.censor,\n  'estimate' = s$surv,\n  'std.error' = s$std.err,\n  'strata' = s$strata\n)\n\nNow that we have the ‘tidied’ data, we can start by constructing the base plot we will use to build from. We will map the x axis to time, the y axis to estimate, and the fill to strata.\n\np &lt;- ggplot(plot_data, aes(x = time, y = estimate, color = strata))\n\np\n\n\n\n\nThe primary geom in building the figure is geom_step()\n\np &lt;- p + geom_step(aes(linetype = strata), size = 1)\n\np\n\n\n\n\nThis is pretty close to the plot that is provided from the GGally package already, we just need a few more steps to further clean up the axes, adjust the aesthetics, and to add a theme. I also further expanded the x axes to 550 to provide some additional room for curve annotations.\n\np &lt;- p + \n  scale_x_continuous(breaks = seq(0, 500, 50)) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_classic() +\n  theme(legend.position = 'top',\n        axis.title = element_text(face = 'bold')) +\n  labs(x = 'Days', y = 'Probability of Survival') + \n  coord_cartesian(xlim = c(0, 550)) + \n  ggsci::scale_color_d3()\n\n\np\n\n\n\n\nWe can further supplement this figure by adding markers on the curves using the ggrepel package. The simplest method I found to identify the coordinates of the ideal location of the annotations is by taking the last point of the curves by strata. Then we can use the geom_text_repel() function to add the text label to the curves accordingly. Now that we have the annotations on the figure, we can remove the legends to give the actual figure some additional room.\n\nannotate_data &lt;- plot_data %&gt;%\n  group_by(strata) %&gt;%\n  slice_tail(n = 1)\n\np &lt;- p + \n  ggrepel::geom_text_repel(data = annotate_data, aes(x = time, y = estimate, label = strata),\n                           xlim = c(500, NA)) + \n  theme(legend.position = 'none')\n\np\n\n\n\n\nNext, we can construct the ‘At risk’ table below the figure we just constructed. The table is actually a ggplot, where we are constructing a table of number of at risk plotted by time on the x axis and strata on the y axis. Since the time interval for the KM figure is per every 50 days, we will extract the ‘At risk’ data similarly on a per 100 days basis. The most important concept to remember is to make the scale of the x axis scale_x_continuous() is identical to the KM figure to have the alignment match between the two. The number at risk is then plotted using geom_text() with n.risk as the label.\n\ntable_data &lt;- plot_data %&gt;% \n  filter(\n    time %in% seq(0, 500, 50)\n  ) \n\nt &lt;- ggplot(table_data, aes(y = fct_rev(strata), x = time)) + \n  geom_text(aes(label = n.risk)) + \n  scale_x_continuous(breaks = seq(0, 500, 50), limits = c(0, 550))\n\nt\n\n\n\n\nNow that we have a basis of the plot for the table, we can further customize it by adding a theme, and then further clean up the axes and the labels of the figure.\n\nt &lt;- t + \n  theme(panel.background = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        axis.text = element_text(face = 'bold'))\n\nt\n\n\n\n\nWe now have 2 ggplot objects, p and t. The patchwork package is the ‘glue’ we need to put the two plots together.\n\nlibrary(patchwork)\n\nkm &lt;- (p / t) + plot_layout(height = c(1, .25))\n\nkm\n\n\n\n\nNow we have the final figure! I hope this post was informative in the possibilities with ggplot2. The benefits of making this figure from scratch as opposed to the packages available are the ability to further customize the figure to meet your needs.\n\nSession info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 11 x64 (build 22621)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] patchwork_1.1.3 lubridate_1.9.3 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.3     purrr_1.0.2     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.4   tidyverse_2.0.0 survival_3.5-5 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4       xfun_0.41          ggrepel_0.9.4      rstatix_0.7.2     \n [5] GGally_2.1.2       lattice_0.21-8     tzdb_0.4.0         vctrs_0.6.4       \n [9] tools_4.3.1        generics_0.1.3     fansi_1.0.5        pkgconfig_2.0.3   \n[13] Matrix_1.5-4.1     data.table_1.14.8  RColorBrewer_1.1-3 lifecycle_1.0.4   \n[17] compiler_4.3.1     farver_2.1.1       textshaping_0.3.7  munsell_0.5.0     \n[21] ggsci_3.0.0        carData_3.0-5      htmltools_0.5.7    yaml_2.3.7        \n[25] pillar_1.9.0       car_3.1-2          ggpubr_0.6.0       survminer_0.4.9   \n[29] abind_1.4-5        km.ci_0.5-6        commonmark_1.9.0   tidyselect_1.2.0  \n[33] digest_0.6.33      stringi_1.7.12     labeling_0.4.3     splines_4.3.1     \n[37] fastmap_1.1.1      grid_4.3.1         colorspace_2.1-0   cli_3.6.1         \n[41] magrittr_2.0.3     utf8_1.2.4         broom_1.0.5        withr_2.5.2       \n[45] scales_1.2.1       backports_1.4.1    timechange_0.2.0   rmarkdown_2.25    \n[49] ggtext_0.1.2       gridExtra_2.3      ggsignif_0.6.4     ragg_1.2.6        \n[53] zoo_1.8-12         hms_1.1.3          evaluate_0.23      knitr_1.45        \n[57] KMsurv_0.1-5       markdown_1.11      survMisc_0.5.6     rlang_1.1.2       \n[61] gridtext_0.1.5     Rcpp_1.0.11        xtable_1.8-4       glue_1.6.2        \n[65] xml2_1.3.5         renv_0.16.0        rstudioapi_0.15.0  reshape_0.8.9     \n[69] jsonlite_1.8.7     R6_2.5.1           plyr_1.8.9         systemfonts_1.0.5"
  },
  {
    "objectID": "posts/leaflet-covid19-mask-usage/index.html",
    "href": "posts/leaflet-covid19-mask-usage/index.html",
    "title": "Recreating the New York Times mask utilization survey data with the R opensource Leaflet package",
    "section": "",
    "text": "We’re going to recreate the NY Times mask-use survey data using R and the leaflet open source interactive mapping package. We can start off by loading the data from the New York Times github repository found here\n\nurl &lt;- 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/mask-use/mask-use-by-county.csv'\n\ndf &lt;- read_csv(url)\n\nNow that we have the data loaded, lets have a look at the data to see what we’re working with\n\nglimpse(df)\n\nRows: 3,142\nColumns: 6\n$ COUNTYFP   &lt;chr&gt; \"01001\", \"01003\", \"01005\", \"01007\", \"01009\", \"01011\", \"0101…\n$ NEVER      &lt;dbl&gt; 0.053, 0.083, 0.067, 0.020, 0.053, 0.031, 0.102, 0.152, 0.1…\n$ RARELY     &lt;dbl&gt; 0.074, 0.059, 0.121, 0.034, 0.114, 0.040, 0.053, 0.108, 0.0…\n$ SOMETIMES  &lt;dbl&gt; 0.134, 0.098, 0.120, 0.096, 0.180, 0.144, 0.257, 0.130, 0.1…\n$ FREQUENTLY &lt;dbl&gt; 0.295, 0.323, 0.201, 0.278, 0.194, 0.286, 0.137, 0.167, 0.1…\n$ ALWAYS     &lt;dbl&gt; 0.444, 0.436, 0.491, 0.572, 0.459, 0.500, 0.451, 0.442, 0.5…\n\ndf\n\n# A tibble: 3,142 × 6\n   COUNTYFP NEVER RARELY SOMETIMES FREQUENTLY ALWAYS\n   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 01001    0.053  0.074     0.134      0.295  0.444\n 2 01003    0.083  0.059     0.098      0.323  0.436\n 3 01005    0.067  0.121     0.12       0.201  0.491\n 4 01007    0.02   0.034     0.096      0.278  0.572\n 5 01009    0.053  0.114     0.18       0.194  0.459\n 6 01011    0.031  0.04      0.144      0.286  0.5  \n 7 01013    0.102  0.053     0.257      0.137  0.451\n 8 01015    0.152  0.108     0.13       0.167  0.442\n 9 01017    0.117  0.037     0.15       0.136  0.56 \n10 01019    0.135  0.027     0.161      0.158  0.52 \n# ℹ 3,132 more rows\n\n\nAccording to the repository, the definitions of the variables are as follows:\n\nCOUNTYFP: The county FIPS code.\nNEVER: The estimated share of people in this county who would say never in response to the question “How often do you wear a mask in public when you expect to be within six feet of another person?”\nRARELY: The estimated share of people in this county who would say rarely\nSOMETIMES: The estimated share of people in this county who would say sometimes\nFREQUENTLY: The estimated share of people in this county who would say frequently\nALWAYS: The estimated share of people in this county who would say always\n\nThey are also plotting the probability of encountering a mask usage among 5 random encounters in the county.\n\nThe chance all five people are wearing masks in five random encounters is calculated by assuming that survey respondents who answered ‘Always’ were wearing masks all of the time, those who answered ‘Frequently’ were wearing masks 80 percent of the time, those who answered ‘Sometimes’ were wearing masks 50 percent of the time, those who answered ‘Rarely’ were wearing masks 20 percent of the time and those who answered ‘Never’ were wearing masks none of the time.\n\nWe can calculate this simply by using the supplied weights (1, .8, .5, .2, and 0) among ALWAYS, FREQUENTLY, SOMETIMES, RARELY, and NEVER mask usage, and taking the sum of the proportion of mask usage among all 5 different types of individuals that have equal probability of encountering.\n\ndf &lt;- df %&gt;%\n  mutate(\n    prob = ((ALWAYS * 1) + (FREQUENTLY * .8) + (SOMETIMES * .5) + (RARELY * .2) + (NEVER * 0)) \n  )\n\nSince we have the county FIPS code data available, we’ll need to merge this data with county geojson data for the United States which I was able to obtain from here\n\ncounties &lt;- rgdal::readOGR('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_5m.json')\n\nOGR data source with driver: GeoJSON \nSource: \"https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_5m.json\", layer: \"gz_2010_us_050_00_5m\"\nwith 3221 features\nIt has 6 fields\n\n\nAfter reading in the US counties data, we can merge the mask usage survey data with the geojson file, by the state and FIPS code. We can create a COUNTYFP variable by pasting together the STATE and COUNTY code\n\ncounties@data &lt;- counties@data %&gt;%\n  mutate(\n    COUNTYFP = paste0(STATE, COUNTY)\n  ) %&gt;%\n  left_join(\n    df\n  )\n\nFurthermore after merging the data, we can create a label by merging together the % mask usage data into a HTML string\n\ncounties@data &lt;- counties@data %&gt;%\n  mutate(\n    label = glue::glue(\n      '&lt;b&gt;{NAME}&lt;/b&gt;&lt;br&gt;\n      {paste0(format(round(NEVER*100, 1), 1), \"%\")} estimated NEVER wear a mask &lt;br&gt;\n      {paste0(format(round(RARELY*100, 1), 1), \"%\")} estimated RARELY wear a mask &lt;br&gt;\n      {paste0(format(round(SOMETIMES*100, 1), 1), \"%\")} estimated SOMETIMES wear a mask &lt;br&gt;\n      {paste0(format(round(FREQUENTLY*100, 1), 1), \"%\")} estimated FREQUENTLY wear a mask &lt;br&gt;\n      {paste0(format(round(ALWAYS*100, 1), 1), \"%\")} estimated ALWAYS wear a mask &lt;br&gt;&lt;br&gt;\n      This translates to a &lt;b&gt;{paste0(format(round(prob*100, 1), 1), \"%\")}&lt;/b&gt; chance that everyone is masked in five random encounters'\n    ),\n    label = map(label, ~ htmltools::HTML(.x))\n  )\n\nFinally, let’s put this all together and create a Chloropleth map using the leaflet package\n\ncolor_pal &lt;- colorNumeric('plasma', counties$prob)\n\nmap &lt;- leaflet(counties) %&gt;%\n  addTiles() %&gt;%\n  fitBounds(\n    lng1 = -131.519605,\n    lng2 = -64.312607,\n    lat1 = 50.623510,\n    lat2 = 23.415249\n  ) %&gt;%\n  addPolygons(\n    fillColor = ~ color_pal(prob),\n    fillOpacity = .75,\n    weight = 1,\n    color = 'white',\n    label = ~ label,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    position = 'bottomright',\n    pal = color_pal,\n    values = ~ counties$prob,\n    title = '% Mask Usage',\n    labFormat = labelFormat(\n      suffix = '%',\n      transform = function(x)\n        x * 100\n    )\n  )\n\nmap\n\n\n\n\n\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] leaflet_2.1.2   lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0  \n [5] dplyr_1.1.1     purrr_1.0.1     readr_2.1.4     tidyr_1.3.0    \n [9] tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.0   xfun_0.38          lattice_0.20-45    colorspace_2.1-0  \n [5] vctrs_0.6.1        generics_0.1.3     viridisLite_0.4.1  htmltools_0.5.4   \n [9] yaml_2.3.6         utf8_1.2.3         rlang_1.1.0        pillar_1.9.0      \n[13] glue_1.6.2         withr_2.5.0        RColorBrewer_1.1-3 sp_1.6-0          \n[17] bit64_4.0.5        lifecycle_1.0.3    munsell_0.5.0      gtable_0.3.3      \n[21] htmlwidgets_1.6.2  evaluate_0.19      knitr_1.41         tzdb_0.3.0        \n[25] fastmap_1.1.0      crosstalk_1.2.0    parallel_4.2.2     curl_5.0.0        \n[29] fansi_1.0.4        renv_0.16.0        scales_1.2.1       vroom_1.6.1       \n[33] jsonlite_1.8.4     farver_2.1.1       bit_4.0.5          gridExtra_2.3     \n[37] hms_1.1.3          digest_0.6.31      stringi_1.7.8      grid_4.2.2        \n[41] rgdal_1.6-5        cli_3.6.1          tools_4.2.2        magrittr_2.0.3    \n[45] crayon_1.5.2       pkgconfig_2.0.3    ellipsis_0.3.2     timechange_0.2.0  \n[49] viridis_0.6.2      rmarkdown_2.19     rstudioapi_0.14    R6_2.5.1          \n[53] compiler_4.2.2"
  },
  {
    "objectID": "posts/my-life-in-months/index.html",
    "href": "posts/my-life-in-months/index.html",
    "title": "My life in Months - Making a ‘life plot’ in R using ggplot2",
    "section": "",
    "text": "This blog post is inspired by Sharla Gefland twitter post found here, where she made a ‘My Life in Months’ plot.\nAnnotations have always been the bane of my existence in ggplot2, and I figured this would be a fun project to get some practice. Looking at her github repo found here, she made this figure using the waffle plot package found here. Although using the waffle package may simplify some aspects of making this figure, recreating this figure in pure ggplot2 will open up the arguments for further customization that may not be available via the waffle package.\nWe can start off by creating a tibble for the basis of the plot. The goal here is to create a tibble starting from the starting month (month/year) I was born, until the current month/year. I can create this with the help of the lubridate package, which simplifies the handling of dates in R, and using this package to further extract the month and year information from the date sequence.\n\ndf &lt;- tibble(\n  date = seq(mdy('9/1/1987'), floor_date(Sys.Date(), 'month'), 'month')\n) %&gt;%\n  mutate(\n    month = month(date),\n    year = year(date)\n  )\n\ndf &lt;- tibble(date = seq(mdy('9/1/1987'), Sys.Date(), '1 month')) %&gt;%\n  mutate(month = month(date),\n         year = year(date))\n\ndf\n\n# A tibble: 428 × 3\n   date       month  year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 1987-09-01     9  1987\n 2 1987-10-01    10  1987\n 3 1987-11-01    11  1987\n 4 1987-12-01    12  1987\n 5 1988-01-01     1  1988\n 6 1988-02-01     2  1988\n 7 1988-03-01     3  1988\n 8 1988-04-01     4  1988\n 9 1988-05-01     5  1988\n10 1988-06-01     6  1988\n# ℹ 418 more rows\n\n\nUsing the tibble I just created, I can further define the ‘eras’ that I would like to highlight in the life plot.\n\nplot_data &lt;- df %&gt;%\n  mutate(\n    era = case_when(\n      date %in% mdy('9/1/1987'):mdy(\"9/1/1991\") ~ 'Childhood',\n      date %in% mdy('10/1/1991'):mdy('6/1/2005') ~ 'K-12 Grade School',\n      date %in% mdy('7/1/2005'):mdy('12/1/2009') ~ 'BSc in Biological Sciences',\n      date %in% mdy('1/1/2010'):mdy('7/1/2013') ~ 'Pre Graduate Work',\n      date %in% mdy('8/1/2013'):mdy('6/1/2015') ~ 'MPH in Biostatistics & Epidemiology',\n      date %in% mdy('7/1/2015'):mdy('8/1/2016') ~ 'Data Analyst',\n      date %in% mdy('9/1/2016'):Sys.Date() ~ 'Biostatistician'\n    )\n  ) %&gt;%\n  mutate(era = factor(\n    era,\n    levels = c(\n      'Childhood',\n      'K-12 Grade School',\n      'BSc in Biological Sciences',\n      'Pre Graduate Work',\n      'MPH in Biostatistics & Epidemiology',\n      'Data Analyst',\n      'Biostatistician'\n    )\n  ))\n\nplot_data\n\n# A tibble: 428 × 4\n   date       month  year era      \n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    \n 1 1987-09-01     9  1987 Childhood\n 2 1987-10-01    10  1987 Childhood\n 3 1987-11-01    11  1987 Childhood\n 4 1987-12-01    12  1987 Childhood\n 5 1988-01-01     1  1988 Childhood\n 6 1988-02-01     2  1988 Childhood\n 7 1988-03-01     3  1988 Childhood\n 8 1988-04-01     4  1988 Childhood\n 9 1988-05-01     5  1988 Childhood\n10 1988-06-01     6  1988 Childhood\n# ℹ 418 more rows\n\n\nNext, I’ll create a base plot using ggplot2, where I’ll map the x axis to year, and the y axis to month. I’ll also use the geom, geom_tile() to create the ‘blocks’ that we see in the life plot, where we’ll map the fill to era.\n\n ggplot(plot_data, aes(y = month, x = year)) + \n  geom_tile(color = 'white', aes(fill = era), size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nNow that we have a simple base plot to work with, we can further customize and clean up the figure. A trick to give us a bigger ‘space’ to work with is to expand the limits of the y and x axis. Furthermore, I will use scale_fill_d3() to add a fill theme to the plot.\n\nbase_plot &lt;- ggplot(plot_data, aes(y = month, x = year)) + \n  geom_tile(color = 'white', aes(fill = era), size = 1) + \n  scale_y_continuous(breaks = -6:18, limits = c(-6, 18)) +\n  scale_x_continuous(breaks = 1980:2020) +\n  labs(y = 'Month', x = 'Year') + \n  theme(legend.position = 'bottom') + \n  scale_fill_d3()\n  \n\nbase_plot\n\n\n\n\nAnnotations have always been tricky, because we have to specifically define the coordinates of the annotations we are trying to add. I’m going to start off small with a small annotation on the top left corner with an arrow point to the top left square. The segments are created using the geom_curve() and the text annotations are created using annotate() via geom_text()\n\n## annotate the definition of 1 square = 1 month\nplot &lt;- base_plot +\n  geom_curve(\n    x = 1987,\n    y = 12,\n    xend = 1986,\n    yend = 14,\n    curvature = -.4,\n    arrow = arrow(length = unit(0.01, \"npc\"), ends = 'first'),\n    color = 'black'\n  ) + \n  annotate(\n    'text',\n    x = 1985,\n    y = 15,\n    hjust = 0,\n    label = '1 square = 1 month',\n    family = \"Segoe Script\"\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNext I’ll start to map out exactly where I want each of the labels for the eras to be placed. This definitely took a while, and it helps if you have some forethought on where you want to place the labels.\n\n### set colors \npallete_colors &lt;- pal_d3(\"category10\")(10)\n\n## set size\nannotation_size &lt;- 5\n\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1989,\n    y = -1,\n    label = 'Childhood',\n    color = pallete_colors[[1]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  )  +\n  annotate(\n    'text',\n    x = 1998,\n    y = -1,\n    label = 'K-12 Grade School',\n    color = pallete_colors[[2]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2007.5,\n    y = -1,\n    label = 'BSc in Biological Sciences',\n    color = pallete_colors[[3]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2011,\n    y = 14,\n    label = 'Pre Graduate Employment',\n    color = pallete_colors[[4]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2013,\n    y = -3,\n    label = 'MPH in Biostatistics & Epidemiology',\n    color = pallete_colors[[5]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2012.5,\n    y = 16,\n    label = 'Data Analyst',\n    color = pallete_colors[[6]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) +\n  annotate(\n    'text',\n    x = 2018.5,\n    y = -1,\n    label = 'Biostatistician',\n    color = pallete_colors[[7]],\n    size = annotation_size,\n    family = \"Segoe Script\"\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNow that we have the text placed in all the designated coordinates, we can start working on the arrows.\n\n## add additional curve segments for labels\n\nplot &lt;- plot + \n  geom_curve(\n    x = 1989,\n    y = 1,\n    xend = 1989,\n    yend = -.5,\n    curvature = .2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 1998,\n    y = 1,\n    xend = 1998,\n    yend = -.5,\n    curvature = .2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2007,\n    y = 1,\n    xend = 2007,\n    yend = -.5,\n    curvature = -.2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2011,\n    y = 12,\n    xend = 2011,\n    yend = 13.5,\n    curvature = -.2,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2015,\n    y = 12,\n    xend =  2015,\n    yend = 16,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    color = 'black',\n    curvature = .8\n  ) +\n  geom_curve(\n    x = 2014,\n    y = 1,\n    xend =  2014,\n    yend = -2.5,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    curvature = -0.2,\n    color = 'black'\n  ) +\n  geom_curve(\n    x = 2018,\n    y = 1,\n    xend =  2018,\n    yend = -0.5,\n    arrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\n    curvature = -0.2,\n    color = 'black'\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNow that we have most of the annotations on there, we can add some supplemental annotations, e.g. adding an annotations regarding each column is 1 year, and the segments to finish off the look.\n\n## let's add a label for 1 column equals 1 year of age \n\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1985,\n    y = 6,\n    label = '1 year',\n    angle = 90,\n    size = 7,\n    color = 'black',\n    family = \"Segoe Script\"\n  ) + \n  annotate(\n    'text',\n    x = 1988,\n    y = 13,\n    label = 'age',\n    size = 5,\n    color = 'black',\n    family = \"Segoe Script\"\n  ) +\n  geom_segment(\n    x = 1988.75,\n    y = 13,\n    xend = 1993,\n    yend = 13,\n    arrow = arrow(ends = 'last', length = unit(.01, units = 'npc')),\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1985,\n    xend = 1985,\n    y = 8,\n    yend = 12,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1985,\n    xend = 1985,\n    y = 1,\n    yend = 4,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1984.5,\n    xend = 1985.5,\n    y = 12,\n    yend = 12,\n    color = 'black'\n  ) +\n  geom_segment(\n    x = 1984.5,\n    xend = 1985.5,\n    y = 1,\n    yend = 1,\n    color = 'black'\n  ) \n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nWe’re almost there - now that we have all the annotations we want on there, we can remove the legend and use a theme to further remove the grid as well as the x and y axis.\n\nplot &lt;- plot +\n  theme_void() +\n  theme(\n    legend.position = 'none'\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nLet’s finish off this off by adding a title\n\n## lets add a title\nplot &lt;- plot + \n  annotate(\n    'text',\n    x = 1987,\n    y = -5,\n    label = 'Michael Luu',\n    size = 25,\n    hjust = 0,\n    fontface = 'bold.italic'\n  )\n\nplot\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nFull resolution figure can be found here along with the github repo for the full code here\n\nSession info\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] extrafont_0.19  ggsci_3.0.0     lubridate_1.9.2 forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.1     purrr_1.0.1     readr_2.1.4    \n [9] tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] compiler_4.2.2    pillar_1.9.0      tools_4.2.2       digest_0.6.31    \n [5] timechange_0.2.0  jsonlite_1.8.4    evaluate_0.19     lifecycle_1.0.3  \n [9] gtable_0.3.3      pkgconfig_2.0.3   rlang_1.1.0       cli_3.6.1        \n[13] rstudioapi_0.14   yaml_2.3.6        xfun_0.38         fastmap_1.1.0    \n[17] Rttf2pt1_1.3.12   withr_2.5.0       knitr_1.41        systemfonts_1.0.4\n[21] hms_1.1.3         generics_0.1.3    vctrs_0.6.1       htmlwidgets_1.6.2\n[25] grid_4.2.2        tidyselect_1.2.0  glue_1.6.2        R6_2.5.1         \n[29] textshaping_0.3.6 fansi_1.0.4       rmarkdown_2.19    farver_2.1.1     \n[33] extrafontdb_1.0   tzdb_0.3.0        magrittr_2.0.3    scales_1.2.1     \n[37] htmltools_0.5.4   colorspace_2.1-0  renv_0.16.0       ragg_1.2.5       \n[41] labeling_0.4.2    utf8_1.2.3        stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/quarto-generate-tabs/index.html",
    "href": "posts/quarto-generate-tabs/index.html",
    "title": "Programatically generate Quarto tabs",
    "section": "",
    "text": "When working with a list of objects, it may be useful to organize the objects into tabs instead of a huge list of individual objects\nUsing the iris dataset as a working example, I generate a list of ggplot objects.\n\ndata &lt;- iris |&gt; as_tibble() |&gt; janitor::clean_names()\n\nout &lt;- data |&gt; \n  group_nest(species) |&gt; \n  deframe()\n\nout &lt;- out |&gt; \n  map(\\(data) {\n    \n    ggplot(data, aes(x = sepal_length, y = sepal_width)) + \n      geom_point()\n    \n  })\n\nThe list of ggplot2 objects can be called and presented as below\n\nout\n\n$setosa\n\n\n\n\n\n\n$versicolor\n\n\n\n\n\n\n$virginica\n\n\n\n\n\nInstead of presenting a long list of plots, we can organize the plots into individual tabs. In order to do this, we utilize a combination of imap_chr() and knit_child(). We use imap_chr() to pass on individual plots into knit_child(). We wrap this chunk within a fenced div panel-tabset, and utilize results: asis.\n\n```{r}\n#| eval: false\n\nout &lt;- imap_chr(out, \\(out, title) {\n  \n  text &lt;- glue::glue(\"## `r title`\",\n                     \"```{r}\",\n                     \"out\",\n                     \"```\",\n                     \"\", .sep = '\\n\\n')\n  \n  knitr::knit_child(text = text, envir = environment(),\n                    quiet = T)\n  \n})\n\ncat(out, sep = '\\n')\n```\n\n\nsetosaversicolorvirginica\n\n\n\nout\n\n\n\n\n\n\n\nout\n\n\n\n\n\n\n\nout"
  },
  {
    "objectID": "posts/quarto-webr/index.html",
    "href": "posts/quarto-webr/index.html",
    "title": "Implementation of WebR in Quarto",
    "section": "",
    "text": "WebR is a web implementation of the R statistical software in a web browser built using the Node.js and WebAssembly framework. The official documentation will have a better explanation of how WebR works. However, the gist of WebR is the ability to run R code directly in the web browser, without the need of installing R or running code on a server.\nThe purpose of this blog post is to provide a baseline example implementation of WebR in Quarto. The example instructions provided here is based on the the following github example by James J Balamuta, the developer of the webr quarto extension.\nThe below code chunk will construct a section of the rendered quarto document where R code can be ran directly in the browser. Although the development of WebR is still early, the ability to run R code directly in the browser, and without the need of a server or additional infrastructure is a very power tool.\nSome of the current limitations of WebR is speed, and the need to ‘install’ packages to prime the WebR environment. I can only imagine that issue with speed will only improve in due time. Packages will also need to be specifically compiled for WebAssembly. The current default repository for R packages compiled for WebR can be found here.\nBefore a specific R package can be utilized, we will need to ‘install’ the WebR compiled version of the package in a WebR environment like so.\n\n  🟡 Loading\n    webR...\n  \n    \n    \n      \n    \n  \n  \n  \n\n\nOnce the WebR package has been installed, we can load the library as we normally would.\n\n  🟡 Loading\n    webR..."
  },
  {
    "objectID": "posts/secret-santa-shinylive/index.html",
    "href": "posts/secret-santa-shinylive/index.html",
    "title": "Secret Santa Randomizer using ShinyLive 🎅🎄🎁",
    "section": "",
    "text": "It’s the time of year for giving 🎅🎄🎁\nThe purpose of this post is to showcase a simple Secret Santa Randomizer built using Shiny, the bslib dashboard framework, and ShinyLive (a server-less Shiny implementation).\nThe source code for this application can be found here."
  }
]