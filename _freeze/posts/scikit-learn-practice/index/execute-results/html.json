{
  "hash": "68e77cbbac752e4d3b343c050898d65f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scikit-Learn Practice From the Perspective of an R User\"\ndate: '2024-02-02'\ndraft: false\ntoc: true\nimage: 'https://images.unsplash.com/photo-1591453089816-0fbb971b454c?q=80&w=1470&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'\ncategories:\n  - python\n  - scikit-learn\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\nOne of the premiere machine learning modules in python is `scikit-learn` and it's going to be the first place I'm going to start in my journey of learning the python language.  I am going to go through the process of building a simple logistic regression model using the `tidymodels` package in R, and compare it to the process of `scikit-learn` in python.\n\nIn this example, I will be using the palmerpenguins dataset found in the below link.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- 'https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\n```\n:::\n\n\nThe overall goal is to do a series of very standard steps in training a binary classifier (Logistic Regression).\n\n1. Read in the dataset\n2. Split the dataset into a training and testing set\n3. Train the model on the training set\n4. Evaluate the model on the testing set\n\n# tidymodels\n\nLet's start off with reading in the dataset in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(url, show_col_types = FALSE)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 344 × 9\n   rowid species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n 1     1 Adelie  Torgersen           39.1          18.7               181\n 2     2 Adelie  Torgersen           39.5          17.4               186\n 3     3 Adelie  Torgersen           40.3          18                 195\n 4     4 Adelie  Torgersen           NA            NA                  NA\n 5     5 Adelie  Torgersen           36.7          19.3               193\n 6     6 Adelie  Torgersen           39.3          20.6               190\n 7     7 Adelie  Torgersen           38.9          17.8               181\n 8     8 Adelie  Torgersen           39.2          19.6               195\n 9     9 Adelie  Torgersen           34.1          18.1               193\n10    10 Adelie  Torgersen           42            20.2               190\n# ℹ 334 more rows\n# ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n```\n\n\n:::\n:::\n\n\nSince logistic regression is a binary classifier, we will convert the species variable into a binary outcome. The outcome we are going to train the model for is 'Chinstrap' vs 'Not Chinstrap'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data |> \n  mutate(\n    outcome = case_when(\n      species == 'Chinstrap' ~ 1,\n      .default = 0\n    ),\n    outcome = factor(outcome)\n  )\n```\n:::\n\n\nNow that we have the outcome coded, we can split the data into a training and testing set.  We will use the `initial_split` function from the `rsample` package to split the data into a 75/25 split.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsplit <- initial_split(data, strata = outcome)\n\ntrain <- training(split)\ntest <- testing(split)\n\ntrain\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 258 × 10\n   rowid species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n 1     1 Adelie  Torgersen           39.1          18.7               181\n 2     2 Adelie  Torgersen           39.5          17.4               186\n 3     5 Adelie  Torgersen           36.7          19.3               193\n 4     9 Adelie  Torgersen           34.1          18.1               193\n 5    10 Adelie  Torgersen           42            20.2               190\n 6    12 Adelie  Torgersen           37.8          17.3               180\n 7    13 Adelie  Torgersen           41.1          17.6               182\n 8    14 Adelie  Torgersen           38.6          21.2               191\n 9    15 Adelie  Torgersen           34.6          21.1               198\n10    16 Adelie  Torgersen           36.6          17.8               185\n# ℹ 248 more rows\n# ℹ 4 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>, outcome <fct>\n```\n\n\n:::\n\n```{.r .cell-code}\ntest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 86 × 10\n   rowid species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n 1     3 Adelie  Torgersen           40.3          18                 195\n 2     4 Adelie  Torgersen           NA            NA                  NA\n 3     6 Adelie  Torgersen           39.3          20.6               190\n 4     7 Adelie  Torgersen           38.9          17.8               181\n 5     8 Adelie  Torgersen           39.2          19.6               195\n 6    11 Adelie  Torgersen           37.8          17.1               186\n 7    18 Adelie  Torgersen           42.5          20.7               197\n 8    21 Adelie  Biscoe              37.8          18.3               174\n 9    35 Adelie  Dream               36.4          17                 195\n10    38 Adelie  Dream               42.2          18.5               180\n# ℹ 76 more rows\n# ℹ 4 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>, outcome <fct>\n```\n\n\n:::\n:::\n\n\nNow that we have the data split, we can pre-process the data using the recipe package.  Within recipe, we will define the outcome variable `outcome` as well as define the predictor variables `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Since there are a number of missing data, we are going to use the `step_impute_bag` function to impute the missing data. We will then normalize the predictor variables using the `step_normalize` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <-\n  recipe(outcome ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,\n         data = data) |>\n  step_impute_bag(all_predictors()) |>\n  step_scale(all_numeric_predictors())\n\nrec\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Bagged tree imputation for: all_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Scaling for: all_numeric_predictors()\n```\n\n\n:::\n:::\n\n\nNext we are going to define a logistic regression model specification using the `logistic_reg` function from the `parsnip` package. We will also define the engine as 'glm' and the mode as 'classification'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_spec <- logistic_reg() |> \n  set_engine('glm') |> \n  set_mode('classification')\n\nglm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n\n\n:::\n:::\n\n\nNow we are going to combine the recipe and model specification into a workflow using the `workflow` function from the `workflows` package. We will then use the `last_fit` function to train the model in the training set, and assess the model on the testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf <- workflow(rec, glm_spec)\n\nlf <- last_fit(wf, split)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ A | warning: All predictors are missing; cannot impute\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ B | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x2   B: x1\n```\n\n\n:::\n\n```{.r .cell-code}\nlf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  <list>           <chr>            <list>   <list>   <list>       <list>    \n1 <split [258/86]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\nThere were issues with some computations:\n\n  - Warning(s) x2: All predictors are missing; cannot impute\n  - Warning(s) x1: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n\n\n:::\n:::\n\n\nWe can then assess the performance of the model on the test set using the `conf_mat` function from the `yardstick` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncm <- lf |> \n  collect_predictions() |> \n  conf_mat(truth = outcome, estimate = .pred_class)\n\ncm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  0  1\n         0 68  2\n         1  0 15\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(cm, event_level = 'second')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.976\n 2 kap                  binary         0.923\n 3 sens                 binary         0.882\n 4 spec                 binary         1    \n 5 ppv                  binary         1    \n 6 npv                  binary         0.971\n 7 mcc                  binary         0.926\n 8 j_index              binary         0.882\n 9 bal_accuracy         binary         0.941\n10 detection_prevalence binary         0.176\n11 precision            binary         1    \n12 recall               binary         0.882\n13 f_meas               binary         0.938\n```\n\n\n:::\n\n```{.r .cell-code}\nroc <- lf |> \n  collect_predictions() |> \n  roc_curve(truth = outcome, .pred_1, event_level = 'second') |> \n  autoplot()\n\nroc\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n# scikit-learn\n\nLet's start off with loading the pandas module and read in the dataset using the `read_csv` method.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nurl = 'https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv'\n\ndata = pd.read_csv(url)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     rowid    species     island  ...  body_mass_g     sex  year\n0        1     Adelie  Torgersen  ...       3750.0    male  2007\n1        2     Adelie  Torgersen  ...       3800.0  female  2007\n2        3     Adelie  Torgersen  ...       3250.0  female  2007\n3        4     Adelie  Torgersen  ...          NaN     NaN  2007\n4        5     Adelie  Torgersen  ...       3450.0  female  2007\n..     ...        ...        ...  ...          ...     ...   ...\n339    340  Chinstrap      Dream  ...       4000.0    male  2009\n340    341  Chinstrap      Dream  ...       3400.0  female  2009\n341    342  Chinstrap      Dream  ...       3775.0    male  2009\n342    343  Chinstrap      Dream  ...       4100.0    male  2009\n343    344  Chinstrap      Dream  ...       3775.0  female  2009\n\n[344 rows x 9 columns]\n```\n\n\n:::\n:::\n\n\nNow that we have the data read into memory, let's convert the species variable into a binary outcome. The most intuitive method I found to do this is to use the `where` method from the `numpy` module. This appears most similar to the `mutate` with the `ifelse` function in R.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndata['outcome'] = np.where(data['species'] == 'Chinstrap', 1, 0)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     rowid    species     island  ...     sex  year  outcome\n0        1     Adelie  Torgersen  ...    male  2007        0\n1        2     Adelie  Torgersen  ...  female  2007        0\n2        3     Adelie  Torgersen  ...  female  2007        0\n3        4     Adelie  Torgersen  ...     NaN  2007        0\n4        5     Adelie  Torgersen  ...  female  2007        0\n..     ...        ...        ...  ...     ...   ...      ...\n339    340  Chinstrap      Dream  ...    male  2009        1\n340    341  Chinstrap      Dream  ...  female  2009        1\n341    342  Chinstrap      Dream  ...    male  2009        1\n342    343  Chinstrap      Dream  ...    male  2009        1\n343    344  Chinstrap      Dream  ...  female  2009        1\n\n[344 rows x 10 columns]\n```\n\n\n:::\n:::\n\n\nNow that we have the outcome coded, we can split the data into a training and testing set.  We will use the `train_test_split` method from the `sklearn` module to split the data into a 75/25 split. The `sklearn` module appears to t\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\nX = data.loc[:, ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\ny = data.loc[:, 'outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n```\n:::\n\n\nWe will use a pipeline to pre-process the data using the `KNNImputer` and `StandardScaler` function from the `sklearn` module. We will then train the model using the `LogisticRegression` method.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe = make_pipeline(KNNImputer(), StandardScaler(), LogisticRegression())\n\npipe.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;knnimputer&#x27;, KNNImputer()),\n                (&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;knnimputer&#x27;, KNNImputer()),\n                (&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;logisticregression&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNNImputer</label><div class=\"sk-toggleable__content\"><pre>KNNImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>\n```\n\n:::\n:::\n\n\nNow that the model is fit, let's assess the model performance using the `confusion_matrix` and `classification_report` function from the `sklearn` module.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay\n\nconfusion_matrix(y_test, pipe.predict(X_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[68,  0],\n       [ 1, 17]], dtype=int64)\n```\n\n\n:::\n\n```{.python .cell-code}\nroc_auc_score(y_test, pipe.predict_proba(X_test)[:, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9959150326797386\n```\n\n\n:::\n\n```{.python .cell-code}\ncr = classification_report(y_test, pipe.predict(X_test))\n\nprint(cr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99        68\n           1       1.00      0.94      0.97        18\n\n    accuracy                           0.99        86\n   macro avg       0.99      0.97      0.98        86\nweighted avg       0.99      0.99      0.99        86\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nRocCurveDisplay.from_predictions(y_test, pipe.predict_proba(X_test)[:, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<sklearn.metrics._plot.roc_curve.RocCurveDisplay object at 0x000001FCCBD526B0>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=614}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}